{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BERT_training_application_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQfvsBNzuXjm"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import re\n",
        "import copy\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZtRaP-Atg4N",
        "outputId": "5eb24977-235c-40c2-bea7-306fb4b6a169"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/cs6471proj2/amazon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/cs6471proj2/amazon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfML79MbqucI"
      },
      "source": [
        "# Combine the annotation results from two annotators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feC0dRhiq2cj",
        "outputId": "907b4971-5452-4042-d056-7743b582520d"
      },
      "source": [
        "with open('purpose_classification/complete_idx_label_ruiqi2.json', 'r') as f:\n",
        "    data1 = json.load(f)\n",
        "\n",
        "with open('purpose_classification/complete_idx_label_zexing4.json', 'r') as f:\n",
        "    data2 = json.load(f)\n",
        "\n",
        "data = set()\n",
        "for k, item in data1.items():\n",
        "    data.add((k, item[0]))\n",
        "\n",
        "for k, item in data2.items():\n",
        "    data.add((k, item[0]))\n",
        "\n",
        "assert len(data) ==  (len(data1) + len(data2))\n",
        "\n",
        "print('DATA AMOUNT:', len(data))\n",
        "\n",
        "data = list(data)\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "eval = data[-400:]\n",
        "training = data[:-400]\n",
        "\n",
        "with open('purpose_classification_new/training_idx_target.json', 'w') as f:\n",
        "    json.dump(training, f)\n",
        "\n",
        "with open('purpose_classification_new/eval_idx_target.json', 'w') as f:\n",
        "    json.dump(eval, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA AMOUNT: 3737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWp9kFr7OT0l"
      },
      "source": [
        "# Anotation data anslysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-1_nMU9PUtb"
      },
      "source": [
        "country_worth2lookat = {'china':['china', 'ch'], 'us':['usa', 'us', 'america', 'united states', 'the states'], \n",
        "                      'uk':['uk', 'england', 'britain', 'ireland', 'scotland'], \n",
        "                      'japan':['japan'], 'mexico':['mexico'], 'germany': ['germany'], \n",
        "                      'canada':['canada'], 'india':['india'],  'australia':['australia'], \n",
        "                      'italy':['italy'], 'france':['france'], 'taiwan':['taiwan'], 'vietnam':['vietnam'], \n",
        "                      'thailand':['thailand'], 'brazil':['brazil'], 'korea':['korea'], \n",
        "                      'afghanistan':['afghanistan'], 'spain':['spain'], 'new zealand':['new zealand'], \n",
        "                      'singapore':['singapore'], 'iraq':['iraq'], 'russia':['russia'], 'switzerland':['switzerland']}\n",
        "\n",
        "\n",
        "nationality_worth2lookat = {'american': ['american'], 'english': ['english'], 'german':['german'], \n",
        "                          'chinese':['chinese'], 'british':['british', 'scottish', 'irish'], \n",
        "                          'spanish':['spanish'], 'french':['french'], 'japanese':['japanese'], \n",
        "                          'european':['european'], 'australian':['australian'], \n",
        "                          'african':['african'], 'asian':['asian'], 'italian':['italian'], \n",
        "                          'canadian':['canadian'],  'indian':['indian'], \n",
        "                          'russian':['russian'], 'mexican':['mexican']} # 17\n",
        "\n",
        "# build word2label\n",
        "word2label_country = {}\n",
        "word2label_nationality = {}\n",
        "for key, item in country_worth2lookat.items():\n",
        "    for word in item:\n",
        "        word2label_country[word] = key\n",
        "for key, item in nationality_worth2lookat.items():\n",
        "    for word in item:\n",
        "        word2label_nationality[word] = key\n",
        "\n",
        "path2newfiles = 'purpose_classification'\n",
        "file_handle = open('/'.join([path2newfiles, 'global_country_idx2data.json']), 'r')\n",
        "idx2data = json.load(file_handle)\n",
        "file_handle.close()\n",
        "\n",
        "file_handle = open('/'.join([path2newfiles, 'common_country_sample_idx.json']), 'r')\n",
        "common_country_sample_idx = json.load(file_handle)\n",
        "file_handle.close()\n",
        "\n",
        "file_handle = open('/'.join([path2newfiles, 'global_country_token.json']), 'r')\n",
        "token = json.load(file_handle)\n",
        "file_handle.close()\n",
        "\n",
        "file_handle = open('/'.join([path2newfiles, 'global_country_data.json']), 'r')\n",
        "pos = json.load(file_handle)\n",
        "file_handle.close()\n",
        "\n",
        "file_handle = open('/'.join([path2newfiles, 'global_country2idx.json']), 'r')\n",
        "global_country2idx = json.load(file_handle)\n",
        "file_handle.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD-rHuNxN1Tw",
        "outputId": "f929c460-e0d5-44b4-9a50-0ec299e226ea"
      },
      "source": [
        "\n",
        "with open('purpose_classification/complete_idx_label_ruiqi2.json', 'r') as f:\n",
        "    zexing = json.load(f)\n",
        "with open('purpose_classification/complete_idx_label_zexing4.json', 'r') as f:\n",
        "    ruiqi = json.load(f)\n",
        "\n",
        "# idx2country:\n",
        "idx2country = {}\n",
        "for k, item in global_country2idx.items():\n",
        "    for i in item:\n",
        "        idx2country[int(i)] = k\n",
        "    \n",
        "print(len(zexing)+len(ruiqi))\n",
        "total = copy.deepcopy(zexing)\n",
        "for k, item in ruiqi.items():\n",
        "    total[k] = item\n",
        "\n",
        "print(len(total))\n",
        "\n",
        "dict_ = {}\n",
        "for k, item in total.items():\n",
        "    if idx2country[int(k)] not in word2label_country:\n",
        "        print(idx2country[int(k)])\n",
        "        continue\n",
        "    country = word2label_country[idx2country[int(k)]]\n",
        "    if country not in dict_:\n",
        "        dict_[country] = {item[0]:1}\n",
        "    else:\n",
        "        if item[0] not in dict_[country]:\n",
        "            dict_[country][item[0]] = 1\n",
        "        else:\n",
        "            dict_[country][item[0]] += 1\n",
        "\n",
        "print(dict_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3737\n",
            "3737\n",
            "{'us': {2: 388, 1: 705, 3: 209, 4: 326, 5: 77}, 'china': {1: 829, 5: 35, 4: 11, 2: 13, 3: 10}, 'uk': {3: 89, 2: 84, 1: 70, 4: 109, 5: 13}, 'russia': {5: 1, 1: 3, 4: 3, 2: 2, 3: 4}, 'brazil': {1: 5, 2: 1, 4: 8, 3: 1}, 'afghanistan': {3: 9, 2: 1, 4: 5}, 'mexico': {2: 13, 1: 39, 3: 18, 4: 14}, 'vietnam': {3: 7, 2: 10, 1: 10, 4: 11}, 'canada': {2: 11, 3: 20, 1: 22, 4: 7}, 'thailand': {1: 11, 2: 3, 4: 1, 3: 8}, 'iraq': {3: 3, 2: 4, 4: 5, 1: 1}, 'japan': {1: 44, 3: 23, 2: 13, 4: 13}, 'germany': {1: 46, 2: 8, 3: 13, 4: 5}, 'italy': {1: 14, 4: 11, 3: 16, 2: 6}, 'australia': {3: 24, 1: 12, 2: 13, 4: 10}, 'france': {3: 12, 2: 5, 1: 12, 4: 12}, 'india': {4: 23, 1: 26, 3: 14, 2: 7, 5: 2}, 'taiwan': {1: 29, 2: 1, 4: 1}, 'switzerland': {2: 2, 1: 11, 3: 3, 4: 1}, 'korea': {1: 14, 2: 1, 4: 5, 3: 1}, 'singapore': {1: 6, 3: 6, 4: 6, 2: 6}, 'spain': {3: 2, 2: 4, 4: 9, 1: 3}, 'new zealand': {3: 5, 4: 4, 1: 3, 2: 1}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxG5Ig3oOeYL",
        "outputId": "4c653811-1d6b-4b8c-caed-e10e3b6167ad"
      },
      "source": [
        "stat = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
        "for k, item in zexing.items():\n",
        "    stat[int(item[0])] += 1\n",
        "\n",
        "print(stat)\n",
        "\n",
        "stat1 = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
        "for k, item in ruiqi.items():\n",
        "    stat1[int(item[0])] += 1\n",
        "\n",
        "print(stat1)\n",
        "\n",
        "for k, item in stat.items():\n",
        "    stat1[k] += item\n",
        "\n",
        "print(stat1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 592, 2: 220, 3: 145, 4: 160, 5: 31}\n",
            "{1: 1323, 2: 377, 3: 352, 4: 440, 5: 97}\n",
            "{1: 1915, 2: 597, 3: 497, 4: 600, 5: 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSMxZnddQdkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4a20ae-1e08-4ba0-ceb9-8adf50ed9aeb"
      },
      "source": [
        "# {1: 592, 2: 220, 3: 145, 4: 160, 5: 31}\n",
        "# {1: 788, 2: 248, 3: 241, 4: 167, 5: 66}\n",
        "# {1: 1380, 2: 468, 3: 386, 4: 327, 5: 97}\n",
        "1380/(1380+468+386+327+97)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5191873589164786"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rskZFQ89ETf"
      },
      "source": [
        "# {1: 592, 2: 220, 3: 145, 4: 160, 5: 31}\n",
        "# {1: 1323, 2: 377, 3: 352, 4: 440, 5: 97}\n",
        "# {1: 1915, 2: 597, 3: 497, 4: 600, 5: 128}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwjHFn4tubAb"
      },
      "source": [
        "# BERT RELATED CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmjke-ra2plO"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px533_rF2QzC",
        "outputId": "e3ab902d-9595-49c4-b4b6-ecafe6ce2f25"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 51.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1IT-whxuePC"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import json\n",
        "\n",
        "class HDCTModel(BertModel):\n",
        "    \"\"\"\n",
        "    A model wrapper around hf BERT implementation\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config=config)\n",
        "        self.fc = nn.Linear(config.hidden_size, 5)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, country_mask, input_ids, token_type_ids, attention_mask):\n",
        "        outputs = super().forward(input_ids, attention_mask, token_type_ids)\n",
        "        last_hidden = outputs[0]    # (batch_size, seq_len, hidden_size)\n",
        "        country_hidden_total = torch.sum(last_hidden*country_mask.unsqueeze(-1), dim=1).squeeze()# (batch_size, hidden_size)\n",
        "        country_hidden_n = torch.sum(country_mask, dim=-1).view(last_hidden.size(0), 1)\n",
        "        country_hidden = country_hidden_total/country_hidden_n\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "        logits = self.softmax(self.fc(country_hidden))    # (batch_size, 5)\n",
        "        # print(logits.size())\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mazqojfp0CgU"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpiwqItv485-"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class Country_purpose_dataset(Dataset):\n",
        "    def __init__(self, training=True, entire=False):\n",
        "\n",
        "        self.preprocess_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        if training:\n",
        "            if entire:\n",
        "                with open('purpose_classification_new/training_idx_target.json', 'r') as f:\n",
        "                    self.idx_target = json.load(f)\n",
        "                with open('purpose_classification_new/eval_idx_target.json', 'r') as f:\n",
        "                    mode_idx_target = json.load(f) \n",
        "                for idx, t in mode_idx_target:\n",
        "                    self.idx_target.append((idx, t))\n",
        "            else:\n",
        "                with open('purpose_classification_new/training_idx_target.json', 'r') as f:\n",
        "                    self.idx_target = json.load(f)\n",
        "        else:\n",
        "            if entire:\n",
        "                pass \n",
        "            else:\n",
        "                with open('purpose_classification_new/eval_idx_target.json', 'r') as f:\n",
        "                    self.idx_target = json.load(f)\n",
        "\n",
        "\n",
        "        \n",
        "        print('DATA AMOUNT:', len(self.idx_target))\n",
        "            \n",
        "        # self.idx_target = list(self.data) # data_idx, target\n",
        "\n",
        "        path2newfiles = 'purpose_classification'\n",
        "        file_handle = open('/'.join([path2newfiles, 'global_country_idx2data.json']), 'r')\n",
        "        self.idx2data = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        file_handle = open('/'.join([path2newfiles, 'common_country_sample_idx.json']), 'r')\n",
        "        self.common_country_sample_idx = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        file_handle = open('/'.join([path2newfiles, 'global_country_token.json']), 'r')\n",
        "        self.token = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        file_handle = open('/'.join([path2newfiles, 'global_country_data.json']), 'r')\n",
        "        self.pos = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        file_handle = open('/'.join([path2newfiles, 'global_country2idx.json']), 'r')\n",
        "        self.global_country2idx = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        # get full list of country for each cat_name+row_idx\n",
        "        done = [\"reviews_Musical_Instruments_5\",  \"reviews_Automotive_5\",\n",
        "                \"reviews_Cell_Phones_and_Accessories_5\", \"reviews_Toys_and_Games_5\", \n",
        "                \"reviews_Office_Products_5\", \"reviews_Pet_Supplies_5\",\n",
        "                \"reviews_Digital_Music_5\", 'reviews_Tools_and_Home_Improvement_5',\n",
        "                \"reviews_Clothing_Shoes_and_Jewelry_5\", \"reviews_Amazon_Instant_Video_5\"]\n",
        "\n",
        "        find_complete_country_list = {}\n",
        "        incomplete_data = []\n",
        "        for index, target in self.idx_target:\n",
        "            info = self.idx2data[index]\n",
        "            text = self.token[str(info['category'])][str(info['row_idx'])][1]\n",
        "            incomplete_data.append((index, target, text, info))\n",
        "            find_complete_country_list[(info['category'], int(info['row_idx']))] = 0\n",
        "        \n",
        "        for name in done:\n",
        "            file_path = '/'.join([name+'_all', name])\n",
        "            for row_idx, data in enumerate(open(file_path+'_feature.json', 'r')):\n",
        "                if (name, row_idx) in find_complete_country_list:\n",
        "                    feature = json.loads(data)\n",
        "                    assert len(feature['COUNTRY']) > 0\n",
        "                    find_complete_country_list[(name, row_idx)] = feature['COUNTRY']\n",
        "        \n",
        "        # combine all data to self.data\n",
        "        raw_data = []\n",
        "        skip_list = []\n",
        "        skip_c = 0\n",
        "        for data in incomplete_data:\n",
        "            index, target, text, info = data\n",
        "            assert len(find_complete_country_list[(info['category'], int(info['row_idx']))]) != 0\n",
        "            country_list = find_complete_country_list[(info['category'], info['row_idx'])]\n",
        "\n",
        "            # if  self.pos[str(index)]['end'] >= 400 or int(info['country_idx']) >= 6 or index in ['22154', '26418', '29455', '7214', '500', '7284', '24013', '6981', '2850']:\n",
        "            if self.pos[str(index)]['end'] >= 400 or index in ['22154', '26418', '29455', '7214', '500', '7284', '24013', '6981', '2850']:\n",
        "                skip_list.append(index)\n",
        "                skip_c += 1\n",
        "                continue\n",
        "            raw_data.append((index, target, text, info, country_list, self.pos[str(index)]['start'], self.pos[str(index)]['end'])) # data_idx, target, text, info(info['country_idx'], info['country_name']), country_list, start, end\n",
        "        print(skip_c)\n",
        "\n",
        "        self.data = []\n",
        "        for r_d in raw_data:\n",
        "            if self.check_if_tokenize_can_find_country(r_d, r_d[2]):\n",
        "                self.data.append(r_d)\n",
        "            else:\n",
        "                skip_c += 1\n",
        "                skip_list.append(r_d[0])\n",
        "        \n",
        "        print('SKIP NUMBER', skip_c)\n",
        "        print('SKIP LIST', skip_list)\n",
        "\n",
        "        if training:\n",
        "            if entire:\n",
        "                with open('purpose_classification_new/entire_training_complete_data.json', 'w') as f:\n",
        "                    json.dump(self.data, f)\n",
        "                with open('purpose_classification_new/entire_training_skipped_data.json', 'w') as f:\n",
        "                    json.dump(skip_list, f)\n",
        "            else:\n",
        "                with open('purpose_classification_new/training_complete_data.json', 'w') as f:\n",
        "                    json.dump(self.data, f)\n",
        "                with open('purpose_classification_new/training_skipped_data.json', 'w') as f:\n",
        "                    json.dump(skip_list, f)\n",
        "        else:\n",
        "            if entire:\n",
        "                with open('purpose_classification_new/inference_complete_data.json', 'w') as f:\n",
        "                    json.dump(self.data, f)\n",
        "                with open('purpose_classification_new/inference_skipped_data.json', 'w') as f:\n",
        "                    json.dump(skip_list, f)\n",
        "            else:\n",
        "                with open('purpose_classification_new/eval_complete_data.json', 'w') as f:\n",
        "                    json.dump(self.data, f)\n",
        "                with open('purpose_classification_new/eval_skipped_data.json', 'w') as f:\n",
        "                    json.dump(skip_list, f)\n",
        "    \n",
        "    def check_if_tokenize_can_find_country(self, r_d, text):\n",
        "            d = {}\n",
        "            d['country_name'] = r_d[3]['country_name']\n",
        "            d['country_idx'] = r_d[3]['country_idx']\n",
        "            d['country_list'] = r_d[-3]\n",
        "            d['start'] = r_d[-2]\n",
        "            d['end'] = r_d[-1]\n",
        "            tokens = self.preprocess_tokenizer(text.lower(), return_tensors=\"pt\", padding='max_length', max_length=512, truncation=True)\n",
        "            input_ids, _, _ = tokens['input_ids'], tokens['token_type_ids'], tokens['attention_mask']\n",
        "            start, end = find_country_idx(d, self.preprocess_tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
        "\n",
        "            if start == 1000 and end == 1000:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # (text, target, country_name, country_idx, country_list)\n",
        "        return {'index':self.data[idx][0], 'text':self.data[idx][2], 'target':int(self.data[idx][1])-1, 'country_name':self.data[idx][3]['country_name'], \n",
        "                'country_idx':self.data[idx][3]['country_idx'], 'country_list':self.data[idx][-3], 'start':self.data[idx][-2], 'end':self.data[idx][-1]}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def find_country_idx(d, tokenized_sent):\n",
        "    country_name, country_idx, country_list, start, end = d['country_name'], d['country_idx'], d['country_list'], d['start'], d['end']\n",
        "    words_length = len(country_name.split(' '))\n",
        "    record = []\n",
        "    new_sent = []\n",
        "    # print(tokenized_sent)\n",
        "    for idx, t in enumerate(tokenized_sent):\n",
        "        if idx == 0:\n",
        "            last_t = t\n",
        "            record.append(1)\n",
        "            new_sent.append(t)\n",
        "            continue\n",
        "        \n",
        "        if t.startswith('##'):\n",
        "            record[-1] += 1\n",
        "            new_sent[-1] = new_sent[-1] + t[2:]\n",
        "        else:\n",
        "            record.append(1)\n",
        "            new_sent.append(t)\n",
        "\n",
        "    assert len(record) == len(new_sent)\n",
        "    # print(record)\n",
        "    # print(new_sent)\n",
        "    pointer = 0\n",
        "    start = 0\n",
        "    loop = 0\n",
        "    while 1:\n",
        "        cur_words = country_list[pointer].lower().split(' ')\n",
        "        cur_words_len = len(cur_words)\n",
        "        # print('WORD TO FIND:', cur_words)\n",
        "        for i in range(start, len(new_sent)-cur_words_len, 1):\n",
        "\n",
        "            if len(re.sub(\"[^A-Za-z']+\", '', new_sent[i])) == 0:\n",
        "                continue\n",
        "\n",
        "            w2compare = [re.sub(\"[^A-Za-z']+\", '', new_sent[i])]\n",
        "        \n",
        "            j = i+1\n",
        "            while len(w2compare) < cur_words_len:\n",
        "                if len(re.sub(\"[^A-Za-z']+\", '', new_sent[j])) > 0:\n",
        "                    w2compare.append(new_sent[j])\n",
        "                j += 1\n",
        "            \n",
        "            if w2compare == cur_words:\n",
        "                if country_idx != pointer:\n",
        "                    # print('CURRENT WORD==WORD TO FIND', new_sent[i:j], cur_words, country_idx, pointer, i)\n",
        "                    pointer += 1\n",
        "                    start = i+cur_words_len\n",
        "                    break\n",
        "                else:\n",
        "                    # print('CURRENT WORD==WORD TO FIND and RETURN', new_sent[i:j], cur_words, country_idx, pointer, i)\n",
        "                    start_ = sum(record[:i])\n",
        "                    end_ = sum(record[:j])\n",
        "                    # print(tokenized_sent[start_:end_], new_sent[i:j])\n",
        "                    return start_, end_\n",
        "\n",
        "        if i == len(new_sent)-cur_words_len-1:\n",
        "            print('SKIP')\n",
        "            return 1000, 1000\n",
        "\n",
        "    if start_ == 0 and end_ == 0:\n",
        "        print('ERROR')\n",
        "        import pdb\n",
        "        pdb.set_trace()\n",
        "    \n",
        "    return start_, end_\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
        "def collate_fn(data):\n",
        "    # tokenized sentence batchxsequence x3\n",
        "    # target batchx1\n",
        "    # country_mask batchxsequence\n",
        "    input_ids_list = []\n",
        "    token_type_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    targets = []\n",
        "    country_mask = torch.zeros((len(data), 512))\n",
        "    for idx,d in enumerate(data):\n",
        "        # print(d['target'])\n",
        "        tokens = tokenizer(d['text'].lower(), return_tensors=\"pt\", padding='max_length', max_length=512, truncation=True)\n",
        "        input_ids, token_type_ids, attention_mask = tokens['input_ids'], tokens['token_type_ids'], tokens['attention_mask']\n",
        "        start, end = find_country_idx(d, tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
        "        if start == 1000 and end == 1000:\n",
        "            print('ERROR')\n",
        "            import pdb\n",
        "            pdb.set_trace()\n",
        "        country_mask[idx, start:end] = 1\n",
        "        targets.append(int(d['target']))\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "        input_ids_list.append(input_ids)\n",
        "        token_type_ids_list.append(token_type_ids)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "    # import pdb\n",
        "    # pdb.set_trace()\n",
        "    targets = torch.LongTensor(targets)\n",
        "    input_ids_list = torch.cat(input_ids_list, dim=0)\n",
        "    token_type_ids_list = torch.cat(token_type_ids_list, dim=0)\n",
        "    attention_mask_list = torch.cat(attention_mask_list, dim=0)\n",
        "    \n",
        "    return country_mask, input_ids_list, token_type_ids_list, attention_mask_list, targets\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztqyeyxm0HSy"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1fommzXmUsh",
        "outputId": "e1a0d16a-891e-4dd5-b027-108b9d0f1314"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\r\u001b[K     |██▊                             | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 27.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 24.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 17.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 14.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIOGWxLTZCS1"
      },
      "source": [
        "lr = 0.000001\n",
        "# eps = ? \n",
        "# weight_decay = ? \n",
        "epochs = 50\n",
        "eval_steps = 1\n",
        "output_dir = 'purpose_classification_new/training' # output directory to save the best models "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMSt3osUXEns"
      },
      "source": [
        "# train\n",
        "import logging \n",
        "import torch.optim as optim\n",
        "import os\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "class Trainer: \n",
        "    def __init__(self, model, device, train_dataloader, dev_dataloader):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.dev_dataloader = dev_dataloader\n",
        "        self.criterion = nn.NLLLoss(reduction=\"mean\")\n",
        "        self.training_writter = SummaryWriter(output_dir+'/tensorboard')\n",
        "        self.testing_writter = SummaryWriter(output_dir+'/tensorboard')\n",
        "        self.global_step = 0\n",
        "\n",
        "        \n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        # optimizer = Adamw(params=self.model.parameters(), lr=lr, eps=eps, weight_decay=weight_decay)\n",
        "        optimizer = optim.AdamW(params=self.model.parameters(), lr=lr) \n",
        "        global_step = 0\n",
        "        best_eval_loss = None\n",
        "        train_loss = 0\n",
        "        recall = {0:[0, 0], 1:[0, 0], 2:[0, 0], 3:[0, 0], 4:[0, 0]}\n",
        "        prec = {0:[0, 0], 1:[0, 0], 2:[0, 0], 3:[0, 0], 4:[0, 0]}\n",
        "        for epoch in range(epochs):\n",
        "            print(epoch)\n",
        "\n",
        "            if epoch % eval_steps == 0:\n",
        "                self.model.eval()\n",
        "\n",
        "                eval_loss = 0\n",
        "                eval_recall = {0:[0, 0], 1:[0, 0], 2:[0, 0], 3:[0, 0], 4:[0, 0]}\n",
        "                eval_prec = {0:[0, 0], 1:[0, 0], 2:[0, 0], 3:[0, 0], 4:[0, 0]}\n",
        "                for step, example in enumerate(self.dev_dataloader):\n",
        "                    # get outputs\n",
        "                    country_mask, input_ids, token_type_ids, attention_mask, labels = example\n",
        "                    country_mask = country_mask.to(self.device)\n",
        "                    input_ids = input_ids.to(self.device)\n",
        "                    token_type_ids = token_type_ids.to(self.device)\n",
        "                    attention_mask = attention_mask.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "                    outputs = self.model(country_mask=country_mask, input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "                    # print(labels.size())\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                    eval_loss += loss.item()\n",
        "                    global_step += 1\n",
        "\n",
        "                    eval_recall, eval_prec = updatemetics(eval_recall, eval_prec, outputs.data.cpu().numpy(), labels.data.cpu().numpy(), False)\n",
        "\n",
        "                    del country_mask, input_ids, token_type_ids, attention_mask, labels\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "                for k, item in eval_recall.items():\n",
        "                    if item[1] != 0:\n",
        "                        print('eval recall class%d:'%k, item[0]/item[1], end=' ')\n",
        "                print('\\n')\n",
        "\n",
        "                for k, item in eval_prec.items():\n",
        "                    if item[1] != 0:\n",
        "                        print('eval prec class%d:'%k, item[0]/item[1], end=' ')\n",
        "                print('\\n')\n",
        "\n",
        "                for k, _ in prec.items():\n",
        "                    if eval_recall[k][1] != 0 and eval_prec[k][1] != 0:\n",
        "                        recall_ = eval_recall[k][0]/eval_recall[k][1]\n",
        "                        prec_ = eval_prec[k][0]/eval_prec[k][1]\n",
        "                        if recall_+prec_!= 0:\n",
        "                            print('eval f1 class%d:'%k, 2*recall_*prec_/(recall_+prec_), end=' ')\n",
        "                print('\\n')\n",
        "                eval_loss /= step\n",
        "                print('eval LOSS:', eval_loss)\n",
        "                self.testing_writter.add_scalar('loss', eval_loss, self.global_step)\n",
        "                \n",
        "                if best_eval_loss is None or eval_loss < best_eval_loss:\n",
        "                    logging.info(f\"Saving model with eval_loss = {eval_loss}...\\n\")\n",
        "                    torch.save({\n",
        "                        \"recall\": recall,\n",
        "                        \"prec\": prec,\n",
        "                        \"eval_recall\": eval_recall,\n",
        "                        \"eval_prec\": eval_prec,\n",
        "                        \"epoch\": epoch,\n",
        "                        \"step\": step,\n",
        "                        \"global_step\": global_step,\n",
        "                        \"optimizer\": optimizer.state_dict(),\n",
        "                        \"model\": self.model.state_dict(),\n",
        "                        \"train_loss\": train_loss,\n",
        "                        \"eval_loss\": best_eval_loss\n",
        "                    }, os.path.join(output_dir, f\"epoch{epoch}_global-step{global_step}\"))\n",
        "                    best_eval_loss = eval_loss\n",
        "                logging.info(f\"epoch = {epoch}, step = {step}\\n\")\n",
        "                logging.info(f\"train_loss = {train_loss}\\n\")\n",
        "                logging.info(f\"eval_loss = {eval_loss}\\n\")\n",
        "                print(f\"epoch{epoch}_global-step{global_step}\", ' SAVED ')\n",
        "                self.model.train()\n",
        "\n",
        "\n",
        "            train_loss = 0\n",
        "            recall = {0:[0, 0], 1:[0, 0], 2:[0, 0], 3:[0, 0], 4:[0, 0]}\n",
        "            prec = {0:[0, 0], 1:[0, 0], 2:[0, 0], 3:[0, 0], 4:[0, 0]}\n",
        "            for step, example in enumerate(self.train_dataloader):\n",
        "                # get outputs\n",
        "                country_mask, input_ids, token_type_ids, attention_mask, labels = example\n",
        "                country_mask = country_mask.to(self.device)\n",
        "                input_ids = input_ids.to(self.device)\n",
        "                token_type_ids = token_type_ids.to(self.device)\n",
        "                attention_mask = attention_mask.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                outputs = self.model(country_mask=country_mask, input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "                # print(labels.size())\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # back prop & update variables\n",
        "                # if grad_accumulation_steps > 1:\n",
        "                #     loss = loss / grad_accumulation_steps\n",
        "                loss.backward()\n",
        "                # if (step + 1) % grad_accumulation_steps == 0:)\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                train_loss += loss.item()\n",
        "                global_step += 1\n",
        "\n",
        "                recall, prec = updatemetics(recall, prec, outputs.data.cpu().numpy(), labels.data.cpu().numpy(), False)\n",
        " \n",
        "\n",
        "                del country_mask, input_ids, token_type_ids, attention_mask, labels\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "\n",
        "                if (step+1)%100 == 0:\n",
        "                    print('TRAINIG LOSS:', step, train_loss/(step+1))\n",
        "                    for k, item in recall.items():\n",
        "                        if item[1] != 0:\n",
        "                            print('TRAING recall class%d:'%k, item[0]/item[1], end=' ')\n",
        "                    print('\\n')\n",
        "\n",
        "                    for k, item in prec.items():\n",
        "                        if item[1] != 0:\n",
        "                            print('TRAING prec class%d:'%k, item[0]/item[1], end=' ')\n",
        "                    print('\\n')\n",
        "\n",
        "                    for k, _ in prec.items():\n",
        "                        if recall[k][1] != 0 and prec[k][1] != 0:\n",
        "                            recall_ = recall[k][0]/recall[k][1]\n",
        "                            prec_ = prec[k][0]/prec[k][1]\n",
        "                            if recall_+prec_!= 0:\n",
        "                                print('TRAING f1 class%d:'%k, 2*recall_*prec_/(recall_+prec_), end=' ')\n",
        "                    print('\\n')\n",
        "\n",
        "                self.training_writter.add_scalar('loss', loss.item(), self.global_step)\n",
        "\n",
        "                self.global_step += 1\n",
        "            \n",
        "\n",
        "                # do eval\n",
        "            # if epoch % eval_steps == 0 or step == len(self.dev_dataloader):\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IotKWJEprcF8"
      },
      "source": [
        "## metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oUwyWYzreUN"
      },
      "source": [
        "def updatemetics(recall, prec, outputs, labels, print=True):\n",
        "    max_outputs = np.argmax(outputs, axis=-1)\n",
        "    for data in zip(max_outputs, labels):\n",
        "        out, l = data\n",
        "        if out == l:\n",
        "            recall[l][0] += 1\n",
        "            recall[l][1] += 1\n",
        "        else:\n",
        "            recall[l][1] += 1\n",
        "\n",
        "        if out == l:\n",
        "            prec[out][0] += 1\n",
        "            prec[out][1] += 1\n",
        "        else:\n",
        "            prec[out][1] += 1\n",
        "    \n",
        "    if print:\n",
        "        for k, item in recall.items():\n",
        "            if item[1] != 0:\n",
        "                print('recall class%d:'%k, item[0]/item[1], end=' ')\n",
        "        print('\\n')\n",
        "\n",
        "        for k, item in prec.items():\n",
        "            if item[1] != 0:\n",
        "                print('prec class%d:'%k, item[0]/item[1], end=' ')\n",
        "        print('\\n')\n",
        "\n",
        "        for k, _ in prec.items():\n",
        "            if recall[k][1] != 0 and prec[k][1] != 0:\n",
        "                recall_ = recall[k][0]/recall[k][1]\n",
        "                prec_ = prec[k][0]/prec[k][1]\n",
        "                if recall_+prec_!= 0:\n",
        "                    print('f1 class%d:'%k, 2*recall_*prec_/(recall_+prec_), end=' ')\n",
        "        print('\\n')\n",
        "    \n",
        "    return recall, prec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IymeyyJl0RUm"
      },
      "source": [
        "## main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEt3GYVRy2_x",
        "outputId": "849facff-325c-46bc-850a-57a8039f9df9"
      },
      "source": [
        "dataset_training = Country_purpose_dataset(training = True)\n",
        "dataset_eval = Country_purpose_dataset(training = False)\n",
        "batch_size = 4\n",
        "\n",
        "training_data_loader = DataLoader(dataset=dataset_training, batch_size=batch_size, \n",
        "                               shuffle=True, collate_fn=collate_fn)\n",
        "eval_data_loader = DataLoader(dataset=dataset_eval, batch_size=batch_size, \n",
        "                               shuffle=False, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA AMOUNT: 2258\n",
            "167\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP NUMBER 170\n",
            "SKIP LIST ['21435', '4108', '4452', '25269', '27425', '29490', '31836', '1167', '15614', '2558', '4289', '35257', '19871', '16333', '17670', '27969', '2258', '14885', '1253', '29480', '23742', '28342', '21413', '38283', '10118', '28366', '20145', '33432', '5133', '19372', '17471', '33883', '24771', '20039', '144', '26823', '14531', '33615', '13859', '22562', '21732', '17122', '12961', '17224', '25342', '16877', '6422', '33437', '24935', '31734', '21695', '19579', '19852', '1226', '15936', '38656', '319', '22942', '37233', '38479', '28615', '27836', '16243', '27204', '26418', '33616', '39152', '38771', '2505', '13658', '21152', '39431', '4151', '37171', '28628', '25225', '6879', '27790', '22558', '6263', '24363', '26791', '2866', '1380', '22940', '4161', '6627', '37193', '275', '7214', '7284', '38211', '16982', '2809', '28064', '31212', '1168', '13986', '27726', '624', '20444', '24642', '1228', '27203', '26486', '8842', '38080', '6981', '22154', '20984', '13429', '27205', '18178', '26288', '24013', '21938', '3264', '318', '29108', '21441', '26495', '148', '24469', '26897', '2219', '16717', '2142', '26963', '28336', '1069', '22941', '32550', '22335', '38980', '24068', '32605', '3376', '2850', '366', '20317', '3327', '23210', '5063', '9013', '10587', '38326', '29728', '28487', '31537', '22391', '33876', '29455', '34075', '27232', '15881', '21415', '316', '30071', '25335', '20038', '34339', '26790', '24805', '320', '3860', '13369', '19847', '22019', '3977', '27503']\n",
            "DATA AMOUNT: 400\n",
            "28\n",
            "SKIP NUMBER 28\n",
            "SKIP LIST ['29762', '21463', '23121', '2825', '5044', '365', '23427', '2610', '7758', '4277', '24439', '182', '2837', '22092', '37207', '22086', '29814', '33380', '27107', '22613', '7065', '23639', '19160', '2170', '22995', '31201', '500', '23394']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XUGdq91z7lJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5491d34a-1c9d-40df-d47c-7f33ab9a5b5c"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JOMHfVAz-AF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e96c4fd0-fd76-4338-ae9c-a3aaf04b615d"
      },
      "source": [
        "bert = HDCTModel.from_pretrained('bert-base-uncased').to(device)\n",
        "# bert = HDCTModel(hsize = 600)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of HDCTModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.fc.weight', 'bert.fc.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRoTfNUVMLeh",
        "outputId": "829c04d3-c2b7-4834-f09e-a6cfc59c3736"
      },
      "source": [
        "bert1 = HDCTModel.from_pretrained('bert-base-uncased').to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of HDCTModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.fc.weight', 'bert.fc.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p-DL_lRka28"
      },
      "source": [
        "trainer = Trainer(bert, device, training_data_loader, eval_data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlGnC3UOujfS"
      },
      "source": [
        "        # country_hidden_total = torch.sum(last_hidden*country_mask.unsqueeze(-1), dim=1).squeeze()# (batch_size, hidden_size)\n",
        "        # country_hidden_n = torch.sum(country_mask, dim=-1).view(last_hidden.size(0), 1)\n",
        "        # country_hidden = country_hidden_total/country_hidden_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lDux3dSJkgX7",
        "outputId": "b6ac5c7f-324a-484e-d439-2a9e1e86835b"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(lr)\n",
        "trainer.train()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1e-06\n",
            "0\n",
            "eval recall class0: 0.047619047619047616 eval recall class1: 0.04838709677419355 eval recall class2: 0.5094339622641509 eval recall class3: 0.25 eval recall class4: 0.06666666666666667 \n",
            "\n",
            "eval prec class0: 0.20408163265306123 eval prec class1: 0.3333333333333333 eval prec class2: 0.14210526315789473 eval prec class3: 0.07920792079207921 eval prec class4: 0.043478260869565216 \n",
            "\n",
            "eval f1 class0: 0.07722007722007722 eval f1 class1: 0.08450704225352113 eval f1 class2: 0.2222222222222222 eval f1 class3: 0.12030075187969924 eval f1 class4: 0.052631578947368425 \n",
            "\n",
            "eval LOSS: 1.726101217062577\n",
            "epoch0_global-step93  SAVED \n",
            "TRAINIG LOSS: 99 1.4882776629924774\n",
            "TRAING recall class0: 0.65625 TRAING recall class1: 0.05 TRAING recall class2: 0.11290322580645161 TRAING recall class3: 0.06818181818181818 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5610687022900763 TRAING prec class1: 0.2727272727272727 TRAING prec class2: 0.08536585365853659 TRAING prec class3: 0.07692307692307693 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6049382716049382 TRAING f1 class1: 0.08450704225352113 TRAING f1 class2: 0.09722222222222222 TRAING f1 class3: 0.07228915662650602 \n",
            "\n",
            "TRAINIG LOSS: 199 1.3742271554470062\n",
            "TRAING recall class0: 0.8253968253968254 TRAING recall class1: 0.023622047244094488 TRAING recall class2: 0.06481481481481481 TRAING recall class3: 0.03125 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5498489425981873 TRAING prec class1: 0.2727272727272727 TRAING prec class2: 0.08536585365853659 TRAING prec class3: 0.07692307692307693 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6600181323662738 TRAING f1 class1: 0.043478260869565216 TRAING f1 class2: 0.07368421052631578 TRAING f1 class3: 0.044444444444444446 \n",
            "\n",
            "TRAINIG LOSS: 299 1.3289083099365235\n",
            "TRAING recall class0: 0.8785046728971962 TRAING recall class1: 0.015151515151515152 TRAING recall class2: 0.04046242774566474 TRAING recall class3: 0.02027027027027027 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5325779036827195 TRAING prec class1: 0.21428571428571427 TRAING prec class2: 0.08536585365853659 TRAING prec class3: 0.07692307692307693 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6631393298059964 TRAING f1 class1: 0.028301886792452827 TRAING f1 class2: 0.054901960784313725 TRAING f1 class3: 0.03208556149732621 \n",
            "\n",
            "TRAINIG LOSS: 399 1.2854437343776226\n",
            "TRAING recall class0: 0.9081272084805654 TRAING recall class1: 0.02527075812274368 TRAING recall class2: 0.03125 TRAING recall class3: 0.015151515151515152 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5313576843556168 TRAING prec class1: 0.3333333333333333 TRAING prec class2: 0.08536585365853659 TRAING prec class3: 0.075 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6704347826086957 TRAING f1 class1: 0.04697986577181208 TRAING f1 class2: 0.04575163398692811 TRAING f1 class3: 0.025210084033613446 \n",
            "\n",
            "TRAINIG LOSS: 499 1.2507223425507545\n",
            "TRAING recall class0: 0.9251184834123223 TRAING recall class1: 0.02881844380403458 TRAING recall class2: 0.024475524475524476 TRAING recall class3: 0.012295081967213115 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.530146659424226 TRAING prec class1: 0.3448275862068966 TRAING prec class2: 0.08433734939759036 TRAING prec class3: 0.07317073170731707 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6740331491712709 TRAING f1 class1: 0.05319148936170212 TRAING f1 class2: 0.03794037940379404 TRAING f1 class3: 0.021052631578947368 \n",
            "\n",
            "1\n",
            "eval recall class0: 0.9904761904761905 eval recall class1: 0.12903225806451613 eval recall class2: 0.0 eval recall class3: 0.0 eval recall class4: 0.0 \n",
            "\n",
            "eval prec class0: 0.5875706214689266 eval prec class1: 0.5 eval prec class2: 0.0 \n",
            "\n",
            "eval f1 class0: 0.7375886524822696 eval f1 class1: 0.20512820512820512 \n",
            "\n",
            "eval LOSS: 1.0216875795436942\n",
            "epoch1_global-step708  SAVED \n",
            "TRAINIG LOSS: 99 1.0820041039586068\n",
            "TRAING recall class0: 0.9696969696969697 TRAING recall class1: 0.2222222222222222 TRAING recall class2: 0.02 TRAING recall class3: 0.0 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5378151260504201 TRAING prec class1: 0.43902439024390244 TRAING prec class2: 1.0 TRAING prec class3: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6918918918918919 TRAING f1 class1: 0.2950819672131147 TRAING f1 class2: 0.0392156862745098 \n",
            "\n",
            "TRAINIG LOSS: 199 1.0476533584296703\n",
            "TRAING recall class0: 0.9709443099273608 TRAING recall class1: 0.25874125874125875 TRAING recall class2: 0.018867924528301886 TRAING recall class3: 0.03773584905660377 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5736766809728183 TRAING prec class1: 0.4457831325301205 TRAING prec class2: 0.3333333333333333 TRAING prec class3: 0.3333333333333333 \n",
            "\n",
            "TRAING f1 class0: 0.7212230215827338 TRAING f1 class1: 0.3274336283185841 TRAING f1 class2: 0.03571428571428571 TRAING f1 class3: 0.06779661016949153 \n",
            "\n",
            "TRAINIG LOSS: 299 1.0317788633704186\n",
            "TRAING recall class0: 0.964968152866242 TRAING recall class1: 0.25757575757575757 TRAING recall class2: 0.023121387283236993 TRAING recall class3: 0.07692307692307693 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.584942084942085 TRAING prec class1: 0.4112903225806452 TRAING prec class2: 0.26666666666666666 TRAING prec class3: 0.48 \n",
            "\n",
            "TRAING f1 class0: 0.7283653846153847 TRAING f1 class1: 0.3167701863354037 TRAING f1 class2: 0.0425531914893617 TRAING f1 class3: 0.13259668508287295 \n",
            "\n",
            "TRAINIG LOSS: 399 1.0093593418225646\n",
            "TRAING recall class0: 0.9658421672555948 TRAING recall class1: 0.2924187725631769 TRAING recall class2: 0.0410958904109589 TRAING recall class3: 0.08 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.6038291605301914 TRAING prec class1: 0.44751381215469616 TRAING prec class2: 0.36 TRAING prec class3: 0.4444444444444444 \n",
            "\n",
            "TRAING f1 class0: 0.7430901676483915 TRAING f1 class1: 0.3537117903930131 TRAING f1 class2: 0.07377049180327869 TRAING f1 class3: 0.13559322033898308 \n",
            "\n",
            "TRAINIG LOSS: 499 1.006850472956896\n",
            "TRAING recall class0: 0.9646946564885496 TRAING recall class1: 0.3314285714285714 TRAING recall class2: 0.05693950177935943 TRAING recall class3: 0.07114624505928854 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.6094032549728752 TRAING prec class1: 0.4566929133858268 TRAING prec class2: 0.38095238095238093 TRAING prec class3: 0.4 \n",
            "\n",
            "TRAING f1 class0: 0.7469523457702253 TRAING f1 class1: 0.3841059602649006 TRAING f1 class2: 0.09907120743034056 TRAING f1 class3: 0.12080536912751677 \n",
            "\n",
            "2\n",
            "eval recall class0: 0.9333333333333333 eval recall class1: 0.3225806451612903 eval recall class2: 0.1320754716981132 eval recall class3: 0.09375 eval recall class4: 0.0 \n",
            "\n",
            "eval prec class0: 0.6901408450704225 eval prec class1: 0.3508771929824561 eval prec class2: 0.3888888888888889 eval prec class3: 0.23076923076923078 \n",
            "\n",
            "eval f1 class0: 0.7935222672064778 eval f1 class1: 0.33613445378151263 eval f1 class2: 0.19718309859154928 eval f1 class3: 0.13333333333333333 \n",
            "\n",
            "eval LOSS: 0.90708419850663\n",
            "epoch2_global-step1323  SAVED \n",
            "TRAINIG LOSS: 99 0.8826390191167593\n",
            "TRAING recall class0: 0.9483568075117371 TRAING recall class1: 0.4857142857142857 TRAING recall class2: 0.17857142857142858 TRAING recall class3: 0.06 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.7163120567375887 TRAING prec class1: 0.4358974358974359 TRAING prec class2: 0.35714285714285715 TRAING prec class3: 0.25 \n",
            "\n",
            "TRAING f1 class0: 0.8161616161616162 TRAING f1 class1: 0.45945945945945943 TRAING f1 class2: 0.2380952380952381 TRAING f1 class3: 0.0967741935483871 \n",
            "\n",
            "TRAINIG LOSS: 199 0.8960544074326754\n",
            "TRAING recall class0: 0.9347826086956522 TRAING recall class1: 0.5724137931034483 TRAING recall class2: 0.19047619047619047 TRAING recall class3: 0.12612612612612611 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.730188679245283 TRAING prec class1: 0.42783505154639173 TRAING prec class2: 0.40816326530612246 TRAING prec class3: 0.5185185185185185 \n",
            "\n",
            "TRAING f1 class0: 0.8199152542372882 TRAING f1 class1: 0.48967551622418876 TRAING f1 class2: 0.25974025974025977 TRAING f1 class3: 0.2028985507246377 \n",
            "\n",
            "TRAINIG LOSS: 299 0.9042763686180115\n",
            "TRAING recall class0: 0.9216 TRAING recall class1: 0.5630630630630631 TRAING recall class2: 0.22981366459627328 TRAING recall class3: 0.16339869281045752 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.7346938775510204 TRAING prec class1: 0.4416961130742049 TRAING prec class2: 0.44047619047619047 TRAING prec class3: 0.5102040816326531 \n",
            "\n",
            "TRAING f1 class0: 0.8176011355571328 TRAING f1 class1: 0.49504950495049505 TRAING f1 class2: 0.3020408163265306 TRAING f1 class3: 0.24752475247524752 \n",
            "\n",
            "TRAINIG LOSS: 399 0.9041435349360109\n",
            "TRAING recall class0: 0.9191438763376932 TRAING recall class1: 0.5618374558303887 TRAING recall class2: 0.24778761061946902 TRAING recall class3: 0.1683673469387755 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.7447013487475915 TRAING prec class1: 0.43561643835616437 TRAING prec class2: 0.4444444444444444 TRAING prec class3: 0.4647887323943662 \n",
            "\n",
            "TRAING f1 class0: 0.8227780734433209 TRAING f1 class1: 0.4907407407407407 TRAING f1 class2: 0.3181818181818182 TRAING f1 class3: 0.24719101123595508 \n",
            "\n",
            "TRAINIG LOSS: 499 0.9090983573794365\n",
            "TRAING recall class0: 0.9131679389312977 TRAING recall class1: 0.523943661971831 TRAING recall class2: 0.2826855123674912 TRAING recall class3: 0.17479674796747968 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.7529504327301337 TRAING prec class1: 0.4189189189189189 TRAING prec class2: 0.43956043956043955 TRAING prec class3: 0.4174757281553398 \n",
            "\n",
            "TRAING f1 class0: 0.8253557567917206 TRAING f1 class1: 0.4655819774718398 TRAING f1 class2: 0.3440860215053763 TRAING f1 class3: 0.2464183381088825 \n",
            "\n",
            "3\n",
            "eval recall class0: 0.9 eval recall class1: 0.4032258064516129 eval recall class2: 0.41509433962264153 eval recall class3: 0.15625 eval recall class4: 0.0 \n",
            "\n",
            "eval prec class0: 0.7777777777777778 eval prec class1: 0.43859649122807015 eval prec class2: 0.4583333333333333 eval prec class3: 0.20833333333333334 \n",
            "\n",
            "eval f1 class0: 0.8344370860927153 eval f1 class1: 0.42016806722689076 eval f1 class2: 0.43564356435643564 eval f1 class3: 0.17857142857142858 \n",
            "\n",
            "eval LOSS: 0.8469707557283666\n",
            "epoch3_global-step1938  SAVED \n",
            "TRAINIG LOSS: 99 0.8602717454731464\n",
            "TRAING recall class0: 0.9138755980861244 TRAING recall class1: 0.49333333333333335 TRAING recall class2: 0.5283018867924528 TRAING recall class3: 0.2978723404255319 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8127659574468085 TRAING prec class1: 0.4868421052631579 TRAING prec class2: 0.509090909090909 TRAING prec class3: 0.4117647058823529 \n",
            "\n",
            "TRAING f1 class0: 0.8603603603603603 TRAING f1 class1: 0.4900662251655629 TRAING f1 class2: 0.5185185185185185 TRAING f1 class3: 0.345679012345679 \n",
            "\n",
            "TRAINIG LOSS: 199 0.8426956721395255\n",
            "TRAING recall class0: 0.9065420560747663 TRAING recall class1: 0.496 TRAING recall class2: 0.5084745762711864 TRAING recall class3: 0.2857142857142857 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.825531914893617 TRAING prec class1: 0.39490445859872614 TRAING prec class2: 0.5454545454545454 TRAING prec class3: 0.4444444444444444 \n",
            "\n",
            "TRAING f1 class0: 0.8641425389755012 TRAING f1 class1: 0.43971631205673756 TRAING f1 class2: 0.5263157894736842 TRAING f1 class3: 0.34782608695652173 \n",
            "\n",
            "TRAINIG LOSS: 299 0.8587755224605401\n",
            "TRAING recall class0: 0.8942307692307693 TRAING recall class1: 0.4950980392156863 TRAING recall class2: 0.510752688172043 TRAING recall class3: 0.3402777777777778 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8193832599118943 TRAING prec class1: 0.4279661016949153 TRAING prec class2: 0.5277777777777778 TRAING prec class3: 0.47572815533980584 \n",
            "\n",
            "TRAING f1 class0: 0.8551724137931035 TRAING f1 class1: 0.4590909090909091 TRAING f1 class2: 0.5191256830601094 TRAING f1 class3: 0.3967611336032389 \n",
            "\n",
            "TRAINIG LOSS: 399 0.8445025713555515\n",
            "TRAING recall class0: 0.8965102286401926 TRAING recall class1: 0.5124555160142349 TRAING recall class2: 0.5042016806722689 TRAING recall class3: 0.3197969543147208 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.830546265328874 TRAING prec class1: 0.4458204334365325 TRAING prec class2: 0.4819277108433735 TRAING prec class3: 0.48091603053435117 \n",
            "\n",
            "TRAING f1 class0: 0.8622685185185185 TRAING f1 class1: 0.4768211920529802 TRAING f1 class2: 0.4928131416837782 TRAING f1 class3: 0.3841463414634147 \n",
            "\n",
            "TRAINIG LOSS: 499 0.8338557178899646\n",
            "TRAING recall class0: 0.8990476190476191 TRAING recall class1: 0.5272206303724928 TRAING recall class2: 0.512280701754386 TRAING recall class3: 0.3253012048192771 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8331862312444837 TRAING prec class1: 0.4577114427860697 TRAING prec class2: 0.4850498338870432 TRAING prec class3: 0.49390243902439024 \n",
            "\n",
            "TRAING f1 class0: 0.8648648648648648 TRAING f1 class1: 0.4900133155792277 TRAING f1 class2: 0.49829351535836175 TRAING f1 class3: 0.39225181598062947 \n",
            "\n",
            "4\n",
            "eval recall class0: 0.8952380952380953 eval recall class1: 0.5 eval recall class2: 0.49056603773584906 eval recall class3: 0.1875 eval recall class4: 0.0 \n",
            "\n",
            "eval prec class0: 0.8506787330316742 eval prec class1: 0.43661971830985913 eval prec class2: 0.5531914893617021 eval prec class3: 0.18181818181818182 \n",
            "\n",
            "eval f1 class0: 0.8723897911832946 eval f1 class1: 0.46616541353383456 eval f1 class2: 0.52 eval f1 class3: 0.1846153846153846 \n",
            "\n",
            "eval LOSS: 0.8174733670387903\n",
            "epoch4_global-step2553  SAVED \n",
            "TRAINIG LOSS: 99 0.7834952795505523\n",
            "TRAING recall class0: 0.8867924528301887 TRAING recall class1: 0.603448275862069 TRAING recall class2: 0.5079365079365079 TRAING recall class3: 0.4 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8703703703703703 TRAING prec class1: 0.4268292682926829 TRAING prec class2: 0.5714285714285714 TRAING prec class3: 0.4782608695652174 \n",
            "\n",
            "TRAING f1 class0: 0.8785046728971964 TRAING f1 class1: 0.5 TRAING f1 class2: 0.5378151260504201 TRAING f1 class3: 0.43564356435643564 \n",
            "\n",
            "TRAINIG LOSS: 199 0.807340735103935\n",
            "TRAING recall class0: 0.8861386138613861 TRAING recall class1: 0.5539568345323741 TRAING recall class2: 0.5371900826446281 TRAING recall class3: 0.44144144144144143 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8364485981308412 TRAING prec class1: 0.4935897435897436 TRAING prec class2: 0.5603448275862069 TRAING prec class3: 0.49 \n",
            "\n",
            "TRAING f1 class0: 0.8605769230769231 TRAING f1 class1: 0.5220338983050847 TRAING f1 class2: 0.5485232067510548 TRAING f1 class3: 0.46445497630331756 \n",
            "\n",
            "TRAINIG LOSS: 299 0.804748061572512\n",
            "TRAING recall class0: 0.8932676518883416 TRAING recall class1: 0.5339366515837104 TRAING recall class2: 0.5454545454545454 TRAING recall class3: 0.42857142857142855 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8395061728395061 TRAING prec class1: 0.49372384937238495 TRAING prec class2: 0.5325443786982249 TRAING prec class3: 0.5 \n",
            "\n",
            "TRAING f1 class0: 0.8655529037390612 TRAING f1 class1: 0.5130434782608696 TRAING f1 class2: 0.5389221556886228 TRAING f1 class3: 0.4615384615384615 \n",
            "\n",
            "TRAINIG LOSS: 399 0.7789529010141268\n",
            "TRAING recall class0: 0.9016786570743405 TRAING recall class1: 0.5400696864111498 TRAING recall class2: 0.5565610859728507 TRAING recall class3: 0.42028985507246375 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8574686431014823 TRAING prec class1: 0.4725609756097561 TRAING prec class2: 0.5694444444444444 TRAING prec class3: 0.4860335195530726 \n",
            "\n",
            "TRAING f1 class0: 0.8790181180596142 TRAING f1 class1: 0.5040650406504066 TRAING f1 class2: 0.562929061784897 TRAING f1 class3: 0.45077720207253885 \n",
            "\n",
            "TRAINIG LOSS: 499 0.782931963827461\n",
            "TRAING recall class0: 0.8993288590604027 TRAING recall class1: 0.5270655270655271 TRAING recall class2: 0.5774647887323944 TRAING recall class3: 0.4094488188976378 TRAING recall class4: 0.014705882352941176 \n",
            "\n",
            "TRAING prec class0: 0.8589743589743589 TRAING prec class1: 0.46954314720812185 TRAING prec class2: 0.5503355704697986 TRAING prec class3: 0.48372093023255813 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8786885245901639 TRAING f1 class1: 0.4966442953020134 TRAING f1 class2: 0.563573883161512 TRAING f1 class3: 0.4434968017057569 TRAING f1 class4: 0.028985507246376812 \n",
            "\n",
            "5\n",
            "eval recall class0: 0.8904761904761904 eval recall class1: 0.4838709677419355 eval recall class2: 0.5471698113207547 eval recall class3: 0.1875 eval recall class4: 0.0 \n",
            "\n",
            "eval prec class0: 0.8657407407407407 eval prec class1: 0.4411764705882353 eval prec class2: 0.5272727272727272 eval prec class3: 0.18181818181818182 \n",
            "\n",
            "eval f1 class0: 0.8779342723004694 eval f1 class1: 0.4615384615384615 eval f1 class2: 0.537037037037037 eval f1 class3: 0.1846153846153846 \n",
            "\n",
            "eval LOSS: 0.7930744266258958\n",
            "epoch5_global-step3168  SAVED \n",
            "TRAINIG LOSS: 99 0.7747524609416723\n",
            "TRAING recall class0: 0.8708133971291866 TRAING recall class1: 0.5901639344262295 TRAING recall class2: 0.6440677966101694 TRAING recall class3: 0.4807692307692308 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8625592417061612 TRAING prec class1: 0.45 TRAING prec class2: 0.5588235294117647 TRAING prec class3: 0.6097560975609756 \n",
            "\n",
            "TRAING f1 class0: 0.8666666666666666 TRAING f1 class1: 0.5106382978723405 TRAING f1 class2: 0.5984251968503937 TRAING f1 class3: 0.5376344086021505 \n",
            "\n",
            "TRAINIG LOSS: 199 0.7644612557068468\n",
            "TRAING recall class0: 0.8786407766990292 TRAING recall class1: 0.5815602836879432 TRAING recall class2: 0.6293103448275862 TRAING recall class3: 0.41414141414141414 TRAING recall class4: 0.03125 \n",
            "\n",
            "TRAING prec class0: 0.8829268292682927 TRAING prec class1: 0.4823529411764706 TRAING prec class2: 0.5289855072463768 TRAING prec class3: 0.5061728395061729 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8807785888077859 TRAING f1 class1: 0.5273311897106109 TRAING f1 class2: 0.5748031496062992 TRAING f1 class3: 0.45555555555555555 TRAING f1 class4: 0.06060606060606061 \n",
            "\n",
            "TRAINIG LOSS: 299 0.7437369684192041\n",
            "TRAING recall class0: 0.8918918918918919 TRAING recall class1: 0.5920398009950248 TRAING recall class2: 0.6368715083798883 TRAING recall class3: 0.42142857142857143 TRAING recall class4: 0.0392156862745098 \n",
            "\n",
            "TRAING prec class0: 0.893312101910828 TRAING prec class1: 0.49377593360995853 TRAING prec class2: 0.5402843601895735 TRAING prec class3: 0.5 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8926014319809069 TRAING f1 class1: 0.5384615384615385 TRAING f1 class2: 0.5846153846153846 TRAING f1 class3: 0.4573643410852713 TRAING f1 class4: 0.07547169811320754 \n",
            "\n",
            "TRAINIG LOSS: 399 0.7442306529497728\n",
            "TRAING recall class0: 0.893491124260355 TRAING recall class1: 0.5745454545454546 TRAING recall class2: 0.6371681415929203 TRAING recall class3: 0.38461538461538464 TRAING recall class4: 0.05084745762711865 \n",
            "\n",
            "TRAING prec class0: 0.886150234741784 TRAING prec class1: 0.49221183800623053 TRAING prec class2: 0.5179856115107914 TRAING prec class3: 0.5136986301369864 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8898055391868003 TRAING f1 class1: 0.5302013422818792 TRAING f1 class2: 0.5714285714285714 TRAING f1 class3: 0.43988269794721413 TRAING f1 class4: 0.0967741935483871 \n",
            "\n",
            "TRAINIG LOSS: 499 0.7406477029770613\n",
            "TRAING recall class0: 0.8953377735490009 TRAING recall class1: 0.5823863636363636 TRAING recall class2: 0.6254416961130742 TRAING recall class3: 0.39183673469387753 TRAING recall class4: 0.043478260869565216 \n",
            "\n",
            "TRAING prec class0: 0.8843984962406015 TRAING prec class1: 0.49278846153846156 TRAING prec class2: 0.5283582089552239 TRAING prec class3: 0.5274725274725275 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8898345153664302 TRAING f1 class1: 0.5338541666666666 TRAING f1 class2: 0.5728155339805825 TRAING f1 class3: 0.44964871194379397 TRAING f1 class4: 0.08333333333333333 \n",
            "\n",
            "6\n",
            "eval recall class0: 0.8904761904761904 eval recall class1: 0.4838709677419355 eval recall class2: 0.5660377358490566 eval recall class3: 0.21875 eval recall class4: 0.0 \n",
            "\n",
            "eval prec class0: 0.8738317757009346 eval prec class1: 0.44776119402985076 eval prec class2: 0.5357142857142857 eval prec class3: 0.2 \n",
            "\n",
            "eval f1 class0: 0.8820754716981133 eval f1 class1: 0.4651162790697675 eval f1 class2: 0.5504587155963302 eval f1 class3: 0.20895522388059704 \n",
            "\n",
            "eval LOSS: 0.7827288774854463\n",
            "epoch6_global-step3783  SAVED \n",
            "TRAINIG LOSS: 99 0.6905930368974805\n",
            "TRAING recall class0: 0.8932038834951457 TRAING recall class1: 0.6527777777777778 TRAING recall class2: 0.6274509803921569 TRAING recall class3: 0.5396825396825397 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.9108910891089109 TRAING prec class1: 0.6025641025641025 TRAING prec class2: 0.48484848484848486 TRAING prec class3: 0.6296296296296297 \n",
            "\n",
            "TRAING f1 class0: 0.9019607843137256 TRAING f1 class1: 0.6266666666666667 TRAING f1 class2: 0.5470085470085471 TRAING f1 class3: 0.5811965811965812 \n",
            "\n",
            "TRAINIG LOSS: 199 0.7052199609205126\n",
            "TRAING recall class0: 0.8983050847457628 TRAING recall class1: 0.6058394160583942 TRAING recall class2: 0.6203703703703703 TRAING recall class3: 0.5470085470085471 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.9048780487804878 TRAING prec class1: 0.515527950310559 TRAING prec class2: 0.5403225806451613 TRAING prec class3: 0.6095238095238096 \n",
            "\n",
            "TRAING f1 class0: 0.9015795868772782 TRAING f1 class1: 0.5570469798657718 TRAING f1 class2: 0.5775862068965516 TRAING f1 class3: 0.5765765765765767 \n",
            "\n",
            "TRAINIG LOSS: 299 0.7142364653324087\n",
            "TRAING recall class0: 0.9042207792207793 TRAING recall class1: 0.5915492957746479 TRAING recall class2: 0.6411764705882353 TRAING recall class3: 0.55 TRAING recall class4: 0.04878048780487805 \n",
            "\n",
            "TRAING prec class0: 0.9042207792207793 TRAING prec class1: 0.5526315789473685 TRAING prec class2: 0.5618556701030928 TRAING prec class3: 0.5534591194968553 TRAING prec class4: 0.6666666666666666 \n",
            "\n",
            "TRAING f1 class0: 0.9042207792207793 TRAING f1 class1: 0.5714285714285715 TRAING f1 class2: 0.5989010989010989 TRAING f1 class3: 0.5517241379310345 TRAING f1 class4: 0.0909090909090909 \n",
            "\n",
            "TRAINIG LOSS: 399 0.7064422391355037\n",
            "TRAING recall class0: 0.9063625450180072 TRAING recall class1: 0.5901060070671378 TRAING recall class2: 0.672566371681416 TRAING recall class3: 0.5276381909547738 TRAING recall class4: 0.06779661016949153 \n",
            "\n",
            "TRAING prec class0: 0.9074519230769231 TRAING prec class1: 0.5493421052631579 TRAING prec class2: 0.5650557620817844 TRAING prec class3: 0.5555555555555556 TRAING prec class4: 0.6666666666666666 \n",
            "\n",
            "TRAING f1 class0: 0.9069069069069069 TRAING f1 class1: 0.5689948892674617 TRAING f1 class2: 0.6141414141414141 TRAING f1 class3: 0.5412371134020618 TRAING f1 class4: 0.12307692307692307 \n",
            "\n",
            "TRAINIG LOSS: 499 0.7025195424929261\n",
            "TRAING recall class0: 0.9018112488083889 TRAING recall class1: 0.5949008498583569 TRAING recall class2: 0.6785714285714286 TRAING recall class3: 0.4940239043824701 TRAING recall class4: 0.07462686567164178 \n",
            "\n",
            "TRAING prec class0: 0.9096153846153846 TRAING prec class1: 0.5384615384615384 TRAING prec class2: 0.5621301775147929 TRAING prec class3: 0.5511111111111111 TRAING prec class4: 0.7142857142857143 \n",
            "\n",
            "TRAING f1 class0: 0.9056965055050263 TRAING f1 class1: 0.5652759084791386 TRAING f1 class2: 0.6148867313915857 TRAING f1 class3: 0.5210084033613446 TRAING f1 class4: 0.13513513513513511 \n",
            "\n",
            "7\n",
            "eval recall class0: 0.8857142857142857 eval recall class1: 0.5 eval recall class2: 0.6037735849056604 eval recall class3: 0.21875 eval recall class4: 0.13333333333333333 \n",
            "\n",
            "eval prec class0: 0.8899521531100478 eval prec class1: 0.4626865671641791 eval prec class2: 0.5423728813559322 eval prec class3: 0.2 eval prec class4: 1.0 \n",
            "\n",
            "eval f1 class0: 0.8878281622911695 eval f1 class1: 0.4806201550387597 eval f1 class2: 0.5714285714285714 eval f1 class3: 0.20895522388059704 eval f1 class4: 0.23529411764705882 \n",
            "\n",
            "eval LOSS: 0.7749891490068125\n",
            "epoch7_global-step4398  SAVED \n",
            "TRAINIG LOSS: 99 0.6332971804589033\n",
            "TRAING recall class0: 0.8834080717488789 TRAING recall class1: 0.5932203389830508 TRAING recall class2: 0.7241379310344828 TRAING recall class3: 0.5882352941176471 TRAING recall class4: 0.1111111111111111 \n",
            "\n",
            "TRAING prec class0: 0.9248826291079812 TRAING prec class1: 0.5 TRAING prec class2: 0.6176470588235294 TRAING prec class3: 0.625 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9036697247706422 TRAING f1 class1: 0.5426356589147286 TRAING f1 class2: 0.6666666666666667 TRAING f1 class3: 0.6060606060606061 TRAING f1 class4: 0.19999999999999998 \n",
            "\n",
            "TRAINIG LOSS: 199 0.6345093689020723\n",
            "TRAING recall class0: 0.9038031319910514 TRAING recall class1: 0.609375 TRAING recall class2: 0.6607142857142857 TRAING recall class3: 0.574468085106383 TRAING recall class4: 0.05263157894736842 \n",
            "\n",
            "TRAING prec class0: 0.9160997732426304 TRAING prec class1: 0.5571428571428572 TRAING prec class2: 0.5736434108527132 TRAING prec class3: 0.6067415730337079 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9099099099099099 TRAING f1 class1: 0.582089552238806 TRAING f1 class2: 0.6141078838174274 TRAING f1 class3: 0.5901639344262296 TRAING f1 class4: 0.1 \n",
            "\n",
            "TRAINIG LOSS: 299 0.6587484431887667\n",
            "TRAING recall class0: 0.911042944785276 TRAING recall class1: 0.599009900990099 TRAING recall class2: 0.6809815950920245 TRAING recall class3: 0.5165562913907285 TRAING recall class4: 0.03125 \n",
            "\n",
            "TRAING prec class0: 0.9096477794793262 TRAING prec class1: 0.5525114155251142 TRAING prec class2: 0.5692307692307692 TRAING prec class3: 0.5909090909090909 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9103448275862069 TRAING f1 class1: 0.5748218527315915 TRAING f1 class2: 0.6201117318435755 TRAING f1 class3: 0.5512367491166079 TRAING f1 class4: 0.06060606060606061 \n",
            "\n",
            "TRAINIG LOSS: 399 0.6733651007153094\n",
            "TRAING recall class0: 0.9080188679245284 TRAING recall class1: 0.6036363636363636 TRAING recall class2: 0.6891891891891891 TRAING recall class3: 0.5215311004784688 TRAING recall class4: 0.043478260869565216 \n",
            "\n",
            "TRAING prec class0: 0.9101654846335697 TRAING prec class1: 0.5496688741721855 TRAING prec class2: 0.5773584905660377 TRAING prec class3: 0.592391304347826 TRAING prec class4: 0.6666666666666666 \n",
            "\n",
            "TRAING f1 class0: 0.9090909090909091 TRAING f1 class1: 0.5753899480069324 TRAING f1 class2: 0.6283367556468172 TRAING f1 class3: 0.55470737913486 TRAING f1 class4: 0.08163265306122448 \n",
            "\n",
            "TRAINIG LOSS: 499 0.6719885222390294\n",
            "TRAING recall class0: 0.9077946768060836 TRAING recall class1: 0.6057142857142858 TRAING recall class2: 0.6821428571428572 TRAING recall class3: 0.52 TRAING recall class4: 0.08823529411764706 \n",
            "\n",
            "TRAING prec class0: 0.9095238095238095 TRAING prec class1: 0.5492227979274611 TRAING prec class2: 0.5718562874251497 TRAING prec class3: 0.5829596412556054 TRAING prec class4: 0.8571428571428571 \n",
            "\n",
            "TRAING f1 class0: 0.9086584205518554 TRAING f1 class1: 0.5760869565217391 TRAING f1 class2: 0.6221498371335507 TRAING f1 class3: 0.5496828752642707 TRAING f1 class4: 0.16 \n",
            "\n",
            "8\n",
            "eval recall class0: 0.8857142857142857 eval recall class1: 0.4838709677419355 eval recall class2: 0.5849056603773585 eval recall class3: 0.25 eval recall class4: 0.26666666666666666 \n",
            "\n",
            "eval prec class0: 0.8942307692307693 eval prec class1: 0.43478260869565216 eval prec class2: 0.5535714285714286 eval prec class3: 0.23529411764705882 eval prec class4: 0.8 \n",
            "\n",
            "eval f1 class0: 0.8899521531100478 eval f1 class1: 0.45801526717557256 eval f1 class2: 0.5688073394495413 eval f1 class3: 0.24242424242424243 eval f1 class4: 0.4 \n",
            "\n",
            "eval LOSS: 0.7715023296945931\n",
            "epoch8_global-step5013  SAVED \n",
            "TRAINIG LOSS: 99 0.6242984919622541\n",
            "TRAING recall class0: 0.9174757281553398 TRAING recall class1: 0.6515151515151515 TRAING recall class2: 0.7647058823529411 TRAING recall class3: 0.6290322580645161 TRAING recall class4: 0.26666666666666666 \n",
            "\n",
            "TRAING prec class0: 0.9219512195121952 TRAING prec class1: 0.6056338028169014 TRAING prec class2: 0.7090909090909091 TRAING prec class3: 0.609375 TRAING prec class4: 0.8 \n",
            "\n",
            "TRAING f1 class0: 0.9197080291970803 TRAING f1 class1: 0.6277372262773722 TRAING f1 class2: 0.7358490566037736 TRAING f1 class3: 0.6190476190476191 TRAING f1 class4: 0.4 \n",
            "\n",
            "TRAINIG LOSS: 199 0.6478621211834252\n",
            "TRAING recall class0: 0.910411622276029 TRAING recall class1: 0.6423357664233577 TRAING recall class2: 0.7008547008547008 TRAING recall class3: 0.5660377358490566 TRAING recall class4: 0.25925925925925924 \n",
            "\n",
            "TRAING prec class0: 0.9215686274509803 TRAING prec class1: 0.5827814569536424 TRAING prec class2: 0.6259541984732825 TRAING prec class3: 0.5882352941176471 TRAING prec class4: 0.875 \n",
            "\n",
            "TRAING f1 class0: 0.9159561510353227 TRAING f1 class1: 0.611111111111111 TRAING f1 class2: 0.6612903225806451 TRAING f1 class3: 0.576923076923077 TRAING f1 class4: 0.39999999999999997 \n",
            "\n",
            "TRAINIG LOSS: 299 0.6441346464926998\n",
            "TRAING recall class0: 0.9112903225806451 TRAING recall class1: 0.6519607843137255 TRAING recall class2: 0.7195121951219512 TRAING recall class3: 0.5416666666666666 TRAING recall class4: 0.2727272727272727 \n",
            "\n",
            "TRAING prec class0: 0.9292763157894737 TRAING prec class1: 0.5807860262008734 TRAING prec class2: 0.5989847715736041 TRAING prec class3: 0.6026490066225165 TRAING prec class4: 0.8 \n",
            "\n",
            "TRAING f1 class0: 0.9201954397394136 TRAING f1 class1: 0.6143187066974596 TRAING f1 class2: 0.6537396121883656 TRAING f1 class3: 0.5705329153605015 TRAING f1 class4: 0.4067796610169491 \n",
            "\n",
            "TRAINIG LOSS: 399 0.6532700086990372\n",
            "TRAING recall class0: 0.9075425790754258 TRAING recall class1: 0.6254416961130742 TRAING recall class2: 0.7142857142857143 TRAING recall class3: 0.5462962962962963 TRAING recall class4: 0.2727272727272727 \n",
            "\n",
            "TRAING prec class0: 0.927860696517413 TRAING prec class1: 0.5728155339805825 TRAING prec class2: 0.5970149253731343 TRAING prec class3: 0.5870646766169154 TRAING prec class4: 0.8333333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.917589175891759 TRAING f1 class1: 0.597972972972973 TRAING f1 class2: 0.6504065040650405 TRAING f1 class3: 0.565947242206235 TRAING f1 class4: 0.4109589041095891 \n",
            "\n",
            "TRAINIG LOSS: 499 0.6422141691893339\n",
            "TRAING recall class0: 0.9120458891013384 TRAING recall class1: 0.632183908045977 TRAING recall class2: 0.7117437722419929 TRAING recall class3: 0.5450980392156862 TRAING recall class4: 0.2571428571428571 \n",
            "\n",
            "TRAING prec class0: 0.928919182083739 TRAING prec class1: 0.583554376657825 TRAING prec class2: 0.60790273556231 TRAING prec class3: 0.569672131147541 TRAING prec class4: 0.782608695652174 \n",
            "\n",
            "TRAING f1 class0: 0.9204052098408104 TRAING f1 class1: 0.606896551724138 TRAING f1 class2: 0.6557377049180327 TRAING f1 class3: 0.5571142284569138 TRAING f1 class4: 0.3870967741935484 \n",
            "\n",
            "9\n",
            "eval recall class0: 0.8904761904761904 eval recall class1: 0.5 eval recall class2: 0.6226415094339622 eval recall class3: 0.21875 eval recall class4: 0.3333333333333333 \n",
            "\n",
            "eval prec class0: 0.8904761904761904 eval prec class1: 0.4626865671641791 eval prec class2: 0.55 eval prec class3: 0.2413793103448276 eval prec class4: 0.8333333333333334 \n",
            "\n",
            "eval f1 class0: 0.8904761904761904 eval f1 class1: 0.4806201550387597 eval f1 class2: 0.5840707964601771 eval f1 class3: 0.22950819672131145 eval f1 class4: 0.47619047619047616 \n",
            "\n",
            "eval LOSS: 0.7678929573410879\n",
            "epoch9_global-step5628  SAVED \n",
            "TRAINIG LOSS: 99 0.5618808487802744\n",
            "TRAING recall class0: 0.9285714285714286 TRAING recall class1: 0.6533333333333333 TRAING recall class2: 0.6666666666666666 TRAING recall class3: 0.5208333333333334 TRAING recall class4: 0.45454545454545453 \n",
            "\n",
            "TRAING prec class0: 0.9411764705882353 TRAING prec class1: 0.5975609756097561 TRAING prec class2: 0.5490196078431373 TRAING prec class3: 0.6410256410256411 TRAING prec class4: 0.7142857142857143 \n",
            "\n",
            "TRAING f1 class0: 0.9348314606741573 TRAING f1 class1: 0.6242038216560509 TRAING f1 class2: 0.6021505376344086 TRAING f1 class3: 0.5747126436781609 TRAING f1 class4: 0.5555555555555556 \n",
            "\n",
            "TRAINIG LOSS: 199 0.5698735837265849\n",
            "TRAING recall class0: 0.9212962962962963 TRAING recall class1: 0.756578947368421 TRAING recall class2: 0.7128712871287128 TRAING recall class3: 0.5108695652173914 TRAING recall class4: 0.5217391304347826 \n",
            "\n",
            "TRAING prec class0: 0.9320843091334895 TRAING prec class1: 0.6609195402298851 TRAING prec class2: 0.6153846153846154 TRAING prec class3: 0.6911764705882353 TRAING prec class4: 0.8571428571428571 \n",
            "\n",
            "TRAING f1 class0: 0.9266589057043073 TRAING f1 class1: 0.7055214723926381 TRAING f1 class2: 0.6605504587155964 TRAING f1 class3: 0.5875 TRAING f1 class4: 0.6486486486486486 \n",
            "\n",
            "TRAINIG LOSS: 299 0.5920342691304783\n",
            "TRAING recall class0: 0.9248826291079812 TRAING recall class1: 0.719626168224299 TRAING recall class2: 0.7421383647798742 TRAING recall class3: 0.5100671140939598 TRAING recall class4: 0.38461538461538464 \n",
            "\n",
            "TRAING prec class0: 0.9321766561514195 TRAING prec class1: 0.6184738955823293 TRAING prec class2: 0.6243386243386243 TRAING prec class3: 0.6909090909090909 TRAING prec class4: 0.8333333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.9285153181461115 TRAING f1 class1: 0.6652267818574514 TRAING f1 class2: 0.6781609195402298 TRAING f1 class3: 0.5868725868725869 TRAING f1 class4: 0.5263157894736842 \n",
            "\n",
            "TRAINIG LOSS: 399 0.596781165774446\n",
            "TRAING recall class0: 0.9239001189060642 TRAING recall class1: 0.7071428571428572 TRAING recall class2: 0.7181818181818181 TRAING recall class3: 0.5436893203883495 TRAING recall class4: 0.39622641509433965 \n",
            "\n",
            "TRAING prec class0: 0.9305389221556887 TRAING prec class1: 0.6149068322981367 TRAING prec class2: 0.6124031007751938 TRAING prec class3: 0.6956521739130435 TRAING prec class4: 0.875 \n",
            "\n",
            "TRAING f1 class0: 0.9272076372315037 TRAING f1 class1: 0.6578073089700998 TRAING f1 class2: 0.6610878661087866 TRAING f1 class3: 0.6103542234332425 TRAING f1 class4: 0.5454545454545454 \n",
            "\n",
            "TRAINIG LOSS: 499 0.6005434295665473\n",
            "TRAING recall class0: 0.9200761179828735 TRAING recall class1: 0.7040229885057471 TRAING recall class2: 0.7561837455830389 TRAING recall class3: 0.536 TRAING recall class4: 0.4117647058823529 \n",
            "\n",
            "TRAING prec class0: 0.9342995169082126 TRAING prec class1: 0.6265984654731458 TRAING prec class2: 0.6331360946745562 TRAING prec class3: 0.6568627450980392 TRAING prec class4: 0.875 \n",
            "\n",
            "TRAING f1 class0: 0.9271332694151485 TRAING f1 class1: 0.6630581867388362 TRAING f1 class2: 0.6892109500805154 TRAING f1 class3: 0.5903083700440529 TRAING f1 class4: 0.56 \n",
            "\n",
            "10\n",
            "eval recall class0: 0.8809523809523809 eval recall class1: 0.5 eval recall class2: 0.5471698113207547 eval recall class3: 0.25 eval recall class4: 0.3333333333333333 \n",
            "\n",
            "eval prec class0: 0.8809523809523809 eval prec class1: 0.4696969696969697 eval prec class2: 0.5272727272727272 eval prec class3: 0.22857142857142856 eval prec class4: 0.8333333333333334 \n",
            "\n",
            "eval f1 class0: 0.8809523809523809 eval f1 class1: 0.484375 eval f1 class2: 0.537037037037037 eval f1 class3: 0.23880597014925375 eval f1 class4: 0.47619047619047616 \n",
            "\n",
            "eval LOSS: 0.7758656786754727\n",
            "epoch10_global-step6243  SAVED \n",
            "TRAINIG LOSS: 99 0.5847367254830896\n",
            "TRAING recall class0: 0.9238095238095239 TRAING recall class1: 0.7714285714285715 TRAING recall class2: 0.734375 TRAING recall class3: 0.5609756097560976 TRAING recall class4: 0.3333333333333333 \n",
            "\n",
            "TRAING prec class0: 0.919431279620853 TRAING prec class1: 0.675 TRAING prec class2: 0.7230769230769231 TRAING prec class3: 0.6052631578947368 TRAING prec class4: 0.8333333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.9216152019002375 TRAING f1 class1: 0.72 TRAING f1 class2: 0.7286821705426355 TRAING f1 class3: 0.5822784810126583 TRAING f1 class4: 0.47619047619047616 \n",
            "\n",
            "TRAINIG LOSS: 199 0.5886334013193846\n",
            "TRAING recall class0: 0.913953488372093 TRAING recall class1: 0.7285714285714285 TRAING recall class2: 0.7168141592920354 TRAING recall class3: 0.5357142857142857 TRAING recall class4: 0.30303030303030304 \n",
            "\n",
            "TRAING prec class0: 0.9401913875598086 TRAING prec class1: 0.6375 TRAING prec class2: 0.6090225563909775 TRAING prec class3: 0.6 TRAING prec class4: 0.7142857142857143 \n",
            "\n",
            "TRAING f1 class0: 0.9268867924528302 TRAING f1 class1: 0.68 TRAING f1 class2: 0.6585365853658537 TRAING f1 class3: 0.5660377358490566 TRAING f1 class4: 0.42553191489361697 \n",
            "\n",
            "TRAINIG LOSS: 299 0.5746682361016671\n",
            "TRAING recall class0: 0.928125 TRAING recall class1: 0.7201834862385321 TRAING recall class2: 0.7080745341614907 TRAING recall class3: 0.5182481751824818 TRAING recall class4: 0.3409090909090909 \n",
            "\n",
            "TRAING prec class0: 0.945859872611465 TRAING prec class1: 0.6330645161290323 TRAING prec class2: 0.6096256684491979 TRAING prec class3: 0.6016949152542372 TRAING prec class4: 0.7894736842105263 \n",
            "\n",
            "TRAING f1 class0: 0.9369085173501578 TRAING f1 class1: 0.6738197424892705 TRAING f1 class2: 0.6551724137931034 TRAING f1 class3: 0.5568627450980391 TRAING f1 class4: 0.4761904761904762 \n",
            "\n",
            "TRAINIG LOSS: 399 0.5780071137379855\n",
            "TRAING recall class0: 0.9210836277974087 TRAING recall class1: 0.7314487632508834 TRAING recall class2: 0.7272727272727273 TRAING recall class3: 0.544973544973545 TRAING recall class4: 0.3559322033898305 \n",
            "\n",
            "TRAING prec class0: 0.9433051869722557 TRAING prec class1: 0.6369230769230769 TRAING prec class2: 0.6299212598425197 TRAING prec class3: 0.6204819277108434 TRAING prec class4: 0.8076923076923077 \n",
            "\n",
            "TRAING f1 class0: 0.9320619785458879 TRAING f1 class1: 0.6809210526315789 TRAING f1 class2: 0.6751054852320675 TRAING f1 class3: 0.5802816901408452 TRAING f1 class4: 0.4941176470588236 \n",
            "\n",
            "TRAINIG LOSS: 499 0.5784788545127958\n",
            "TRAING recall class0: 0.9216061185468452 TRAING recall class1: 0.7308781869688386 TRAING recall class2: 0.7295373665480427 TRAING recall class3: 0.5691699604743083 TRAING recall class4: 0.3283582089552239 \n",
            "\n",
            "TRAING prec class0: 0.9432485322896281 TRAING prec class1: 0.6386138613861386 TRAING prec class2: 0.6327160493827161 TRAING prec class3: 0.6457399103139013 TRAING prec class4: 0.8148148148148148 \n",
            "\n",
            "TRAING f1 class0: 0.932301740812379 TRAING f1 class1: 0.6816380449141348 TRAING f1 class2: 0.6776859504132232 TRAING f1 class3: 0.6050420168067228 TRAING f1 class4: 0.4680851063829788 \n",
            "\n",
            "11\n",
            "eval recall class0: 0.8809523809523809 eval recall class1: 0.4838709677419355 eval recall class2: 0.5283018867924528 eval recall class3: 0.25 eval recall class4: 0.3333333333333333 \n",
            "\n",
            "eval prec class0: 0.8809523809523809 eval prec class1: 0.46875 eval prec class2: 0.509090909090909 eval prec class3: 0.2222222222222222 eval prec class4: 0.7142857142857143 \n",
            "\n",
            "eval f1 class0: 0.8809523809523809 eval f1 class1: 0.47619047619047616 eval f1 class2: 0.5185185185185185 eval f1 class3: 0.23529411764705882 eval f1 class4: 0.4545454545454545 \n",
            "\n",
            "eval LOSS: 0.7748305734408938\n",
            "epoch11_global-step6858  SAVED \n",
            "TRAINIG LOSS: 99 0.5450419651344418\n",
            "TRAING recall class0: 0.9227053140096618 TRAING recall class1: 0.6470588235294118 TRAING recall class2: 0.8 TRAING recall class3: 0.66 TRAING recall class4: 0.5333333333333333 \n",
            "\n",
            "TRAING prec class0: 0.955 TRAING prec class1: 0.6376811594202898 TRAING prec class2: 0.7164179104477612 TRAING prec class3: 0.6111111111111112 TRAING prec class4: 0.8 \n",
            "\n",
            "TRAING f1 class0: 0.9385749385749386 TRAING f1 class1: 0.6423357664233575 TRAING f1 class2: 0.7559055118110236 TRAING f1 class3: 0.6346153846153846 TRAING f1 class4: 0.64 \n",
            "\n",
            "TRAINIG LOSS: 199 0.5494923804397694\n",
            "TRAING recall class0: 0.9263657957244655 TRAING recall class1: 0.7021276595744681 TRAING recall class2: 0.7692307692307693 TRAING recall class3: 0.6893203883495146 TRAING recall class4: 0.5161290322580645 \n",
            "\n",
            "TRAING prec class0: 0.9629629629629629 TRAING prec class1: 0.673469387755102 TRAING prec class2: 0.6722689075630253 TRAING prec class3: 0.6454545454545455 TRAING prec class4: 0.8421052631578947 \n",
            "\n",
            "TRAING f1 class0: 0.9443099273607748 TRAING f1 class1: 0.6875000000000001 TRAING f1 class2: 0.7174887892376681 TRAING f1 class3: 0.6666666666666666 TRAING f1 class4: 0.6399999999999999 \n",
            "\n",
            "TRAINIG LOSS: 299 0.551214953128559\n",
            "TRAING recall class0: 0.9229559748427673 TRAING recall class1: 0.7089201877934272 TRAING recall class2: 0.7666666666666667 TRAING recall class3: 0.620253164556962 TRAING recall class4: 0.5116279069767442 \n",
            "\n",
            "TRAING prec class0: 0.9467741935483871 TRAING prec class1: 0.6711111111111111 TRAING prec class2: 0.6686046511627907 TRAING prec class3: 0.6282051282051282 TRAING prec class4: 0.8148148148148148 \n",
            "\n",
            "TRAING f1 class0: 0.9347133757961782 TRAING f1 class1: 0.6894977168949772 TRAING f1 class2: 0.7142857142857143 TRAING f1 class3: 0.6242038216560509 TRAING f1 class4: 0.6285714285714286 \n",
            "\n",
            "TRAINIG LOSS: 399 0.5445669504650869\n",
            "TRAING recall class0: 0.9290780141843972 TRAING recall class1: 0.7208480565371025 TRAING recall class2: 0.7735849056603774 TRAING recall class3: 0.6323529411764706 TRAING recall class4: 0.4909090909090909 \n",
            "\n",
            "TRAING prec class0: 0.9492753623188406 TRAING prec class1: 0.6777408637873754 TRAING prec class2: 0.6890756302521008 TRAING prec class3: 0.6417910447761194 TRAING prec class4: 0.84375 \n",
            "\n",
            "TRAING f1 class0: 0.9390681003584229 TRAING f1 class1: 0.6986301369863014 TRAING f1 class2: 0.7288888888888888 TRAING f1 class3: 0.6370370370370371 TRAING f1 class4: 0.6206896551724137 \n",
            "\n",
            "TRAINIG LOSS: 499 0.5404628745075315\n",
            "TRAING recall class0: 0.9323164918970448 TRAING recall class1: 0.7094017094017094 TRAING recall class2: 0.7598566308243727 TRAING recall class3: 0.6627450980392157 TRAING recall class4: 0.4393939393939394 \n",
            "\n",
            "TRAING prec class0: 0.9412897016361886 TRAING prec class1: 0.6840659340659341 TRAING prec class2: 0.6973684210526315 TRAING prec class3: 0.6575875486381323 TRAING prec class4: 0.8055555555555556 \n",
            "\n",
            "TRAING f1 class0: 0.9367816091954022 TRAING f1 class1: 0.6965034965034966 TRAING f1 class2: 0.7272727272727272 TRAING f1 class3: 0.66015625 TRAING f1 class4: 0.5686274509803921 \n",
            "\n",
            "12\n",
            "eval recall class0: 0.8761904761904762 eval recall class1: 0.5 eval recall class2: 0.660377358490566 eval recall class3: 0.1875 eval recall class4: 0.4 \n",
            "\n",
            "eval prec class0: 0.9154228855721394 eval prec class1: 0.45588235294117646 eval prec class2: 0.5384615384615384 eval prec class3: 0.2 eval prec class4: 0.75 \n",
            "\n",
            "eval f1 class0: 0.8953771289537713 eval f1 class1: 0.47692307692307695 eval f1 class2: 0.5932203389830508 eval f1 class3: 0.19354838709677422 eval f1 class4: 0.5217391304347827 \n",
            "\n",
            "eval LOSS: 0.7734836556027522\n",
            "epoch12_global-step7473  SAVED \n",
            "TRAINIG LOSS: 99 0.4738551593478769\n",
            "TRAING recall class0: 0.9372197309417041 TRAING recall class1: 0.6774193548387096 TRAING recall class2: 0.8245614035087719 TRAING recall class3: 0.6875 TRAING recall class4: 0.5 \n",
            "\n",
            "TRAING prec class0: 0.9587155963302753 TRAING prec class1: 0.711864406779661 TRAING prec class2: 0.6619718309859155 TRAING prec class3: 0.7333333333333333 TRAING prec class4: 0.7142857142857143 \n",
            "\n",
            "TRAING f1 class0: 0.9478458049886621 TRAING f1 class1: 0.6942148760330579 TRAING f1 class2: 0.734375 TRAING f1 class3: 0.7096774193548386 TRAING f1 class4: 0.588235294117647 \n",
            "\n",
            "TRAINIG LOSS: 199 0.4731303239939734\n",
            "TRAING recall class0: 0.9472477064220184 TRAING recall class1: 0.7573529411764706 TRAING recall class2: 0.8055555555555556 TRAING recall class3: 0.7142857142857143 TRAING recall class4: 0.45454545454545453 \n",
            "\n",
            "TRAING prec class0: 0.9604651162790697 TRAING prec class1: 0.7410071942446043 TRAING prec class2: 0.696 TRAING prec class3: 0.7526881720430108 TRAING prec class4: 0.7692307692307693 \n",
            "\n",
            "TRAING f1 class0: 0.953810623556582 TRAING f1 class1: 0.7490909090909091 TRAING f1 class2: 0.7467811158798283 TRAING f1 class3: 0.7329842931937173 TRAING f1 class4: 0.5714285714285714 \n",
            "\n",
            "TRAINIG LOSS: 299 0.5005777179542928\n",
            "TRAING recall class0: 0.9395866454689984 TRAING recall class1: 0.7524271844660194 TRAING recall class2: 0.8224852071005917 TRAING recall class3: 0.7077922077922078 TRAING recall class4: 0.42857142857142855 \n",
            "\n",
            "TRAING prec class0: 0.9501607717041801 TRAING prec class1: 0.7416267942583732 TRAING prec class2: 0.6780487804878049 TRAING prec class3: 0.7676056338028169 TRAING prec class4: 0.8181818181818182 \n",
            "\n",
            "TRAING f1 class0: 0.9448441247002399 TRAING f1 class1: 0.7469879518072288 TRAING f1 class2: 0.7433155080213903 TRAING f1 class3: 0.7364864864864864 TRAING f1 class4: 0.5625 \n",
            "\n",
            "TRAINIG LOSS: 399 0.5050439030374401\n",
            "TRAING recall class0: 0.9365269461077844 TRAING recall class1: 0.7464285714285714 TRAING recall class2: 0.8078602620087336 TRAING recall class3: 0.7 TRAING recall class4: 0.44642857142857145 \n",
            "\n",
            "TRAING prec class0: 0.9536585365853658 TRAING prec class1: 0.718213058419244 TRAING prec class2: 0.6954887218045113 TRAING prec class3: 0.7329842931937173 TRAING prec class4: 0.78125 \n",
            "\n",
            "TRAING f1 class0: 0.9450151057401813 TRAING f1 class1: 0.7320490367775832 TRAING f1 class2: 0.7474747474747475 TRAING f1 class3: 0.7161125319693096 TRAING f1 class4: 0.5681818181818182 \n",
            "\n",
            "TRAINIG LOSS: 499 0.5116704314444214\n",
            "TRAING recall class0: 0.9351763584366063 TRAING recall class1: 0.7628571428571429 TRAING recall class2: 0.7885304659498208 TRAING recall class3: 0.6719367588932806 TRAING recall class4: 0.463768115942029 \n",
            "\n",
            "TRAING prec class0: 0.9542801556420234 TRAING prec class1: 0.7063492063492064 TRAING prec class2: 0.6853582554517134 TRAING prec class3: 0.7327586206896551 TRAING prec class4: 0.7804878048780488 \n",
            "\n",
            "TRAING f1 class0: 0.9446316803081367 TRAING f1 class1: 0.7335164835164835 TRAING f1 class2: 0.7333333333333333 TRAING f1 class3: 0.7010309278350515 TRAING f1 class4: 0.5818181818181818 \n",
            "\n",
            "13\n",
            "eval recall class0: 0.8761904761904762 eval recall class1: 0.5 eval recall class2: 0.5471698113207547 eval recall class3: 0.25 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8888888888888888 eval prec class1: 0.4696969696969697 eval prec class2: 0.5087719298245614 eval prec class3: 0.24242424242424243 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.882494004796163 eval f1 class1: 0.484375 eval f1 class2: 0.5272727272727273 eval f1 class3: 0.24615384615384617 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.779896072415716\n",
            "epoch13_global-step8088  SAVED \n",
            "TRAINIG LOSS: 99 0.4576630295533687\n",
            "TRAING recall class0: 0.9587155963302753 TRAING recall class1: 0.7910447761194029 TRAING recall class2: 0.7551020408163265 TRAING recall class3: 0.7368421052631579 TRAING recall class4: 0.5555555555555556 \n",
            "\n",
            "TRAING prec class0: 0.9587155963302753 TRAING prec class1: 0.7361111111111112 TRAING prec class2: 0.7115384615384616 TRAING prec class3: 0.8076923076923077 TRAING prec class4: 0.8333333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.9587155963302753 TRAING f1 class1: 0.7625899280575541 TRAING f1 class2: 0.7326732673267328 TRAING f1 class3: 0.7706422018348624 TRAING f1 class4: 0.6666666666666667 \n",
            "\n",
            "TRAINIG LOSS: 199 0.45724474513437596\n",
            "TRAING recall class0: 0.9599056603773585 TRAING recall class1: 0.8148148148148148 TRAING recall class2: 0.8 TRAING recall class3: 0.7142857142857143 TRAING recall class4: 0.6190476190476191 \n",
            "\n",
            "TRAING prec class0: 0.9599056603773585 TRAING prec class1: 0.738255033557047 TRAING prec class2: 0.8 TRAING prec class3: 0.7653061224489796 TRAING prec class4: 0.9285714285714286 \n",
            "\n",
            "TRAING f1 class0: 0.9599056603773585 TRAING f1 class1: 0.7746478873239436 TRAING f1 class2: 0.8000000000000002 TRAING f1 class3: 0.7389162561576355 TRAING f1 class4: 0.742857142857143 \n",
            "\n",
            "TRAINIG LOSS: 299 0.4612923952657729\n",
            "TRAING recall class0: 0.9533437013996889 TRAING recall class1: 0.7839195979899497 TRAING recall class2: 0.7964071856287425 TRAING recall class3: 0.7207792207792207 TRAING recall class4: 0.5675675675675675 \n",
            "\n",
            "TRAING prec class0: 0.9623233908948194 TRAING prec class1: 0.7358490566037735 TRAING prec class2: 0.7430167597765364 TRAING prec class3: 0.75 TRAING prec class4: 0.875 \n",
            "\n",
            "TRAING f1 class0: 0.9578125 TRAING f1 class1: 0.7591240875912408 TRAING f1 class2: 0.7687861271676302 TRAING f1 class3: 0.7350993377483442 TRAING f1 class4: 0.6885245901639344 \n",
            "\n",
            "TRAINIG LOSS: 399 0.4715223422390409\n",
            "TRAING recall class0: 0.9457547169811321 TRAING recall class1: 0.7906137184115524 TRAING recall class2: 0.8202764976958525 TRAING recall class3: 0.680952380952381 TRAING recall class4: 0.5625 \n",
            "\n",
            "TRAING prec class0: 0.9593301435406698 TRAING prec class1: 0.7324414715719063 TRAING prec class2: 0.7447698744769874 TRAING prec class3: 0.7409326424870466 TRAING prec class4: 0.8181818181818182 \n",
            "\n",
            "TRAING f1 class0: 0.9524940617577198 TRAING f1 class1: 0.7604166666666666 TRAING f1 class2: 0.7807017543859649 TRAING f1 class3: 0.7096774193548387 TRAING f1 class4: 0.6666666666666666 \n",
            "\n",
            "TRAINIG LOSS: 499 0.478730019477196\n",
            "TRAING recall class0: 0.9475691134413727 TRAING recall class1: 0.7863247863247863 TRAING recall class2: 0.8156028368794326 TRAING recall class3: 0.6895161290322581 TRAING recall class4: 0.5428571428571428 \n",
            "\n",
            "TRAING prec class0: 0.9585342333654774 TRAING prec class1: 0.7379679144385026 TRAING prec class2: 0.7443365695792881 TRAING prec class3: 0.7276595744680852 TRAING prec class4: 0.8444444444444444 \n",
            "\n",
            "TRAING f1 class0: 0.9530201342281879 TRAING f1 class1: 0.7613793103448275 TRAING f1 class2: 0.7783417935702199 TRAING f1 class3: 0.7080745341614908 TRAING f1 class4: 0.6608695652173913 \n",
            "\n",
            "14\n",
            "eval recall class0: 0.8761904761904762 eval recall class1: 0.5161290322580645 eval recall class2: 0.5849056603773585 eval recall class3: 0.1875 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8975609756097561 eval prec class1: 0.45714285714285713 eval prec class2: 0.5254237288135594 eval prec class3: 0.20689655172413793 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.8867469879518073 eval f1 class1: 0.4848484848484849 eval f1 class2: 0.5535714285714286 eval f1 class3: 0.19672131147540986 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.7824303403613158\n",
            "epoch14_global-step8703  SAVED \n",
            "TRAINIG LOSS: 99 0.43761983767151835\n",
            "TRAING recall class0: 0.9447236180904522 TRAING recall class1: 0.8333333333333334 TRAING recall class2: 0.8688524590163934 TRAING recall class3: 0.7446808510638298 TRAING recall class4: 0.7333333333333333 \n",
            "\n",
            "TRAING prec class0: 0.9791666666666666 TRAING prec class1: 0.7831325301204819 TRAING prec class2: 0.7571428571428571 TRAING prec class3: 0.7954545454545454 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9616368286445013 TRAING f1 class1: 0.8074534161490683 TRAING f1 class2: 0.8091603053435115 TRAING f1 class3: 0.7692307692307692 TRAING f1 class4: 0.846153846153846 \n",
            "\n",
            "TRAINIG LOSS: 199 0.444595936704427\n",
            "TRAING recall class0: 0.941031941031941 TRAING recall class1: 0.8193548387096774 TRAING recall class2: 0.8879310344827587 TRAING recall class3: 0.7319587628865979 TRAING recall class4: 0.68 \n",
            "\n",
            "TRAING prec class0: 0.979539641943734 TRAING prec class1: 0.7888198757763976 TRAING prec class2: 0.7304964539007093 TRAING prec class3: 0.8160919540229885 TRAING prec class4: 0.85 \n",
            "\n",
            "TRAING f1 class0: 0.9598997493734335 TRAING f1 class1: 0.8037974683544303 TRAING f1 class2: 0.8015564202334631 TRAING f1 class3: 0.7717391304347826 TRAING f1 class4: 0.7555555555555556 \n",
            "\n",
            "TRAINIG LOSS: 299 0.4520532820606604\n",
            "TRAING recall class0: 0.9531502423263328 TRAING recall class1: 0.8240740740740741 TRAING recall class2: 0.863905325443787 TRAING recall class3: 0.7337662337662337 TRAING recall class4: 0.5952380952380952 \n",
            "\n",
            "TRAING prec class0: 0.9735973597359736 TRAING prec class1: 0.7739130434782608 TRAING prec class2: 0.7564766839378239 TRAING prec class3: 0.8014184397163121 TRAING prec class4: 0.8333333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.963265306122449 TRAING f1 class1: 0.7982062780269059 TRAING f1 class2: 0.8066298342541437 TRAING f1 class3: 0.7661016949152543 TRAING f1 class4: 0.6944444444444444 \n",
            "\n",
            "TRAINIG LOSS: 399 0.45958009381778536\n",
            "TRAING recall class0: 0.9493365500603136 TRAING recall class1: 0.8181818181818182 TRAING recall class2: 0.8348214285714286 TRAING recall class3: 0.7330097087378641 TRAING recall class4: 0.6363636363636364 \n",
            "\n",
            "TRAING prec class0: 0.968019680196802 TRAING prec class1: 0.7647058823529411 TRAING prec class2: 0.7450199203187251 TRAING prec class3: 0.798941798941799 TRAING prec class4: 0.8536585365853658 \n",
            "\n",
            "TRAING f1 class0: 0.9585870889159562 TRAING f1 class1: 0.7905405405405405 TRAING f1 class2: 0.7873684210526316 TRAING f1 class3: 0.7645569620253165 TRAING f1 class4: 0.7291666666666666 \n",
            "\n",
            "TRAINIG LOSS: 499 0.44751338044181466\n",
            "TRAING recall class0: 0.9533777354900095 TRAING recall class1: 0.8132183908045977 TRAING recall class2: 0.8374558303886925 TRAING recall class3: 0.746031746031746 TRAING recall class4: 0.6060606060606061 \n",
            "\n",
            "TRAING prec class0: 0.9690522243713733 TRAING prec class1: 0.7648648648648648 TRAING prec class2: 0.7620578778135049 TRAING prec class3: 0.7899159663865546 TRAING prec class4: 0.851063829787234 \n",
            "\n",
            "TRAING f1 class0: 0.9611510791366906 TRAING f1 class1: 0.7883008356545962 TRAING f1 class2: 0.797979797979798 TRAING f1 class3: 0.7673469387755103 TRAING f1 class4: 0.7079646017699115 \n",
            "\n",
            "15\n",
            "eval recall class0: 0.8761904761904762 eval recall class1: 0.5161290322580645 eval recall class2: 0.5660377358490566 eval recall class3: 0.1875 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8932038834951457 eval prec class1: 0.463768115942029 eval prec class2: 0.5172413793103449 eval prec class3: 0.2 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.8846153846153847 eval f1 class1: 0.4885496183206106 eval f1 class2: 0.5405405405405406 eval f1 class3: 0.19354838709677422 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.7932146642048358\n",
            "epoch15_global-step9318  SAVED \n",
            "TRAINIG LOSS: 99 0.4496213669702411\n",
            "TRAING recall class0: 0.9259259259259259 TRAING recall class1: 0.8026315789473685 TRAING recall class2: 0.873015873015873 TRAING recall class3: 0.78125 TRAING recall class4: 0.5 \n",
            "\n",
            "TRAING prec class0: 0.9615384615384616 TRAING prec class1: 0.7261904761904762 TRAING prec class2: 0.8088235294117647 TRAING prec class3: 0.8771929824561403 TRAING prec class4: 0.4444444444444444 \n",
            "\n",
            "TRAING f1 class0: 0.9433962264150944 TRAING f1 class1: 0.7625 TRAING f1 class2: 0.8396946564885497 TRAING f1 class3: 0.8264462809917354 TRAING f1 class4: 0.47058823529411764 \n",
            "\n",
            "TRAINIG LOSS: 199 0.4554283956461586\n",
            "TRAING recall class0: 0.9382716049382716 TRAING recall class1: 0.8043478260869565 TRAING recall class2: 0.8548387096774194 TRAING recall class3: 0.7454545454545455 TRAING recall class4: 0.43478260869565216 \n",
            "\n",
            "TRAING prec class0: 0.9718670076726342 TRAING prec class1: 0.7207792207792207 TRAING prec class2: 0.7737226277372263 TRAING prec class3: 0.7961165048543689 TRAING prec class4: 0.6666666666666666 \n",
            "\n",
            "TRAING f1 class0: 0.9547738693467336 TRAING f1 class1: 0.7602739726027397 TRAING f1 class2: 0.8122605363984675 TRAING f1 class3: 0.7699530516431926 TRAING f1 class4: 0.5263157894736841 \n",
            "\n",
            "TRAINIG LOSS: 299 0.43405420660817373\n",
            "TRAING recall class0: 0.9498381877022654 TRAING recall class1: 0.7942583732057417 TRAING recall class2: 0.8439306358381503 TRAING recall class3: 0.7757575757575758 TRAING recall class4: 0.5142857142857142 \n",
            "\n",
            "TRAING prec class0: 0.9734660033167496 TRAING prec class1: 0.751131221719457 TRAING prec class2: 0.7891891891891892 TRAING prec class3: 0.7664670658682635 TRAING prec class4: 0.75 \n",
            "\n",
            "TRAING f1 class0: 0.9615069615069616 TRAING f1 class1: 0.7720930232558141 TRAING f1 class2: 0.8156424581005587 TRAING f1 class3: 0.7710843373493977 TRAING f1 class4: 0.6101694915254237 \n",
            "\n",
            "TRAINIG LOSS: 399 0.42712730597588233\n",
            "TRAING recall class0: 0.9497005988023952 TRAING recall class1: 0.8113879003558719 TRAING recall class2: 0.831858407079646 TRAING recall class3: 0.7342995169082126 TRAING recall class4: 0.5686274509803921 \n",
            "\n",
            "TRAING prec class0: 0.9730061349693252 TRAING prec class1: 0.7450980392156863 TRAING prec class2: 0.7768595041322314 TRAING prec class3: 0.7562189054726368 TRAING prec class4: 0.8055555555555556 \n",
            "\n",
            "TRAING f1 class0: 0.9612121212121212 TRAING f1 class1: 0.776831345826235 TRAING f1 class2: 0.8034188034188033 TRAING f1 class3: 0.7450980392156863 TRAING f1 class4: 0.6666666666666667 \n",
            "\n",
            "TRAINIG LOSS: 499 0.4222610844252631\n",
            "TRAING recall class0: 0.9512893982808023 TRAING recall class1: 0.8132183908045977 TRAING recall class2: 0.8450704225352113 TRAING recall class3: 0.7372549019607844 TRAING recall class4: 0.5757575757575758 \n",
            "\n",
            "TRAING prec class0: 0.9717073170731707 TRAING prec class1: 0.7566844919786097 TRAING prec class2: 0.7741935483870968 TRAING prec class3: 0.7704918032786885 TRAING prec class4: 0.8085106382978723 \n",
            "\n",
            "TRAING f1 class0: 0.9613899613899615 TRAING f1 class1: 0.7839335180055402 TRAING f1 class2: 0.8080808080808081 TRAING f1 class3: 0.7535070140280561 TRAING f1 class4: 0.672566371681416 \n",
            "\n",
            "16\n",
            "eval recall class0: 0.8857142857142857 eval recall class1: 0.5161290322580645 eval recall class2: 0.5471698113207547 eval recall class3: 0.1875 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8773584905660378 eval prec class1: 0.49230769230769234 eval prec class2: 0.5178571428571429 eval prec class3: 0.2 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.8815165876777252 eval f1 class1: 0.5039370078740157 eval f1 class2: 0.5321100917431193 eval f1 class3: 0.19354838709677422 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.8064726243028417\n",
            "epoch16_global-step9933  SAVED \n",
            "TRAINIG LOSS: 99 0.38305324855260553\n",
            "TRAING recall class0: 0.9711538461538461 TRAING recall class1: 0.855072463768116 TRAING recall class2: 0.8833333333333333 TRAING recall class3: 0.803921568627451 TRAING recall class4: 0.5 \n",
            "\n",
            "TRAING prec class0: 0.9711538461538461 TRAING prec class1: 0.8805970149253731 TRAING prec class2: 0.828125 TRAING prec class3: 0.7592592592592593 TRAING prec class4: 0.8571428571428571 \n",
            "\n",
            "TRAING f1 class0: 0.9711538461538461 TRAING f1 class1: 0.8676470588235295 TRAING f1 class2: 0.8548387096774193 TRAING f1 class3: 0.780952380952381 TRAING f1 class4: 0.631578947368421 \n",
            "\n",
            "TRAINIG LOSS: 199 0.390194645288866\n",
            "TRAING recall class0: 0.9639423076923077 TRAING recall class1: 0.8571428571428571 TRAING recall class2: 0.8583333333333333 TRAING recall class3: 0.7553191489361702 TRAING recall class4: 0.6333333333333333 \n",
            "\n",
            "TRAING prec class0: 0.9709443099273608 TRAING prec class1: 0.8108108108108109 TRAING prec class2: 0.824 TRAING prec class3: 0.7634408602150538 TRAING prec class4: 0.9047619047619048 \n",
            "\n",
            "TRAING f1 class0: 0.9674306393244875 TRAING f1 class1: 0.8333333333333334 TRAING f1 class2: 0.8408163265306122 TRAING f1 class3: 0.7593582887700534 TRAING f1 class4: 0.7450980392156863 \n",
            "\n",
            "TRAINIG LOSS: 299 0.3921493954382216\n",
            "TRAING recall class0: 0.9599358974358975 TRAING recall class1: 0.8436018957345972 TRAING recall class2: 0.8651685393258427 TRAING recall class3: 0.7872340425531915 TRAING recall class4: 0.6521739130434783 \n",
            "\n",
            "TRAING prec class0: 0.9724025974025974 TRAING prec class1: 0.8165137614678899 TRAING prec class2: 0.806282722513089 TRAING prec class3: 0.7816901408450704 TRAING prec class4: 0.9090909090909091 \n",
            "\n",
            "TRAING f1 class0: 0.9661290322580646 TRAING f1 class1: 0.8298368298368298 TRAING f1 class2: 0.8346883468834688 TRAING f1 class3: 0.784452296819788 TRAING f1 class4: 0.759493670886076 \n",
            "\n",
            "TRAINIG LOSS: 399 0.39276715684332886\n",
            "TRAING recall class0: 0.9608076009501187 TRAING recall class1: 0.8669064748201439 TRAING recall class2: 0.8603603603603603 TRAING recall class3: 0.7738693467336684 TRAING recall class4: 0.6101694915254238 \n",
            "\n",
            "TRAING prec class0: 0.97352587244284 TRAING prec class1: 0.8033333333333333 TRAING prec class2: 0.799163179916318 TRAING prec class3: 0.8105263157894737 TRAING prec class4: 0.9 \n",
            "\n",
            "TRAING f1 class0: 0.967124925283921 TRAING f1 class1: 0.8339100346020762 TRAING f1 class2: 0.8286334056399132 TRAING f1 class3: 0.7917737789203085 TRAING f1 class4: 0.7272727272727273 \n",
            "\n",
            "TRAINIG LOSS: 499 0.3916123897861689\n",
            "TRAING recall class0: 0.9637404580152672 TRAING recall class1: 0.8542857142857143 TRAING recall class2: 0.8571428571428571 TRAING recall class3: 0.7698412698412699 TRAING recall class4: 0.6571428571428571 \n",
            "\n",
            "TRAING prec class0: 0.974903474903475 TRAING prec class1: 0.8081081081081081 TRAING prec class2: 0.7973421926910299 TRAING prec class3: 0.8117154811715481 TRAING prec class4: 0.8518518518518519 \n",
            "\n",
            "TRAING f1 class0: 0.9692898272552783 TRAING f1 class1: 0.8305555555555556 TRAING f1 class2: 0.8261617900172117 TRAING f1 class3: 0.7902240325865579 TRAING f1 class4: 0.7419354838709677 \n",
            "\n",
            "17\n",
            "eval recall class0: 0.8857142857142857 eval recall class1: 0.5161290322580645 eval recall class2: 0.5471698113207547 eval recall class3: 0.21875 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8857142857142857 eval prec class1: 0.49230769230769234 eval prec class2: 0.5087719298245614 eval prec class3: 0.22580645161290322 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.8857142857142857 eval f1 class1: 0.5039370078740157 eval f1 class2: 0.5272727272727273 eval f1 class3: 0.2222222222222222 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.8139094614079627\n",
            "epoch17_global-step10548  SAVED \n",
            "TRAINIG LOSS: 99 0.3961861390573904\n",
            "TRAING recall class0: 0.943127962085308 TRAING recall class1: 0.8805970149253731 TRAING recall class2: 0.875 TRAING recall class3: 0.7222222222222222 TRAING recall class4: 0.65 \n",
            "\n",
            "TRAING prec class0: 0.9660194174757282 TRAING prec class1: 0.8082191780821918 TRAING prec class2: 0.75 TRAING prec class3: 0.8125 TRAING prec class4: 0.7647058823529411 \n",
            "\n",
            "TRAING f1 class0: 0.9544364508393285 TRAING f1 class1: 0.8428571428571429 TRAING f1 class2: 0.8076923076923077 TRAING f1 class3: 0.7647058823529411 TRAING f1 class4: 0.7027027027027027 \n",
            "\n",
            "TRAINIG LOSS: 199 0.37765815743943676\n",
            "TRAING recall class0: 0.9589371980676329 TRAING recall class1: 0.8961038961038961 TRAING recall class2: 0.8315789473684211 TRAING recall class3: 0.7524752475247525 TRAING recall class4: 0.6666666666666666 \n",
            "\n",
            "TRAING prec class0: 0.9754299754299754 TRAING prec class1: 0.8165680473372781 TRAING prec class2: 0.797979797979798 TRAING prec class3: 0.7916666666666666 TRAING prec class4: 0.8275862068965517 \n",
            "\n",
            "TRAING f1 class0: 0.9671132764920829 TRAING f1 class1: 0.8544891640866873 TRAING f1 class2: 0.8144329896907218 TRAING f1 class3: 0.7715736040609136 TRAING f1 class4: 0.7384615384615385 \n",
            "\n",
            "TRAINIG LOSS: 299 0.3694468400487676\n",
            "TRAING recall class0: 0.963258785942492 TRAING recall class1: 0.908256880733945 TRAING recall class2: 0.845679012345679 TRAING recall class3: 0.76 TRAING recall class4: 0.6363636363636364 \n",
            "\n",
            "TRAING prec class0: 0.9773095623987034 TRAING prec class1: 0.8181818181818182 TRAING prec class2: 0.8203592814371258 TRAING prec class3: 0.8085106382978723 TRAING prec class4: 0.8484848484848485 \n",
            "\n",
            "TRAING f1 class0: 0.9702333065164923 TRAING f1 class1: 0.8608695652173914 TRAING f1 class2: 0.8328267477203648 TRAING f1 class3: 0.7835051546391754 TRAING f1 class4: 0.7272727272727273 \n",
            "\n",
            "TRAINIG LOSS: 399 0.3726367235614452\n",
            "TRAING recall class0: 0.9616766467065868 TRAING recall class1: 0.8842105263157894 TRAING recall class2: 0.8454545454545455 TRAING recall class3: 0.7512437810945274 TRAING recall class4: 0.6779661016949152 \n",
            "\n",
            "TRAING prec class0: 0.9768856447688564 TRAING prec class1: 0.8102893890675241 TRAING prec class2: 0.8157894736842105 TRAING prec class3: 0.7864583333333334 TRAING prec class4: 0.851063829787234 \n",
            "\n",
            "TRAING f1 class0: 0.9692214846107423 TRAING f1 class1: 0.8456375838926175 TRAING f1 class2: 0.8303571428571429 TRAING f1 class3: 0.7684478371501272 TRAING f1 class4: 0.7547169811320754 \n",
            "\n",
            "TRAINIG LOSS: 499 0.3741248380690813\n",
            "TRAING recall class0: 0.9634263715110684 TRAING recall class1: 0.8711484593837535 TRAING recall class2: 0.8469750889679716 TRAING recall class3: 0.7707509881422925 TRAING recall class4: 0.7 \n",
            "\n",
            "TRAING prec class0: 0.9756335282651072 TRAING prec class1: 0.8120104438642297 TRAING prec class2: 0.8122866894197952 TRAING prec class3: 0.8091286307053942 TRAING prec class4: 0.8596491228070176 \n",
            "\n",
            "TRAING f1 class0: 0.9694915254237287 TRAING f1 class1: 0.8405405405405406 TRAING f1 class2: 0.8292682926829268 TRAING f1 class3: 0.7894736842105262 TRAING f1 class4: 0.7716535433070866 \n",
            "\n",
            "18\n",
            "eval recall class0: 0.8857142857142857 eval recall class1: 0.5161290322580645 eval recall class2: 0.5849056603773585 eval recall class3: 0.21875 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8899521531100478 eval prec class1: 0.5 eval prec class2: 0.5254237288135594 eval prec class3: 0.22580645161290322 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.8878281622911695 eval f1 class1: 0.5079365079365079 eval f1 class2: 0.5535714285714286 eval f1 class3: 0.2222222222222222 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.8257223818800412\n",
            "epoch18_global-step11163  SAVED \n",
            "TRAINIG LOSS: 99 0.2968707666918635\n",
            "TRAING recall class0: 0.9620853080568721 TRAING recall class1: 0.9137931034482759 TRAING recall class2: 0.9833333333333333 TRAING recall class3: 0.8688524590163934 TRAING recall class4: 0.6 \n",
            "\n",
            "TRAING prec class0: 0.9902439024390244 TRAING prec class1: 0.8688524590163934 TRAING prec class2: 0.8428571428571429 TRAING prec class3: 0.9298245614035088 TRAING prec class4: 0.8571428571428571 \n",
            "\n",
            "TRAING f1 class0: 0.9759615384615384 TRAING f1 class1: 0.8907563025210085 TRAING f1 class2: 0.9076923076923077 TRAING f1 class3: 0.8983050847457625 TRAING f1 class4: 0.7058823529411764 \n",
            "\n",
            "TRAINIG LOSS: 199 0.3217542465496808\n",
            "TRAING recall class0: 0.9631336405529954 TRAING recall class1: 0.8702290076335878 TRAING recall class2: 0.9385964912280702 TRAING recall class3: 0.8333333333333334 TRAING recall class4: 0.7368421052631579 \n",
            "\n",
            "TRAING prec class0: 0.9835294117647059 TRAING prec class1: 0.8636363636363636 TRAING prec class2: 0.84251968503937 TRAING prec class3: 0.85 TRAING prec class4: 0.875 \n",
            "\n",
            "TRAING f1 class0: 0.9732246798603027 TRAING f1 class1: 0.8669201520912547 TRAING f1 class2: 0.8879668049792531 TRAING f1 class3: 0.8415841584158417 TRAING f1 class4: 0.7999999999999999 \n",
            "\n",
            "TRAINIG LOSS: 299 0.34286074217408896\n",
            "TRAING recall class0: 0.9636075949367089 TRAING recall class1: 0.8792270531400966 TRAING recall class2: 0.9101796407185628 TRAING recall class3: 0.8012422360248447 TRAING recall class4: 0.696969696969697 \n",
            "\n",
            "TRAING prec class0: 0.9854368932038835 TRAING prec class1: 0.8387096774193549 TRAING prec class2: 0.8306010928961749 TRAING prec class3: 0.8486842105263158 TRAING prec class4: 0.7666666666666667 \n",
            "\n",
            "TRAING f1 class0: 0.9744 TRAING f1 class1: 0.8584905660377359 TRAING f1 class2: 0.8685714285714285 TRAING f1 class3: 0.8242811501597443 TRAING f1 class4: 0.7301587301587302 \n",
            "\n",
            "TRAINIG LOSS: 399 0.33749508439330383\n",
            "TRAING recall class0: 0.9623085983510011 TRAING recall class1: 0.8966789667896679 TRAING recall class2: 0.9041095890410958 TRAING recall class3: 0.8038277511961722 TRAING recall class4: 0.6730769230769231 \n",
            "\n",
            "TRAING prec class0: 0.985524728588661 TRAING prec class1: 0.8379310344827586 TRAING prec class2: 0.8215767634854771 TRAING prec class3: 0.8484848484848485 TRAING prec class4: 0.8333333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.9737783075089391 TRAING f1 class1: 0.8663101604278074 TRAING f1 class2: 0.8608695652173913 TRAING f1 class3: 0.8255528255528256 TRAING f1 class4: 0.74468085106383 \n",
            "\n",
            "TRAINIG LOSS: 499 0.33753421473037454\n",
            "TRAING recall class0: 0.9657142857142857 TRAING recall class1: 0.8923512747875354 TRAING recall class2: 0.8928571428571429 TRAING recall class3: 0.8134920634920635 TRAING recall class4: 0.6615384615384615 \n",
            "\n",
            "TRAING prec class0: 0.9825581395348837 TRAING prec class1: 0.8445040214477212 TRAING prec class2: 0.8305647840531561 TRAING prec class3: 0.8436213991769548 TRAING prec class4: 0.8431372549019608 \n",
            "\n",
            "TRAING f1 class0: 0.9740634005763689 TRAING f1 class1: 0.8677685950413223 TRAING f1 class2: 0.8605851979345955 TRAING f1 class3: 0.8282828282828283 TRAING f1 class4: 0.7413793103448276 \n",
            "\n",
            "19\n",
            "eval recall class0: 0.8761904761904762 eval recall class1: 0.5483870967741935 eval recall class2: 0.6226415094339622 eval recall class3: 0.125 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8975609756097561 eval prec class1: 0.4788732394366197 eval prec class2: 0.532258064516129 eval prec class3: 0.16 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.8867469879518073 eval f1 class1: 0.5112781954887218 eval f1 class2: 0.5739130434782608 eval f1 class3: 0.14035087719298245 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.8293430887481562\n",
            "epoch19_global-step11778  SAVED \n",
            "TRAINIG LOSS: 99 0.3144199257902801\n",
            "TRAING recall class0: 0.9644670050761421 TRAING recall class1: 0.9125 TRAING recall class2: 0.9333333333333333 TRAING recall class3: 0.7843137254901961 TRAING recall class4: 1.0 \n",
            "\n",
            "TRAING prec class0: 0.9895833333333334 TRAING prec class1: 0.8795180722891566 TRAING prec class2: 0.8484848484848485 TRAING prec class3: 0.9090909090909091 TRAING prec class4: 0.8 \n",
            "\n",
            "TRAING f1 class0: 0.9768637532133675 TRAING f1 class1: 0.8957055214723926 TRAING f1 class2: 0.888888888888889 TRAING f1 class3: 0.8421052631578948 TRAING f1 class4: 0.888888888888889 \n",
            "\n",
            "TRAINIG LOSS: 199 0.32212096304632726\n",
            "TRAING recall class0: 0.973621103117506 TRAING recall class1: 0.8802816901408451 TRAING recall class2: 0.8773584905660378 TRAING recall class3: 0.7777777777777778 TRAING recall class4: 0.7777777777777778 \n",
            "\n",
            "TRAING prec class0: 0.9830508474576272 TRAING prec class1: 0.8445945945945946 TRAING prec class2: 0.808695652173913 TRAING prec class3: 0.8571428571428571 TRAING prec class4: 0.8076923076923077 \n",
            "\n",
            "TRAING f1 class0: 0.9783132530120482 TRAING f1 class1: 0.8620689655172414 TRAING f1 class2: 0.8416289592760181 TRAING f1 class3: 0.8155339805825242 TRAING f1 class4: 0.7924528301886792 \n",
            "\n",
            "TRAINIG LOSS: 299 0.31811223823577167\n",
            "TRAING recall class0: 0.9698890649762282 TRAING recall class1: 0.8883495145631068 TRAING recall class2: 0.8827160493827161 TRAING recall class3: 0.8209876543209876 TRAING recall class4: 0.7692307692307693 \n",
            "\n",
            "TRAING prec class0: 0.9839228295819936 TRAING prec class1: 0.8591549295774648 TRAING prec class2: 0.8265895953757225 TRAING prec class3: 0.8580645161290322 TRAING prec class4: 0.8108108108108109 \n",
            "\n",
            "TRAING f1 class0: 0.976855546687949 TRAING f1 class1: 0.873508353221957 TRAING f1 class2: 0.853731343283582 TRAING f1 class3: 0.8391167192429022 TRAING f1 class4: 0.7894736842105263 \n",
            "\n",
            "TRAINIG LOSS: 399 0.3092788138287142\n",
            "TRAING recall class0: 0.976218787158145 TRAING recall class1: 0.9084507042253521 TRAING recall class2: 0.8878923766816144 TRAING recall class3: 0.8097560975609757 TRAING recall class4: 0.7659574468085106 \n",
            "\n",
            "TRAING prec class0: 0.9844124700239808 TRAING prec class1: 0.86 TRAING prec class2: 0.853448275862069 TRAING prec class3: 0.8691099476439791 TRAING prec class4: 0.8372093023255814 \n",
            "\n",
            "TRAING f1 class0: 0.9802985074626865 TRAING f1 class1: 0.8835616438356164 TRAING f1 class2: 0.8703296703296703 TRAING f1 class3: 0.8383838383838385 TRAING f1 class4: 0.8 \n",
            "\n",
            "TRAINIG LOSS: 499 0.3103775193612091\n",
            "TRAING recall class0: 0.9789473684210527 TRAING recall class1: 0.8923512747875354 TRAING recall class2: 0.8861209964412812 TRAING recall class3: 0.82421875 TRAING recall class4: 0.7538461538461538 \n",
            "\n",
            "TRAING prec class0: 0.9846005774783445 TRAING prec class1: 0.8559782608695652 TRAING prec class2: 0.867595818815331 TRAING prec class3: 0.8473895582329317 TRAING prec class4: 0.8596491228070176 \n",
            "\n",
            "TRAING f1 class0: 0.9817658349328215 TRAING f1 class1: 0.8737864077669903 TRAING f1 class2: 0.8767605633802816 TRAING f1 class3: 0.8356435643564357 TRAING f1 class4: 0.8032786885245902 \n",
            "\n",
            "20\n",
            "eval recall class0: 0.8857142857142857 eval recall class1: 0.5483870967741935 eval recall class2: 0.5660377358490566 eval recall class3: 0.15625 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8857142857142857 eval prec class1: 0.4927536231884058 eval prec class2: 0.5172413793103449 eval prec class3: 0.19230769230769232 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.8857142857142857 eval f1 class1: 0.5190839694656489 eval f1 class2: 0.5405405405405406 eval f1 class3: 0.1724137931034483 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.847972659993937\n",
            "epoch20_global-step12393  SAVED \n",
            "TRAINIG LOSS: 99 0.2643452016497031\n",
            "TRAING recall class0: 0.9850746268656716 TRAING recall class1: 0.9746835443037974 TRAING recall class2: 0.9245283018867925 TRAING recall class3: 0.9245283018867925 TRAING recall class4: 0.7857142857142857 \n",
            "\n",
            "TRAING prec class0: 0.9850746268656716 TRAING prec class1: 0.927710843373494 TRAING prec class2: 0.9607843137254902 TRAING prec class3: 0.9245283018867925 TRAING prec class4: 0.9166666666666666 \n",
            "\n",
            "TRAING f1 class0: 0.9850746268656716 TRAING f1 class1: 0.9506172839506173 TRAING f1 class2: 0.9423076923076923 TRAING f1 class3: 0.9245283018867925 TRAING f1 class4: 0.8461538461538461 \n",
            "\n",
            "TRAINIG LOSS: 199 0.260685197385028\n",
            "TRAING recall class0: 0.9927536231884058 TRAING recall class1: 0.948051948051948 TRAING recall class2: 0.9090909090909091 TRAING recall class3: 0.8823529411764706 TRAING recall class4: 0.8064516129032258 \n",
            "\n",
            "TRAING prec class0: 0.9856115107913669 TRAING prec class1: 0.9299363057324841 TRAING prec class2: 0.8910891089108911 TRAING prec class3: 0.9278350515463918 TRAING prec class4: 0.8928571428571429 \n",
            "\n",
            "TRAING f1 class0: 0.9891696750902527 TRAING f1 class1: 0.9389067524115756 TRAING f1 class2: 0.9 TRAING f1 class3: 0.9045226130653266 TRAING f1 class4: 0.8474576271186439 \n",
            "\n",
            "TRAINIG LOSS: 299 0.26676147429427755\n",
            "TRAING recall class0: 0.9888535031847133 TRAING recall class1: 0.9398148148148148 TRAING recall class2: 0.9337748344370861 TRAING recall class3: 0.8805031446540881 TRAING recall class4: 0.8260869565217391 \n",
            "\n",
            "TRAING prec class0: 0.9904306220095693 TRAING prec class1: 0.9144144144144144 TRAING prec class2: 0.8980891719745223 TRAING prec class3: 0.9150326797385621 TRAING prec class4: 0.926829268292683 \n",
            "\n",
            "TRAING f1 class0: 0.9896414342629481 TRAING f1 class1: 0.9269406392694065 TRAING f1 class2: 0.9155844155844156 TRAING f1 class3: 0.8974358974358974 TRAING f1 class4: 0.8735632183908046 \n",
            "\n",
            "TRAINIG LOSS: 399 0.27006703670718707\n",
            "TRAING recall class0: 0.9846153846153847 TRAING recall class1: 0.9328621908127208 TRAING recall class2: 0.9330143540669856 TRAING recall class3: 0.8620689655172413 TRAING recall class4: 0.8333333333333334 \n",
            "\n",
            "TRAING prec class0: 0.9904761904761905 TRAING prec class1: 0.9072164948453608 TRAING prec class2: 0.8823529411764706 TRAING prec class3: 0.9020618556701031 TRAING prec class4: 0.9259259259259259 \n",
            "\n",
            "TRAING f1 class0: 0.9875370919881307 TRAING f1 class1: 0.9198606271777003 TRAING f1 class2: 0.9069767441860465 TRAING f1 class3: 0.8816120906801007 TRAING f1 class4: 0.8771929824561403 \n",
            "\n",
            "TRAINIG LOSS: 499 0.28463794391974806\n",
            "TRAING recall class0: 0.9799426934097422 TRAING recall class1: 0.9342857142857143 TRAING recall class2: 0.9142857142857143 TRAING recall class3: 0.8346456692913385 TRAING recall class4: 0.8115942028985508 \n",
            "\n",
            "TRAING prec class0: 0.9865384615384616 TRAING prec class1: 0.8885869565217391 TRAING prec class2: 0.8767123287671232 TRAING prec class3: 0.8907563025210085 TRAING prec class4: 0.9032258064516129 \n",
            "\n",
            "TRAING f1 class0: 0.983229516051749 TRAING f1 class1: 0.9108635097493036 TRAING f1 class2: 0.8951048951048951 TRAING f1 class3: 0.8617886178861789 TRAING f1 class4: 0.8549618320610687 \n",
            "\n",
            "21\n",
            "eval recall class0: 0.8857142857142857 eval recall class1: 0.532258064516129 eval recall class2: 0.5660377358490566 eval recall class3: 0.1875 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8857142857142857 eval prec class1: 0.5076923076923077 eval prec class2: 0.5172413793103449 eval prec class3: 0.2 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.8857142857142857 eval f1 class1: 0.5196850393700787 eval f1 class2: 0.5405405405405406 eval f1 class3: 0.19354838709677422 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.8654474332794527\n",
            "epoch21_global-step13008  SAVED \n",
            "TRAINIG LOSS: 99 0.23889368458651006\n",
            "TRAING recall class0: 0.9906542056074766 TRAING recall class1: 0.9508196721311475 TRAING recall class2: 0.9636363636363636 TRAING recall class3: 0.86 TRAING recall class4: 0.85 \n",
            "\n",
            "TRAING prec class0: 0.9953051643192489 TRAING prec class1: 0.9354838709677419 TRAING prec class2: 0.8833333333333333 TRAING prec class3: 0.8958333333333334 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9929742388758782 TRAING f1 class1: 0.943089430894309 TRAING f1 class2: 0.9217391304347826 TRAING f1 class3: 0.8775510204081632 TRAING f1 class4: 0.9189189189189189 \n",
            "\n",
            "TRAINIG LOSS: 199 0.2606585532007739\n",
            "TRAING recall class0: 0.9858156028368794 TRAING recall class1: 0.92 TRAING recall class2: 0.9375 TRAING recall class3: 0.875 TRAING recall class4: 0.7857142857142857 \n",
            "\n",
            "TRAING prec class0: 0.9858156028368794 TRAING prec class1: 0.92 TRAING prec class2: 0.875 TRAING prec class3: 0.9074074074074074 TRAING prec class4: 0.9166666666666666 \n",
            "\n",
            "TRAING f1 class0: 0.9858156028368794 TRAING f1 class1: 0.92 TRAING f1 class2: 0.9051724137931034 TRAING f1 class3: 0.8909090909090909 TRAING f1 class4: 0.8461538461538461 \n",
            "\n",
            "TRAINIG LOSS: 299 0.24915217969721803\n",
            "TRAING recall class0: 0.9858044164037855 TRAING recall class1: 0.9195979899497487 TRAING recall class2: 0.9404761904761905 TRAING recall class3: 0.8789808917197452 TRAING recall class4: 0.8571428571428571 \n",
            "\n",
            "TRAING prec class0: 0.9904912836767037 TRAING prec class1: 0.9336734693877551 TRAING prec class2: 0.8729281767955801 TRAING prec class3: 0.9019607843137255 TRAING prec class4: 0.9230769230769231 \n",
            "\n",
            "TRAING f1 class0: 0.9881422924901186 TRAING f1 class1: 0.9265822784810125 TRAING f1 class2: 0.9054441260744985 TRAING f1 class3: 0.8903225806451612 TRAING f1 class4: 0.888888888888889 \n",
            "\n",
            "TRAINIG LOSS: 399 0.25182230504986364\n",
            "TRAING recall class0: 0.9880810488676997 TRAING recall class1: 0.9078014184397163 TRAING recall class2: 0.9388646288209607 TRAING recall class3: 0.8814432989690721 TRAING recall class4: 0.8035714285714286 \n",
            "\n",
            "TRAING prec class0: 0.986904761904762 TRAING prec class1: 0.9411764705882353 TRAING prec class2: 0.8884297520661157 TRAING prec class3: 0.8724489795918368 TRAING prec class4: 0.9 \n",
            "\n",
            "TRAING f1 class0: 0.9874925550923169 TRAING f1 class1: 0.924187725631769 TRAING f1 class2: 0.9129511677282378 TRAING f1 class3: 0.8769230769230768 TRAING f1 class4: 0.8490566037735849 \n",
            "\n",
            "TRAINIG LOSS: 499 0.25456677547143775\n",
            "TRAING recall class0: 0.9875717017208413 TRAING recall class1: 0.9230769230769231 TRAING recall class2: 0.9263157894736842 TRAING recall class3: 0.8669354838709677 TRAING recall class4: 0.7714285714285715 \n",
            "\n",
            "TRAING prec class0: 0.9885167464114832 TRAING prec class1: 0.9178470254957507 TRAING prec class2: 0.8888888888888888 TRAING prec class3: 0.8775510204081632 TRAING prec class4: 0.9 \n",
            "\n",
            "TRAING f1 class0: 0.9880439980870397 TRAING f1 class1: 0.9204545454545455 TRAING f1 class2: 0.9072164948453608 TRAING f1 class3: 0.8722109533468559 TRAING f1 class4: 0.8307692307692307 \n",
            "\n",
            "22\n",
            "eval recall class0: 0.8761904761904762 eval recall class1: 0.532258064516129 eval recall class2: 0.6037735849056604 eval recall class3: 0.25 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8932038834951457 eval prec class1: 0.4925373134328358 eval prec class2: 0.5423728813559322 eval prec class3: 0.25806451612903225 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.8846153846153847 eval f1 class1: 0.5116279069767442 eval f1 class2: 0.5714285714285714 eval f1 class3: 0.25396825396825395 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.8824006412279509\n",
            "epoch22_global-step13623  SAVED \n",
            "TRAINIG LOSS: 99 0.20320600162260236\n",
            "TRAING recall class0: 0.9903381642512077 TRAING recall class1: 0.9682539682539683 TRAING recall class2: 1.0 TRAING recall class3: 0.9545454545454546 TRAING recall class4: 0.5555555555555556 \n",
            "\n",
            "TRAING prec class0: 0.9855769230769231 TRAING prec class1: 0.9838709677419355 TRAING prec class2: 0.9166666666666666 TRAING prec class3: 0.9692307692307692 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9879518072289156 TRAING f1 class1: 0.976 TRAING f1 class2: 0.9565217391304348 TRAING f1 class3: 0.9618320610687022 TRAING f1 class4: 0.7142857142857143 \n",
            "\n",
            "TRAINIG LOSS: 199 0.1951659924699925\n",
            "TRAING recall class0: 0.9864253393665159 TRAING recall class1: 0.9661016949152542 TRAING recall class2: 0.9444444444444444 TRAING recall class3: 0.9385964912280702 TRAING recall class4: 0.6111111111111112 \n",
            "\n",
            "TRAING prec class0: 0.9886621315192744 TRAING prec class1: 0.9344262295081968 TRAING prec class2: 0.9272727272727272 TRAING prec class3: 0.9304347826086956 TRAING prec class4: 0.9166666666666666 \n",
            "\n",
            "TRAING f1 class0: 0.9875424688561723 TRAING f1 class1: 0.95 TRAING f1 class2: 0.9357798165137615 TRAING f1 class3: 0.9344978165938864 TRAING f1 class4: 0.7333333333333334 \n",
            "\n",
            "TRAINIG LOSS: 299 0.22237290632833417\n",
            "TRAING recall class0: 0.9795918367346939 TRAING recall class1: 0.926829268292683 TRAING recall class2: 0.9192546583850931 TRAING recall class3: 0.9325153374233128 TRAING recall class4: 0.7647058823529411 \n",
            "\n",
            "TRAING prec class0: 0.9873417721518988 TRAING prec class1: 0.9223300970873787 TRAING prec class2: 0.9079754601226994 TRAING prec class3: 0.8837209302325582 TRAING prec class4: 0.9629629629629629 \n",
            "\n",
            "TRAING f1 class0: 0.983451536643026 TRAING f1 class1: 0.9245742092457422 TRAING f1 class2: 0.9135802469135801 TRAING f1 class3: 0.9074626865671642 TRAING f1 class4: 0.8524590163934426 \n",
            "\n",
            "TRAINIG LOSS: 399 0.22904425893269945\n",
            "TRAING recall class0: 0.9787985865724381 TRAING recall class1: 0.9244604316546763 TRAING recall class2: 0.9128440366972477 TRAING recall class3: 0.9182692307692307 TRAING recall class4: 0.7659574468085106 \n",
            "\n",
            "TRAING prec class0: 0.9869358669833729 TRAING prec class1: 0.9081272084805654 TRAING prec class2: 0.9004524886877828 TRAING prec class3: 0.8925233644859814 TRAING prec class4: 0.9 \n",
            "\n",
            "TRAING f1 class0: 0.982850384387936 TRAING f1 class1: 0.9162210338680927 TRAING f1 class2: 0.9066059225512529 TRAING f1 class3: 0.9052132701421802 TRAING f1 class4: 0.8275862068965516 \n",
            "\n",
            "TRAINIG LOSS: 499 0.2271072475113906\n",
            "TRAING recall class0: 0.9829222011385199 TRAING recall class1: 0.9257142857142857 TRAING recall class2: 0.910394265232975 TRAING recall class3: 0.9123505976095617 TRAING recall class4: 0.7575757575757576 \n",
            "\n",
            "TRAING prec class0: 0.9885496183206107 TRAING prec class1: 0.9101123595505618 TRAING prec class2: 0.900709219858156 TRAING prec class3: 0.8875968992248062 TRAING prec class4: 0.8928571428571429 \n",
            "\n",
            "TRAING f1 class0: 0.9857278782112274 TRAING f1 class1: 0.9178470254957507 TRAING f1 class2: 0.9055258467023173 TRAING f1 class3: 0.899803536345776 TRAING f1 class4: 0.819672131147541 \n",
            "\n",
            "23\n",
            "eval recall class0: 0.8809523809523809 eval recall class1: 0.5806451612903226 eval recall class2: 0.5094339622641509 eval recall class3: 0.15625 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.8851674641148325 eval prec class1: 0.45569620253164556 eval prec class2: 0.5294117647058824 eval prec class3: 0.20833333333333334 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.883054892601432 eval f1 class1: 0.5106382978723404 eval f1 class2: 0.5192307692307693 eval f1 class3: 0.17857142857142858 eval f1 class4: 0.5833333333333334 \n",
            "\n",
            "eval LOSS: 0.9065904436891129\n",
            "epoch23_global-step14238  SAVED \n",
            "TRAINIG LOSS: 99 0.21422202777583152\n",
            "TRAING recall class0: 0.9907834101382489 TRAING recall class1: 0.9545454545454546 TRAING recall class2: 0.8679245283018868 TRAING recall class3: 0.9433962264150944 TRAING recall class4: 0.9090909090909091 \n",
            "\n",
            "TRAING prec class0: 0.9953703703703703 TRAING prec class1: 0.8873239436619719 TRAING prec class2: 0.92 TRAING prec class3: 0.9433962264150944 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9930715935334872 TRAING f1 class1: 0.9197080291970803 TRAING f1 class2: 0.8932038834951457 TRAING f1 class3: 0.9433962264150944 TRAING f1 class4: 0.9523809523809523 \n",
            "\n",
            "TRAINIG LOSS: 199 0.19922617666306905\n",
            "TRAING recall class0: 0.9905437352245863 TRAING recall class1: 0.9705882352941176 TRAING recall class2: 0.9152542372881356 TRAING recall class3: 0.8958333333333334 TRAING recall class4: 0.8518518518518519 \n",
            "\n",
            "TRAING prec class0: 0.9928909952606635 TRAING prec class1: 0.9230769230769231 TRAING prec class2: 0.9230769230769231 TRAING prec class3: 0.9148936170212766 TRAING prec class4: 0.9583333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.991715976331361 TRAING f1 class1: 0.946236559139785 TRAING f1 class2: 0.9191489361702128 TRAING f1 class3: 0.9052631578947369 TRAING f1 class4: 0.9019607843137256 \n",
            "\n",
            "TRAINIG LOSS: 299 0.20156850369569534\n",
            "TRAING recall class0: 0.9871382636655949 TRAING recall class1: 0.9727272727272728 TRAING recall class2: 0.9152542372881356 TRAING recall class3: 0.9071428571428571 TRAING recall class4: 0.8780487804878049 \n",
            "\n",
            "TRAING prec class0: 0.9935275080906149 TRAING prec class1: 0.9385964912280702 TRAING prec class2: 0.9364161849710982 TRAING prec class3: 0.900709219858156 TRAING prec class4: 0.9 \n",
            "\n",
            "TRAING f1 class0: 0.9903225806451612 TRAING f1 class1: 0.955357142857143 TRAING f1 class2: 0.9257142857142857 TRAING f1 class3: 0.9039145907473309 TRAING f1 class4: 0.888888888888889 \n",
            "\n",
            "TRAINIG LOSS: 399 0.2008450093009742\n",
            "TRAING recall class0: 0.988009592326139 TRAING recall class1: 0.9619377162629758 TRAING recall class2: 0.9285714285714286 TRAING recall class3: 0.9095477386934674 TRAING recall class4: 0.8888888888888888 \n",
            "\n",
            "TRAING prec class0: 0.9927710843373494 TRAING prec class1: 0.9423728813559322 TRAING prec class2: 0.9203539823008849 TRAING prec class3: 0.9187817258883249 TRAING prec class4: 0.9230769230769231 \n",
            "\n",
            "TRAING f1 class0: 0.9903846153846153 TRAING f1 class1: 0.952054794520548 TRAING f1 class2: 0.9244444444444444 TRAING f1 class3: 0.9141414141414141 TRAING f1 class4: 0.9056603773584906 \n",
            "\n",
            "TRAINIG LOSS: 499 0.19895629848027602\n",
            "TRAING recall class0: 0.9884947267497604 TRAING recall class1: 0.9634831460674157 TRAING recall class2: 0.9395017793594306 TRAING recall class3: 0.9133858267716536 TRAING recall class4: 0.8636363636363636 \n",
            "\n",
            "TRAING prec class0: 0.9923002887391723 TRAING prec class1: 0.9449035812672176 TRAING prec class2: 0.9198606271777003 TRAING prec class3: 0.928 TRAING prec class4: 0.9344262295081968 \n",
            "\n",
            "TRAING f1 class0: 0.9903938520653218 TRAING f1 class1: 0.9541029207232267 TRAING f1 class2: 0.9295774647887323 TRAING f1 class3: 0.9206349206349207 TRAING f1 class4: 0.8976377952755905 \n",
            "\n",
            "24\n",
            "eval recall class0: 0.8809523809523809 eval recall class1: 0.5483870967741935 eval recall class2: 0.6415094339622641 eval recall class3: 0.21875 eval recall class4: 0.4666666666666667 \n",
            "\n",
            "eval prec class0: 0.893719806763285 eval prec class1: 0.53125 eval prec class2: 0.5573770491803278 eval prec class3: 0.23333333333333334 eval prec class4: 0.7 \n",
            "\n",
            "eval f1 class0: 0.8872901678657075 eval f1 class1: 0.5396825396825397 eval f1 class2: 0.5964912280701753 eval f1 class3: 0.22580645161290322 eval f1 class4: 0.56 \n",
            "\n",
            "eval LOSS: 0.9299187098880318\n",
            "epoch24_global-step14853  SAVED \n",
            "TRAINIG LOSS: 99 0.20491863979026675\n",
            "TRAING recall class0: 0.9849246231155779 TRAING recall class1: 0.9054054054054054 TRAING recall class2: 0.9516129032258065 TRAING recall class3: 0.9148936170212766 TRAING recall class4: 0.8888888888888888 \n",
            "\n",
            "TRAING prec class0: 0.9949238578680203 TRAING prec class1: 0.9571428571428572 TRAING prec class2: 0.921875 TRAING prec class3: 0.8431372549019608 TRAING prec class4: 0.8888888888888888 \n",
            "\n",
            "TRAING f1 class0: 0.9898989898989898 TRAING f1 class1: 0.9305555555555555 TRAING f1 class2: 0.9365079365079365 TRAING f1 class3: 0.8775510204081632 TRAING f1 class4: 0.8888888888888888 \n",
            "\n",
            "TRAINIG LOSS: 199 0.18304884910350666\n",
            "TRAING recall class0: 0.9924812030075187 TRAING recall class1: 0.9361702127659575 TRAING recall class2: 0.9689922480620154 TRAING recall class3: 0.9320388349514563 TRAING recall class4: 0.8928571428571429 \n",
            "\n",
            "TRAING prec class0: 0.9974811083123426 TRAING prec class1: 0.9635036496350365 TRAING prec class2: 0.9541984732824428 TRAING prec class3: 0.9056603773584906 TRAING prec class4: 0.8620689655172413 \n",
            "\n",
            "TRAING f1 class0: 0.9949748743718593 TRAING f1 class1: 0.9496402877697842 TRAING f1 class2: 0.9615384615384615 TRAING f1 class3: 0.9186602870813397 TRAING f1 class4: 0.8771929824561403 \n",
            "\n",
            "TRAINIG LOSS: 299 0.17805970231226334\n",
            "TRAING recall class0: 0.9918032786885246 TRAING recall class1: 0.9395348837209302 TRAING recall class2: 0.9655172413793104 TRAING recall class3: 0.9559748427672956 TRAING recall class4: 0.8809523809523809 \n",
            "\n",
            "TRAING prec class0: 0.9967051070840197 TRAING prec class1: 0.9619047619047619 TRAING prec class2: 0.9438202247191011 TRAING prec class3: 0.9325153374233128 TRAING prec class4: 0.8809523809523809 \n",
            "\n",
            "TRAING f1 class0: 0.9942481511914544 TRAING f1 class1: 0.9505882352941177 TRAING f1 class2: 0.9545454545454545 TRAING f1 class3: 0.9440993788819876 TRAING f1 class4: 0.8809523809523809 \n",
            "\n",
            "TRAINIG LOSS: 399 0.18091481959738304\n",
            "TRAING recall class0: 0.9891826923076923 TRAING recall class1: 0.9468085106382979 TRAING recall class2: 0.9563318777292577 TRAING recall class3: 0.9471153846153846 TRAING recall class4: 0.8571428571428571 \n",
            "\n",
            "TRAING prec class0: 0.9939613526570048 TRAING prec class1: 0.956989247311828 TRAING prec class2: 0.9399141630901288 TRAING prec class3: 0.9292452830188679 TRAING prec class4: 0.875 \n",
            "\n",
            "TRAING f1 class0: 0.9915662650602409 TRAING f1 class1: 0.9518716577540107 TRAING f1 class2: 0.948051948051948 TRAING f1 class3: 0.9380952380952382 TRAING f1 class4: 0.8659793814432989 \n",
            "\n",
            "TRAINIG LOSS: 499 0.181429540167097\n",
            "TRAING recall class0: 0.9886148007590133 TRAING recall class1: 0.9488636363636364 TRAING recall class2: 0.95 TRAING recall class3: 0.9435483870967742 TRAING recall class4: 0.8333333333333334 \n",
            "\n",
            "TRAING prec class0: 0.9942748091603053 TRAING prec class1: 0.9515669515669516 TRAING prec class2: 0.9399293286219081 TRAING prec class3: 0.9105058365758755 TRAING prec class4: 0.9016393442622951 \n",
            "\n",
            "TRAING f1 class0: 0.9914367269267363 TRAING f1 class1: 0.9502133712660028 TRAING f1 class2: 0.9449378330373002 TRAING f1 class3: 0.9267326732673267 TRAING f1 class4: 0.8661417322834646 \n",
            "\n",
            "25\n",
            "eval recall class0: 0.8761904761904762 eval recall class1: 0.5161290322580645 eval recall class2: 0.6792452830188679 eval recall class3: 0.25 eval recall class4: 0.5333333333333333 \n",
            "\n",
            "eval prec class0: 0.8975609756097561 eval prec class1: 0.5423728813559322 eval prec class2: 0.5454545454545454 eval prec class3: 0.25806451612903225 eval prec class4: 0.7272727272727273 \n",
            "\n",
            "eval f1 class0: 0.8867469879518073 eval f1 class1: 0.5289256198347108 eval f1 class2: 0.6050420168067228 eval f1 class3: 0.25396825396825395 eval f1 class4: 0.6153846153846153 \n",
            "\n",
            "eval LOSS: 0.9435291843028213\n",
            "epoch25_global-step15468  SAVED \n",
            "TRAINIG LOSS: 99 0.17997076424304395\n",
            "TRAING recall class0: 1.0 TRAING recall class1: 0.9761904761904762 TRAING recall class2: 0.8958333333333334 TRAING recall class3: 0.95 TRAING recall class4: 1.0 \n",
            "\n",
            "TRAING prec class0: 0.9948979591836735 TRAING prec class1: 0.9425287356321839 TRAING prec class2: 0.9555555555555556 TRAING prec class3: 0.9827586206896551 TRAING prec class4: 0.9285714285714286 \n",
            "\n",
            "TRAING f1 class0: 0.9974424552429668 TRAING f1 class1: 0.95906432748538 TRAING f1 class2: 0.924731182795699 TRAING f1 class3: 0.9661016949152542 TRAING f1 class4: 0.962962962962963 \n",
            "\n",
            "TRAINIG LOSS: 199 0.16816897725570015\n",
            "TRAING recall class0: 0.9927007299270073 TRAING recall class1: 0.9733333333333334 TRAING recall class2: 0.9473684210526315 TRAING recall class3: 0.9615384615384616 TRAING recall class4: 0.9047619047619048 \n",
            "\n",
            "TRAING prec class0: 0.9975550122249389 TRAING prec class1: 0.954248366013072 TRAING prec class2: 0.9391304347826087 TRAING prec class3: 0.970873786407767 TRAING prec class4: 0.95 \n",
            "\n",
            "TRAING f1 class0: 0.9951219512195123 TRAING f1 class1: 0.9636963696369638 TRAING f1 class2: 0.9432314410480349 TRAING f1 class3: 0.966183574879227 TRAING f1 class4: 0.9268292682926829 \n",
            "\n",
            "TRAINIG LOSS: 299 0.16188218715212618\n",
            "TRAING recall class0: 0.9951923076923077 TRAING recall class1: 0.9726027397260274 TRAING recall class2: 0.9476744186046512 TRAING recall class3: 0.9594594594594594 TRAING recall class4: 0.918918918918919 \n",
            "\n",
            "TRAING prec class0: 0.9967897271268058 TRAING prec class1: 0.9594594594594594 TRAING prec class2: 0.9532163742690059 TRAING prec class3: 0.9530201342281879 TRAING prec class4: 0.9714285714285714 \n",
            "\n",
            "TRAING f1 class0: 0.9959903769045709 TRAING f1 class1: 0.9659863945578231 TRAING f1 class2: 0.9504373177842567 TRAING f1 class3: 0.9562289562289561 TRAING f1 class4: 0.9444444444444445 \n",
            "\n",
            "TRAINIG LOSS: 399 0.15826791435771156\n",
            "TRAING recall class0: 0.9928057553956835 TRAING recall class1: 0.975609756097561 TRAING recall class2: 0.956140350877193 TRAING recall class3: 0.9545454545454546 TRAING recall class4: 0.9433962264150944 \n",
            "\n",
            "TRAING prec class0: 0.9963898916967509 TRAING prec class1: 0.9621993127147767 TRAING prec class2: 0.956140350877193 TRAING prec class3: 0.9545454545454546 TRAING prec class4: 0.9615384615384616 \n",
            "\n",
            "TRAING f1 class0: 0.9945945945945946 TRAING f1 class1: 0.9688581314878891 TRAING f1 class2: 0.956140350877193 TRAING f1 class3: 0.9545454545454546 TRAING f1 class4: 0.9523809523809524 \n",
            "\n",
            "TRAINIG LOSS: 499 0.15844438352994622\n",
            "TRAING recall class0: 0.9932692307692308 TRAING recall class1: 0.9715909090909091 TRAING recall class2: 0.9507042253521126 TRAING recall class3: 0.9610894941634242 TRAING recall class4: 0.9552238805970149 \n",
            "\n",
            "TRAING prec class0: 0.996142719382835 TRAING prec class1: 0.9661016949152542 TRAING prec class2: 0.9574468085106383 TRAING prec class3: 0.95 TRAING prec class4: 0.9552238805970149 \n",
            "\n",
            "TRAING f1 class0: 0.9947038998555608 TRAING f1 class1: 0.9688385269121812 TRAING f1 class2: 0.9540636042402827 TRAING f1 class3: 0.9555125725338491 TRAING f1 class4: 0.9552238805970149 \n",
            "\n",
            "26\n",
            "eval recall class0: 0.8952380952380953 eval recall class1: 0.5967741935483871 eval recall class2: 0.5849056603773585 eval recall class3: 0.1875 eval recall class4: 0.5333333333333333 \n",
            "\n",
            "eval prec class0: 0.8826291079812206 eval prec class1: 0.5285714285714286 eval prec class2: 0.5740740740740741 eval prec class3: 0.25 eval prec class4: 0.7272727272727273 \n",
            "\n",
            "eval f1 class0: 0.888888888888889 eval f1 class1: 0.5606060606060607 eval f1 class2: 0.5794392523364486 eval f1 class3: 0.21428571428571427 eval f1 class4: 0.6153846153846153 \n",
            "\n",
            "eval LOSS: 0.9687391636796984\n",
            "epoch26_global-step16083  SAVED \n",
            "TRAINIG LOSS: 99 0.12194890375481919\n",
            "TRAING recall class0: 1.0 TRAING recall class1: 0.9714285714285714 TRAING recall class2: 0.9661016949152542 TRAING recall class3: 0.9285714285714286 TRAING recall class4: 0.8888888888888888 \n",
            "\n",
            "TRAING prec class0: 0.995475113122172 TRAING prec class1: 0.9855072463768116 TRAING prec class2: 0.9344262295081968 TRAING prec class3: 0.9512195121951219 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9977324263038548 TRAING f1 class1: 0.9784172661870504 TRAING f1 class2: 0.95 TRAING f1 class3: 0.9397590361445782 TRAING f1 class4: 0.9411764705882353 \n",
            "\n",
            "TRAINIG LOSS: 199 0.1395111959881615\n",
            "TRAING recall class0: 0.9954233409610984 TRAING recall class1: 0.9795918367346939 TRAING recall class2: 0.9495798319327731 TRAING recall class3: 0.9358974358974359 TRAING recall class4: 0.8421052631578947 \n",
            "\n",
            "TRAING prec class0: 0.9931506849315068 TRAING prec class1: 0.972972972972973 TRAING prec class2: 0.9495798319327731 TRAING prec class3: 0.9358974358974359 TRAING prec class4: 0.9411764705882353 \n",
            "\n",
            "TRAING f1 class0: 0.9942857142857143 TRAING f1 class1: 0.976271186440678 TRAING f1 class2: 0.9495798319327731 TRAING f1 class3: 0.9358974358974359 TRAING f1 class4: 0.8888888888888888 \n",
            "\n",
            "TRAINIG LOSS: 299 0.13763504722310851\n",
            "TRAING recall class0: 0.9953051643192489 TRAING recall class1: 0.9851485148514851 TRAING recall class2: 0.9617486338797814 TRAING recall class3: 0.9562043795620438 TRAING recall class4: 0.9230769230769231 \n",
            "\n",
            "TRAING prec class0: 0.9953051643192489 TRAING prec class1: 0.9802955665024631 TRAING prec class2: 0.9617486338797814 TRAING prec class3: 0.9492753623188406 TRAING prec class4: 0.972972972972973 \n",
            "\n",
            "TRAING f1 class0: 0.9953051643192489 TRAING f1 class1: 0.9827160493827161 TRAING f1 class2: 0.9617486338797814 TRAING f1 class3: 0.9527272727272728 TRAING f1 class4: 0.9473684210526315 \n",
            "\n",
            "TRAINIG LOSS: 399 0.13768761279439787\n",
            "TRAING recall class0: 0.996437054631829 TRAING recall class1: 0.9709090909090909 TRAING recall class2: 0.9650655021834061 TRAING recall class3: 0.96 TRAING recall class4: 0.9074074074074074 \n",
            "\n",
            "TRAING prec class0: 0.9952550415183867 TRAING prec class1: 0.9816176470588235 TRAING prec class2: 0.9525862068965517 TRAING prec class3: 0.9458128078817734 TRAING prec class4: 0.98 \n",
            "\n",
            "TRAING f1 class0: 0.9958456973293768 TRAING f1 class1: 0.976234003656307 TRAING f1 class2: 0.9587852494577007 TRAING f1 class3: 0.9528535980148883 TRAING f1 class4: 0.9423076923076924 \n",
            "\n",
            "TRAINIG LOSS: 499 0.13379654216277412\n",
            "TRAING recall class0: 0.9971590909090909 TRAING recall class1: 0.9714285714285714 TRAING recall class2: 0.9675090252707581 TRAING recall class3: 0.9595141700404858 TRAING recall class4: 0.9142857142857143 \n",
            "\n",
            "TRAING prec class0: 0.9962157048249763 TRAING prec class1: 0.9798270893371758 TRAING prec class2: 0.950354609929078 TRAING prec class3: 0.9518072289156626 TRAING prec class4: 0.9846153846153847 \n",
            "\n",
            "TRAING f1 class0: 0.9966871746332229 TRAING f1 class1: 0.975609756097561 TRAING f1 class2: 0.9588550983899822 TRAING f1 class3: 0.9556451612903225 TRAING f1 class4: 0.9481481481481482 \n",
            "\n",
            "27\n",
            "eval recall class0: 0.8952380952380953 eval recall class1: 0.532258064516129 eval recall class2: 0.5471698113207547 eval recall class3: 0.21875 eval recall class4: 0.5333333333333333 \n",
            "\n",
            "eval prec class0: 0.8826291079812206 eval prec class1: 0.5 eval prec class2: 0.5370370370370371 eval prec class3: 0.25 eval prec class4: 0.7272727272727273 \n",
            "\n",
            "eval f1 class0: 0.888888888888889 eval f1 class1: 0.515625 eval f1 class2: 0.5420560747663552 eval f1 class3: 0.23333333333333334 eval f1 class4: 0.6153846153846153 \n",
            "\n",
            "eval LOSS: 1.0018073548720998\n",
            "epoch27_global-step16698  SAVED \n",
            "TRAINIG LOSS: 99 0.11243207773659378\n",
            "TRAING recall class0: 1.0 TRAING recall class1: 0.9848484848484849 TRAING recall class2: 0.9830508474576272 TRAING recall class3: 0.9795918367346939 TRAING recall class4: 0.9090909090909091 \n",
            "\n",
            "TRAING prec class0: 1.0 TRAING prec class1: 0.9848484848484849 TRAING prec class2: 0.9830508474576272 TRAING prec class3: 0.96 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 1.0 TRAING f1 class1: 0.9848484848484849 TRAING f1 class2: 0.9830508474576272 TRAING f1 class3: 0.9696969696969697 TRAING f1 class4: 0.9523809523809523 \n",
            "\n",
            "TRAINIG LOSS: 199 0.12095898310304619\n",
            "TRAING recall class0: 0.995249406175772 TRAING recall class1: 0.9712230215827338 TRAING recall class2: 0.981651376146789 TRAING recall class3: 0.9809523809523809 TRAING recall class4: 0.9230769230769231 \n",
            "\n",
            "TRAING prec class0: 0.9976190476190476 TRAING prec class1: 0.9854014598540146 TRAING prec class2: 0.981651376146789 TRAING prec class3: 0.9626168224299065 TRAING prec class4: 0.8888888888888888 \n",
            "\n",
            "TRAING f1 class0: 0.9964328180737217 TRAING f1 class1: 0.9782608695652174 TRAING f1 class2: 0.981651376146789 TRAING f1 class3: 0.9716981132075471 TRAING f1 class4: 0.9056603773584906 \n",
            "\n",
            "TRAINIG LOSS: 299 0.12215578521136194\n",
            "TRAING recall class0: 0.9937304075235109 TRAING recall class1: 0.9760765550239234 TRAING recall class2: 0.9683544303797469 TRAING recall class3: 0.9738562091503268 TRAING recall class4: 0.9285714285714286 \n",
            "\n",
            "TRAING prec class0: 0.9952904238618524 TRAING prec class1: 0.9807692307692307 TRAING prec class2: 0.9745222929936306 TRAING prec class3: 0.9551282051282052 TRAING prec class4: 0.9285714285714286 \n",
            "\n",
            "TRAING f1 class0: 0.9945098039215686 TRAING f1 class1: 0.9784172661870504 TRAING f1 class2: 0.9714285714285714 TRAING f1 class3: 0.964401294498382 TRAING f1 class4: 0.9285714285714286 \n",
            "\n",
            "TRAINIG LOSS: 399 0.11998386391962412\n",
            "TRAING recall class0: 0.9951923076923077 TRAING recall class1: 0.9822695035460993 TRAING recall class2: 0.9692982456140351 TRAING recall class3: 0.9698492462311558 TRAING recall class4: 0.9152542372881356 \n",
            "\n",
            "TRAING prec class0: 0.9963898916967509 TRAING prec class1: 0.9787985865724381 TRAING prec class2: 0.9822222222222222 TRAING prec class3: 0.9554455445544554 TRAING prec class4: 0.9152542372881356 \n",
            "\n",
            "TRAING f1 class0: 0.9957907396271798 TRAING f1 class1: 0.9805309734513273 TRAING f1 class2: 0.9757174392935983 TRAING f1 class3: 0.9625935162094762 TRAING f1 class4: 0.9152542372881356 \n",
            "\n",
            "TRAINIG LOSS: 499 0.1169570596597623\n",
            "TRAING recall class0: 0.9961941008563273 TRAING recall class1: 0.9828080229226361 TRAING recall class2: 0.975177304964539 TRAING recall class3: 0.967741935483871 TRAING recall class4: 0.9285714285714286 \n",
            "\n",
            "TRAING prec class0: 0.9971428571428571 TRAING prec class1: 0.9772079772079773 TRAING prec class2: 0.9821428571428571 TRAING prec class3: 0.963855421686747 TRAING prec class4: 0.9285714285714286 \n",
            "\n",
            "TRAING f1 class0: 0.9966682532127559 TRAING f1 class1: 0.9800000000000001 TRAING f1 class2: 0.9786476868327402 TRAING f1 class3: 0.96579476861167 TRAING f1 class4: 0.9285714285714286 \n",
            "\n",
            "28\n",
            "eval recall class0: 0.8904761904761904 eval recall class1: 0.5967741935483871 eval recall class2: 0.5471698113207547 eval recall class3: 0.1875 eval recall class4: 0.5333333333333333 \n",
            "\n",
            "eval prec class0: 0.8947368421052632 eval prec class1: 0.5 eval prec class2: 0.5370370370370371 eval prec class3: 0.25 eval prec class4: 0.7272727272727273 \n",
            "\n",
            "eval f1 class0: 0.8926014319809068 eval f1 class1: 0.5441176470588236 eval f1 class2: 0.5420560747663552 eval f1 class3: 0.21428571428571427 eval f1 class4: 0.6153846153846153 \n",
            "\n",
            "eval LOSS: 1.0271310409019783\n",
            "epoch28_global-step17313  SAVED \n",
            "TRAINIG LOSS: 99 0.10918850451358594\n",
            "TRAING recall class0: 0.9907407407407407 TRAING recall class1: 1.0 TRAING recall class2: 0.9830508474576272 TRAING recall class3: 0.975 TRAING recall class4: 0.8125 \n",
            "\n",
            "TRAING prec class0: 1.0 TRAING prec class1: 0.971830985915493 TRAING prec class2: 0.9830508474576272 TRAING prec class3: 0.9069767441860465 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9953488372093023 TRAING f1 class1: 0.9857142857142858 TRAING f1 class2: 0.9830508474576272 TRAING f1 class3: 0.9397590361445783 TRAING f1 class4: 0.896551724137931 \n",
            "\n",
            "TRAINIG LOSS: 199 0.09720204415207263\n",
            "TRAING recall class0: 0.9930394431554525 TRAING recall class1: 1.0 TRAING recall class2: 0.9819819819819819 TRAING recall class3: 0.978021978021978 TRAING recall class4: 0.9117647058823529 \n",
            "\n",
            "TRAING prec class0: 1.0 TRAING prec class1: 0.9637681159420289 TRAING prec class2: 0.990909090909091 TRAING prec class3: 0.956989247311828 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9965075669383004 TRAING f1 class1: 0.981549815498155 TRAING f1 class2: 0.9864253393665158 TRAING f1 class3: 0.967391304347826 TRAING f1 class4: 0.9538461538461539 \n",
            "\n",
            "TRAINIG LOSS: 299 0.10012083595540995\n",
            "TRAING recall class0: 0.995334370139969 TRAING recall class1: 0.9855769230769231 TRAING recall class2: 0.975609756097561 TRAING recall class3: 0.9787234042553191 TRAING recall class4: 0.9318181818181818 \n",
            "\n",
            "TRAING prec class0: 0.9968847352024922 TRAING prec class1: 0.9761904761904762 TRAING prec class2: 0.9876543209876543 TRAING prec class3: 0.965034965034965 TRAING prec class4: 0.9534883720930233 \n",
            "\n",
            "TRAING f1 class0: 0.9961089494163424 TRAING f1 class1: 0.9808612440191389 TRAING f1 class2: 0.9815950920245398 TRAING f1 class3: 0.971830985915493 TRAING f1 class4: 0.942528735632184 \n",
            "\n",
            "TRAINIG LOSS: 399 0.10294421573664295\n",
            "TRAING recall class0: 0.9952267303102625 TRAING recall class1: 0.9893238434163701 TRAING recall class2: 0.9824561403508771 TRAING recall class3: 0.9748743718592965 TRAING recall class4: 0.9259259259259259 \n",
            "\n",
            "TRAING prec class0: 0.9976076555023924 TRAING prec class1: 0.9754385964912281 TRAING prec class2: 0.9911504424778761 TRAING prec class3: 0.9651741293532339 TRAING prec class4: 0.9615384615384616 \n",
            "\n",
            "TRAING f1 class0: 0.9964157706093191 TRAING f1 class1: 0.9823321554770319 TRAING f1 class2: 0.9867841409691629 TRAING f1 class3: 0.97 TRAING f1 class4: 0.9433962264150944 \n",
            "\n",
            "TRAINIG LOSS: 499 0.09999370239791461\n",
            "TRAING recall class0: 0.9962121212121212 TRAING recall class1: 0.9914285714285714 TRAING recall class2: 0.9855595667870036 TRAING recall class3: 0.972 TRAING recall class4: 0.9402985074626866 \n",
            "\n",
            "TRAING prec class0: 0.9981024667931688 TRAING prec class1: 0.9774647887323944 TRAING prec class2: 0.9891304347826086 TRAING prec class3: 0.972 TRAING prec class4: 0.9692307692307692 \n",
            "\n",
            "TRAING f1 class0: 0.9971563981042654 TRAING f1 class1: 0.9843971631205674 TRAING f1 class2: 0.9873417721518988 TRAING f1 class3: 0.972 TRAING f1 class4: 0.9545454545454547 \n",
            "\n",
            "29\n",
            "eval recall class0: 0.8952380952380953 eval recall class1: 0.5806451612903226 eval recall class2: 0.6226415094339622 eval recall class3: 0.21875 eval recall class4: 0.5333333333333333 \n",
            "\n",
            "eval prec class0: 0.8909952606635071 eval prec class1: 0.5625 eval prec class2: 0.55 eval prec class3: 0.2692307692307692 eval prec class4: 0.7272727272727273 \n",
            "\n",
            "eval f1 class0: 0.8931116389548693 eval f1 class1: 0.5714285714285715 eval f1 class2: 0.5840707964601771 eval f1 class3: 0.2413793103448276 eval f1 class4: 0.6153846153846153 \n",
            "\n",
            "eval LOSS: 1.0511539150655786\n",
            "epoch29_global-step17928  SAVED \n",
            "TRAINIG LOSS: 99 0.08583954887697473\n",
            "TRAING recall class0: 1.0 TRAING recall class1: 0.9885057471264368 TRAING recall class2: 0.9827586206896551 TRAING recall class3: 1.0 TRAING recall class4: 1.0 \n",
            "\n",
            "TRAING prec class0: 0.9946808510638298 TRAING prec class1: 1.0 TRAING prec class2: 1.0 TRAING prec class3: 1.0 TRAING prec class4: 0.95 \n",
            "\n",
            "TRAING f1 class0: 0.9973333333333333 TRAING f1 class1: 0.9942196531791908 TRAING f1 class2: 0.9913043478260869 TRAING f1 class3: 1.0 TRAING f1 class4: 0.9743589743589743 \n",
            "\n",
            "TRAINIG LOSS: 199 0.08565498944546562\n",
            "TRAING recall class0: 0.9974811083123426 TRAING recall class1: 0.9873417721518988 TRAING recall class2: 0.9907407407407407 TRAING recall class3: 0.9907407407407407 TRAING recall class4: 1.0 \n",
            "\n",
            "TRAING prec class0: 0.9974811083123426 TRAING prec class1: 0.9936305732484076 TRAING prec class2: 0.981651376146789 TRAING prec class3: 1.0 TRAING prec class4: 0.9666666666666667 \n",
            "\n",
            "TRAING f1 class0: 0.9974811083123426 TRAING f1 class1: 0.9904761904761905 TRAING f1 class2: 0.9861751152073732 TRAING f1 class3: 0.9953488372093023 TRAING f1 class4: 0.983050847457627 \n",
            "\n",
            "TRAINIG LOSS: 299 0.09058451807514455\n",
            "TRAING recall class0: 0.9967266775777414 TRAING recall class1: 0.9822222222222222 TRAING recall class2: 0.9767441860465116 TRAING recall class3: 0.9934640522875817 TRAING recall class4: 0.9230769230769231 \n",
            "\n",
            "TRAING prec class0: 0.9983606557377049 TRAING prec class1: 0.9866071428571429 TRAING prec class2: 0.9767441860465116 TRAING prec class3: 0.9743589743589743 TRAING prec class4: 0.9473684210526315 \n",
            "\n",
            "TRAING f1 class0: 0.9975429975429976 TRAING f1 class1: 0.9844097995545658 TRAING f1 class2: 0.9767441860465116 TRAING f1 class3: 0.9838187702265372 TRAING f1 class4: 0.935064935064935 \n",
            "\n",
            "TRAINIG LOSS: 399 0.09131143718754174\n",
            "TRAING recall class0: 0.997610513739546 TRAING recall class1: 0.9829351535836177 TRAING recall class2: 0.9774774774774775 TRAING recall class3: 0.9846153846153847 TRAING recall class4: 0.9245283018867925 \n",
            "\n",
            "TRAING prec class0: 0.997610513739546 TRAING prec class1: 0.9863013698630136 TRAING prec class2: 0.9774774774774775 TRAING prec class3: 0.9696969696969697 TRAING prec class4: 0.9607843137254902 \n",
            "\n",
            "TRAING f1 class0: 0.997610513739546 TRAING f1 class1: 0.9846153846153846 TRAING f1 class2: 0.9774774774774775 TRAING f1 class3: 0.9770992366412214 TRAING f1 class4: 0.9423076923076923 \n",
            "\n",
            "TRAINIG LOSS: 499 0.09068913292977959\n",
            "TRAING recall class0: 0.9962264150943396 TRAING recall class1: 0.9858757062146892 TRAING recall class2: 0.9747292418772563 TRAING recall class3: 0.9835390946502057 TRAING recall class4: 0.9242424242424242 \n",
            "\n",
            "TRAING prec class0: 0.9962264150943396 TRAING prec class1: 0.9830985915492958 TRAING prec class2: 0.9818181818181818 TRAING prec class3: 0.9715447154471545 TRAING prec class4: 0.953125 \n",
            "\n",
            "TRAING f1 class0: 0.9962264150943396 TRAING f1 class1: 0.9844851904090268 TRAING f1 class2: 0.9782608695652173 TRAING f1 class3: 0.9775051124744376 TRAING f1 class4: 0.9384615384615383 \n",
            "\n",
            "30\n",
            "eval recall class0: 0.8904761904761904 eval recall class1: 0.5483870967741935 eval recall class2: 0.6415094339622641 eval recall class3: 0.28125 eval recall class4: 0.5333333333333333 \n",
            "\n",
            "eval prec class0: 0.8947368421052632 eval prec class1: 0.5573770491803278 eval prec class2: 0.5573770491803278 eval prec class3: 0.3 eval prec class4: 0.7272727272727273 \n",
            "\n",
            "eval f1 class0: 0.8926014319809068 eval f1 class1: 0.5528455284552846 eval f1 class2: 0.5964912280701753 eval f1 class3: 0.29032258064516125 eval f1 class4: 0.6153846153846153 \n",
            "\n",
            "eval LOSS: 1.070478306387288\n",
            "epoch30_global-step18543  SAVED \n",
            "TRAINIG LOSS: 99 0.0719290692533832\n",
            "TRAING recall class0: 0.9950980392156863 TRAING recall class1: 0.9857142857142858 TRAING recall class2: 1.0 TRAING recall class3: 1.0 TRAING recall class4: 1.0 \n",
            "\n",
            "TRAING prec class0: 1.0 TRAING prec class1: 1.0 TRAING prec class2: 0.9855072463768116 TRAING prec class3: 0.9782608695652174 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9975429975429976 TRAING f1 class1: 0.9928057553956835 TRAING f1 class2: 0.9927007299270074 TRAING f1 class3: 0.989010989010989 TRAING f1 class4: 1.0 \n",
            "\n",
            "TRAINIG LOSS: 199 0.06984248470747843\n",
            "TRAING recall class0: 0.9976798143851509 TRAING recall class1: 0.9922480620155039 TRAING recall class2: 0.9827586206896551 TRAING recall class3: 0.9897959183673469 TRAING recall class4: 1.0 \n",
            "\n",
            "TRAING prec class0: 0.9976798143851509 TRAING prec class1: 0.9922480620155039 TRAING prec class2: 0.991304347826087 TRAING prec class3: 0.9897959183673469 TRAING prec class4: 0.9629629629629629 \n",
            "\n",
            "TRAING f1 class0: 0.9976798143851509 TRAING f1 class1: 0.9922480620155039 TRAING f1 class2: 0.9870129870129869 TRAING f1 class3: 0.9897959183673469 TRAING f1 class4: 0.9811320754716981 \n",
            "\n",
            "TRAINIG LOSS: 299 0.0779294295492582\n",
            "TRAING recall class0: 0.9953632148377125 TRAING recall class1: 0.9855072463768116 TRAING recall class2: 0.9807692307692307 TRAING recall class3: 0.9863945578231292 TRAING recall class4: 0.9534883720930233 \n",
            "\n",
            "TRAING prec class0: 0.9984496124031008 TRAING prec class1: 0.9855072463768116 TRAING prec class2: 0.9807692307692307 TRAING prec class3: 0.9666666666666667 TRAING prec class4: 0.9761904761904762 \n",
            "\n",
            "TRAING f1 class0: 0.9969040247678019 TRAING f1 class1: 0.9855072463768116 TRAING f1 class2: 0.9807692307692307 TRAING f1 class3: 0.9764309764309764 TRAING f1 class4: 0.9647058823529412 \n",
            "\n",
            "TRAINIG LOSS: 399 0.07820694668742362\n",
            "TRAING recall class0: 0.9964539007092199 TRAING recall class1: 0.9896193771626297 TRAING recall class2: 0.9856459330143541 TRAING recall class3: 0.9848484848484849 TRAING recall class4: 0.9137931034482759 \n",
            "\n",
            "TRAING prec class0: 0.9988151658767772 TRAING prec class1: 0.9862068965517241 TRAING prec class2: 0.9856459330143541 TRAING prec class3: 0.9605911330049262 TRAING prec class4: 0.9814814814814815 \n",
            "\n",
            "TRAING f1 class0: 0.9976331360946745 TRAING f1 class1: 0.9879101899827287 TRAING f1 class2: 0.9856459330143541 TRAING f1 class3: 0.972568578553616 TRAING f1 class4: 0.9464285714285714 \n",
            "\n",
            "TRAINIG LOSS: 499 0.07781510068359784\n",
            "TRAING recall class0: 0.997148288973384 TRAING recall class1: 0.9915966386554622 TRAING recall class2: 0.9855595667870036 TRAING recall class3: 0.9878542510121457 TRAING recall class4: 0.9253731343283582 \n",
            "\n",
            "TRAING prec class0: 0.9990476190476191 TRAING prec class1: 0.9860724233983287 TRAING prec class2: 0.9891304347826086 TRAING prec class3: 0.9682539682539683 TRAING prec class4: 0.9841269841269841 \n",
            "\n",
            "TRAING f1 class0: 0.9980970504281638 TRAING f1 class1: 0.9888268156424581 TRAING f1 class2: 0.9873417721518988 TRAING f1 class3: 0.9779559118236473 TRAING f1 class4: 0.9538461538461538 \n",
            "\n",
            "31\n",
            "eval recall class0: 0.9 eval recall class1: 0.532258064516129 eval recall class2: 0.6415094339622641 eval recall class3: 0.28125 eval recall class4: 0.5333333333333333 \n",
            "\n",
            "eval prec class0: 0.8790697674418605 eval prec class1: 0.5892857142857143 eval prec class2: 0.5573770491803278 eval prec class3: 0.32142857142857145 eval prec class4: 0.6666666666666666 \n",
            "\n",
            "eval f1 class0: 0.8894117647058823 eval f1 class1: 0.5593220338983049 eval f1 class2: 0.5964912280701753 eval f1 class3: 0.30000000000000004 eval f1 class4: 0.5925925925925926 \n",
            "\n",
            "eval LOSS: 1.1013682168426306\n",
            "epoch31_global-step19158  SAVED \n",
            "TRAINIG LOSS: 99 0.056988187786191705\n",
            "TRAING recall class0: 1.0 TRAING recall class1: 0.9864864864864865 TRAING recall class2: 0.9855072463768116 TRAING recall class3: 1.0 TRAING recall class4: 1.0 \n",
            "\n",
            "TRAING prec class0: 1.0 TRAING prec class1: 1.0 TRAING prec class2: 1.0 TRAING prec class3: 0.9459459459459459 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 1.0 TRAING f1 class1: 0.9931972789115647 TRAING f1 class2: 0.9927007299270074 TRAING f1 class3: 0.9722222222222222 TRAING f1 class4: 1.0 \n",
            "\n",
            "TRAINIG LOSS: 199 0.05693705434678122\n",
            "TRAING recall class0: 1.0 TRAING recall class1: 0.9859154929577465 TRAING recall class2: 0.9916666666666667 TRAING recall class3: 1.0 TRAING recall class4: 1.0 \n",
            "\n",
            "TRAING prec class0: 0.9976580796252927 TRAING prec class1: 1.0 TRAING prec class2: 1.0 TRAING prec class3: 0.978494623655914 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9988276670574443 TRAING f1 class1: 0.9929078014184397 TRAING f1 class2: 0.99581589958159 TRAING f1 class3: 0.9891304347826088 TRAING f1 class4: 1.0 \n",
            "\n",
            "TRAINIG LOSS: 299 0.062128737943712625\n",
            "TRAING recall class0: 1.0 TRAING recall class1: 0.9902912621359223 TRAING recall class2: 0.9883040935672515 TRAING recall class3: 1.0 TRAING recall class4: 0.9411764705882353 \n",
            "\n",
            "TRAING prec class0: 0.9984520123839009 TRAING prec class1: 1.0 TRAING prec class2: 1.0 TRAING prec class3: 0.9664429530201343 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9992254066615027 TRAING f1 class1: 0.9951219512195122 TRAING f1 class2: 0.9941176470588234 TRAING f1 class3: 0.9829351535836178 TRAING f1 class4: 0.9696969696969697 \n",
            "\n",
            "TRAINIG LOSS: 399 0.06492159415822243\n",
            "TRAING recall class0: 1.0 TRAING recall class1: 0.9891696750902527 TRAING recall class2: 0.9788135593220338 TRAING recall class3: 1.0 TRAING recall class4: 0.9615384615384616 \n",
            "\n",
            "TRAING prec class0: 0.9976442873969376 TRAING prec class1: 1.0 TRAING prec class2: 1.0 TRAING prec class3: 0.9690721649484536 TRAING prec class4: 0.9615384615384616 \n",
            "\n",
            "TRAING f1 class0: 0.9988207547169812 TRAING f1 class1: 0.9945553539019965 TRAING f1 class2: 0.9892933618843683 TRAING f1 class3: 0.9842931937172774 TRAING f1 class4: 0.9615384615384616 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-1399fca77b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-b44b22f43736>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;31m# if grad_accumulation_steps > 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m#     loss = loss / grad_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0;31m# if (step + 1) % grad_accumulation_steps == 0:)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXOktBrozY06"
      },
      "source": [
        "#inference on the entire dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Iunx2ofwKeX"
      },
      "source": [
        "## entire training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zCo2TSW5wN2J",
        "outputId": "ecbe002d-fb6d-4909-ab34-619ec0311970"
      },
      "source": [
        "epochs = 30\n",
        "eval_steps = 1\n",
        "output_dir = 'purpose_classification_new/entire_training' # output directory to save the best models\n",
        "bert = HDCTModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "dataset_training = Country_purpose_dataset(training = True, entire=True)\n",
        "dataset_eval = Country_purpose_dataset(training = False)\n",
        "batch_size = 4\n",
        "\n",
        "training_data_loader = DataLoader(dataset=dataset_training, batch_size=batch_size, \n",
        "                               shuffle=True, collate_fn=collate_fn)\n",
        "eval_data_loader = DataLoader(dataset=dataset_eval, batch_size=batch_size, \n",
        "                               shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "trainer = Trainer(bert, device, training_data_loader, eval_data_loader)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of HDCTModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.fc.weight', 'bert.fc.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA AMOUNT: 3737\n",
            "259\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP NUMBER 267\n",
            "SKIP LIST ['28615', '33615', '27203', '22092', '6320', '182', '18178', '1618', '29315', '31201', '21732', '22987', '36191', '19852', '21415', '3256', '200', '37233', '27230', '28714', '1172', '17225', '7065', '22995', '22391', '22335', '28336', '14531', '3264', '4452', '481', '26495', '28487', '1226', '38326', '275', '26188', '18435', '2505', '26418', '25949', '31750', '35028', '1359', '20317', '24805', '15936', '22942', '2601', '38980', '6981', '28628', '25342', '15834', '27205', '20038', '26929', '14756', '23121', '24935', '22613', '8842', '16877', '28864', '39103', '22625', '24470', '147', '320', '7214', '3006', '22622', '33437', '23742', '27791', '17122', '6263', '20444', '36268', '13369', '366', '22558', '16243', '17471', '27204', '34339', '30514', '24439', '7758', '19372', '2610', '38771', '953', '20039', '31537', '3255', '13986', '26486', '3327', '26897', '27726', '26963', '14885', '16717', '21152', '2170', '29762', '19373', '33876', '27425', '29814', '968', '5044', '23427', '17670', '21938', '6879', '319', '4289', '6546', '16301', '27969', '24244', '24469', '39431', '36080', '27206', '26288', '37207', '38211', '31202', '21463', '16982', '23639', '20984', '26823', '38283', '32605', '33616', '28064', '25269', '23210', '1253', '4108', '10693', '24771', '7284', '15614', '4277', '24642', '21441', '37784', '22086', '5063', '2142', '27836', '19579', '24363', '318', '365', '13859', '1228', '28366', '3860', '22738', '27790', '39152', '9013', '21416', '17520', '38080', '10118', '21413', '38479', '144', '27107', '33432', '29455', '13658', '2809', '35257', '26790', '2825', '6422', '2558', '31212', '19847', '148', '3445', '2837', '19871', '4826', '23394', '22562', '10587', '22848', '21435', '8722', '29108', '13429', '500', '4151', '24013', '30071', '24447', '4161', '6815', '624', '616', '21009', '2258', '26791', '34075', '27232', '6627', '20145', '1168', '1167', '31209', '25335', '31836', '15881', '22940', '29480', '32550', '5133', '12961', '2866', '37193', '4595', '1069', '21125', '33883', '31630', '1380', '3376', '16333', '21695', '33380', '21902', '2396', '17224', '31734', '2783', '316', '22154', '25225', '22941', '29728', '28342', '2219', '38656', '24068', '29490', '23053', '2850', '19526', '19160', '37171', '26887', '19010', '499', '20866', '22019', '3977', '25582', '27503']\n",
            "DATA AMOUNT: 400\n",
            "34\n",
            "SKIP\n",
            "SKIP NUMBER 35\n",
            "SKIP LIST ['5133', '12961', '2866', '37193', '4595', '1069', '21125', '33883', '31630', '1380', '3376', '16333', '21695', '33380', '21902', '2396', '17224', '31734', '2783', '316', '22154', '25225', '22941', '29728', '28342', '2219', '38656', '24068', '29490', '23053', '2850', '19526', '19160', '37171', '27503']\n",
            "0\n",
            "eval recall class0: 0.00546448087431694 eval recall class1: 0.676923076923077 eval recall class2: 0.1320754716981132 eval recall class3: 0.2641509433962264 eval recall class4: 0.0 \n",
            "\n",
            "eval prec class0: 0.16666666666666666 eval prec class1: 0.1752988047808765 eval prec class2: 0.25 eval prec class3: 0.18666666666666668 eval prec class4: 0.0 \n",
            "\n",
            "eval f1 class0: 0.010582010582010581 eval f1 class1: 0.27848101265822783 eval f1 class2: 0.17283950617283952 eval f1 class3: 0.21874999999999997 \n",
            "\n",
            "eval LOSS: 1.8115623455781202\n",
            "epoch0_global-step92  SAVED \n",
            "TRAINIG LOSS: 99 1.5742752504348756\n",
            "TRAING recall class0: 0.4063926940639269 TRAING recall class1: 0.3275862068965517 TRAING recall class2: 0.11764705882352941 TRAING recall class3: 0.15873015873015872 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5668789808917197 TRAING prec class1: 0.12666666666666668 TRAING prec class2: 0.3 TRAING prec class3: 0.14285714285714285 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.47340425531914887 TRAING f1 class1: 0.18269230769230768 TRAING f1 class2: 0.16901408450704228 TRAING f1 class3: 0.15037593984962405 \n",
            "\n",
            "TRAINIG LOSS: 199 1.4262124356627464\n",
            "TRAING recall class0: 0.6825396825396826 TRAING recall class1: 0.19469026548672566 TRAING recall class2: 0.07142857142857142 TRAING recall class3: 0.08 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5594795539033457 TRAING prec class1: 0.13664596273291926 TRAING prec class2: 0.3181818181818182 TRAING prec class3: 0.13157894736842105 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6149131767109296 TRAING f1 class1: 0.16058394160583941 TRAING f1 class2: 0.11666666666666664 TRAING f1 class3: 0.09950248756218906 \n",
            "\n",
            "TRAINIG LOSS: 299 1.347679684062799\n",
            "TRAING recall class0: 0.7872340425531915 TRAING recall class1: 0.14035087719298245 TRAING recall class2: 0.04794520547945205 TRAING recall class3: 0.06417112299465241 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.556390977443609 TRAING prec class1: 0.14545454545454545 TRAING prec class2: 0.3181818181818182 TRAING prec class3: 0.1518987341772152 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6519823788546256 TRAING f1 class1: 0.14285714285714285 TRAING f1 class2: 0.08333333333333331 TRAING f1 class3: 0.09022556390977444 \n",
            "\n",
            "TRAINIG LOSS: 399 1.3112240385264158\n",
            "TRAING recall class0: 0.8349056603773585 TRAING recall class1: 0.1092436974789916 TRAING recall class2: 0.034653465346534656 TRAING recall class3: 0.06201550387596899 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.538403041825095 TRAING prec class1: 0.1511627906976744 TRAING prec class2: 0.3181818181818182 TRAING prec class3: 0.18181818181818182 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6546463245492371 TRAING f1 class1: 0.12682926829268293 TRAING f1 class2: 0.06250000000000001 TRAING f1 class3: 0.09248554913294797 \n",
            "\n",
            "TRAINIG LOSS: 499 1.268455085873604\n",
            "TRAING recall class0: 0.8664772727272727 TRAING recall class1: 0.10410094637223975 TRAING recall class2: 0.02766798418972332 TRAING recall class3: 0.07166123778501629 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5427046263345195 TRAING prec class1: 0.1736842105263158 TRAING prec class2: 0.30434782608695654 TRAING prec class3: 0.22448979591836735 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6673960612691465 TRAING f1 class1: 0.13017751479289943 TRAING f1 class2: 0.05072463768115942 TRAING f1 class3: 0.10864197530864199 \n",
            "\n",
            "TRAINIG LOSS: 599 1.2370772146185238\n",
            "TRAING recall class0: 0.8856235107227959 TRAING recall class1: 0.109375 TRAING recall class2: 0.02280130293159609 TRAING recall class3: 0.07258064516129033 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5460333006856023 TRAING prec class1: 0.19444444444444445 TRAING prec class2: 0.28 TRAING prec class3: 0.23684210526315788 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.675552862768858 TRAING f1 class1: 0.14 TRAING f1 class2: 0.04216867469879518 TRAING f1 class3: 0.1111111111111111 \n",
            "\n",
            "TRAINIG LOSS: 699 1.2111430327381407\n",
            "TRAING recall class0: 0.8975945017182131 TRAING recall class1: 0.1352549889135255 TRAING recall class2: 0.02185792349726776 TRAING recall class3: 0.08695652173913043 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5498947368421052 TRAING prec class1: 0.23828125 TRAING prec class2: 0.2962962962962963 TRAING prec class3: 0.2733812949640288 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6819843342036553 TRAING f1 class1: 0.17256011315417255 TRAING f1 class2: 0.04071246819338422 TRAING f1 class3: 0.13194444444444445 \n",
            "\n",
            "TRAINIG LOSS: 799 1.178091642037034\n",
            "TRAING recall class0: 0.9088200238379023 TRAING recall class1: 0.1448692152917505 TRAING recall class2: 0.021634615384615384 TRAING recall class3: 0.11706349206349206 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.5641879393266741 TRAING prec class1: 0.2491349480968858 TRAING prec class2: 0.2903225806451613 TRAING prec class3: 0.3390804597701149 TRAING prec class4: 0.0 \n",
            "\n",
            "TRAING f1 class0: 0.6961880849121205 TRAING f1 class1: 0.183206106870229 TRAING f1 class2: 0.04026845637583893 TRAING f1 class3: 0.17404129793510323 \n",
            "\n",
            "1\n",
            "eval recall class0: 0.9781420765027322 eval recall class1: 0.2923076923076923 eval recall class2: 0.018867924528301886 eval recall class3: 0.24528301886792453 eval recall class4: 0.0 \n",
            "\n",
            "eval prec class0: 0.608843537414966 eval prec class1: 0.5 eval prec class2: 0.5 eval prec class3: 0.41935483870967744 \n",
            "\n",
            "eval f1 class0: 0.7505241090146749 eval f1 class1: 0.36893203883495146 eval f1 class2: 0.03636363636363636 eval f1 class3: 0.30952380952380953 \n",
            "\n",
            "eval LOSS: 0.9605415699246166\n",
            "epoch1_global-step1052  SAVED \n",
            "TRAINIG LOSS: 99 0.9882368394732475\n",
            "TRAING recall class0: 0.925 TRAING recall class1: 0.36585365853658536 TRAING recall class2: 0.07272727272727272 TRAING recall class3: 0.26 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.6445993031358885 TRAING prec class1: 0.47619047619047616 TRAING prec class2: 0.3333333333333333 TRAING prec class3: 0.34210526315789475 \n",
            "\n",
            "TRAING f1 class0: 0.7597535934291582 TRAING f1 class1: 0.41379310344827586 TRAING f1 class2: 0.11940298507462686 TRAING f1 class3: 0.29545454545454547 \n",
            "\n",
            "TRAINIG LOSS: 199 0.9836533488333226\n",
            "TRAING recall class0: 0.9290953545232273 TRAING recall class1: 0.3333333333333333 TRAING recall class2: 0.05660377358490566 TRAING recall class3: 0.26548672566371684 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.6518010291595198 TRAING prec class1: 0.4375 TRAING prec class2: 0.3 TRAING prec class3: 0.35294117647058826 \n",
            "\n",
            "TRAING f1 class0: 0.7661290322580646 TRAING f1 class1: 0.37837837837837834 TRAING f1 class2: 0.09523809523809525 TRAING f1 class3: 0.30303030303030304 \n",
            "\n",
            "TRAINIG LOSS: 299 0.9593288601189852\n",
            "TRAING recall class0: 0.9389067524115756 TRAING recall class1: 0.34536082474226804 TRAING recall class2: 0.07453416149068323 TRAING recall class3: 0.3021978021978022 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.6583990980834273 TRAING prec class1: 0.43790849673202614 TRAING prec class2: 0.4 TRAING prec class3: 0.4230769230769231 \n",
            "\n",
            "TRAING f1 class0: 0.7740225314778 TRAING f1 class1: 0.3861671469740634 TRAING f1 class2: 0.1256544502617801 TRAING f1 class3: 0.3525641025641026 \n",
            "\n",
            "TRAINIG LOSS: 399 0.9458804167620838\n",
            "TRAING recall class0: 0.9289940828402367 TRAING recall class1: 0.3424124513618677 TRAING recall class2: 0.0891089108910891 TRAING recall class3: 0.34285714285714286 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.6826086956521739 TRAING prec class1: 0.44 TRAING prec class2: 0.3829787234042553 TRAING prec class3: 0.41379310344827586 \n",
            "\n",
            "TRAING f1 class0: 0.7869674185463658 TRAING f1 class1: 0.3851203501094092 TRAING f1 class2: 0.14457831325301204 TRAING f1 class3: 0.375 \n",
            "\n",
            "TRAINIG LOSS: 499 0.9349868493974209\n",
            "TRAING recall class0: 0.9335863377609108 TRAING recall class1: 0.3394495412844037 TRAING recall class2: 0.092 TRAING recall class3: 0.343042071197411 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.6910112359550562 TRAING prec class1: 0.43873517786561267 TRAING prec class2: 0.3898305084745763 TRAING prec class3: 0.4015151515151515 \n",
            "\n",
            "TRAING f1 class0: 0.7941888619854721 TRAING f1 class1: 0.38275862068965516 TRAING f1 class2: 0.14886731391585759 TRAING f1 class3: 0.36998254799301916 \n",
            "\n",
            "TRAINIG LOSS: 599 0.9187570125609636\n",
            "TRAING recall class0: 0.9289617486338798 TRAING recall class1: 0.3641160949868074 TRAING recall class2: 0.10423452768729642 TRAING recall class3: 0.36768802228412256 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.7062314540059347 TRAING prec class1: 0.4353312302839117 TRAING prec class2: 0.43243243243243246 TRAING prec class3: 0.4074074074074074 \n",
            "\n",
            "TRAING f1 class0: 0.8024275118004046 TRAING f1 class1: 0.3965517241379311 TRAING f1 class2: 0.1679790026246719 TRAING f1 class3: 0.3865300146412884 \n",
            "\n",
            "TRAINIG LOSS: 699 0.908354638378535\n",
            "TRAING recall class0: 0.9293405114401077 TRAING recall class1: 0.36054421768707484 TRAING recall class2: 0.1295774647887324 TRAING recall class3: 0.3833718244803695 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.7170301142263759 TRAING prec class1: 0.4332425068119891 TRAING prec class2: 0.4380952380952381 TRAING prec class3: 0.4129353233830846 \n",
            "\n",
            "TRAING f1 class0: 0.809495896834701 TRAING f1 class1: 0.3935643564356436 TRAING f1 class2: 0.19999999999999996 TRAING f1 class3: 0.3976047904191617 \n",
            "\n",
            "TRAINIG LOSS: 799 0.9101145830517635\n",
            "TRAING recall class0: 0.9266547406082289 TRAING recall class1: 0.3522504892367906 TRAING recall class2: 0.14496314496314497 TRAING recall class3: 0.4194831013916501 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.7258290518449323 TRAING prec class1: 0.4326923076923077 TRAING prec class2: 0.44029850746268656 TRAING prec class3: 0.41453831041257366 \n",
            "\n",
            "TRAING f1 class0: 0.8140387637506548 TRAING f1 class1: 0.3883495145631068 TRAING f1 class2: 0.2181146025878004 TRAING f1 class3: 0.4169960474308301 \n",
            "\n",
            "2\n",
            "eval recall class0: 0.9234972677595629 eval recall class1: 0.46153846153846156 eval recall class2: 0.4528301886792453 eval recall class3: 0.5283018867924528 eval recall class4: 0.0 \n",
            "\n",
            "eval prec class0: 0.8203883495145631 eval prec class1: 0.5660377358490566 eval prec class2: 0.5853658536585366 eval prec class3: 0.4307692307692308 \n",
            "\n",
            "eval f1 class0: 0.8688946015424165 eval f1 class1: 0.5084745762711864 eval f1 class2: 0.5106382978723404 eval f1 class3: 0.4745762711864407 \n",
            "\n",
            "eval LOSS: 0.805248342863806\n",
            "epoch2_global-step2012  SAVED \n",
            "TRAINIG LOSS: 99 0.7497213124856352\n",
            "TRAING recall class0: 0.9147982062780269 TRAING recall class1: 0.40350877192982454 TRAING recall class2: 0.35185185185185186 TRAING recall class3: 0.5333333333333333 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8259109311740891 TRAING prec class1: 0.4791666666666667 TRAING prec class2: 0.5277777777777778 TRAING prec class3: 0.463768115942029 \n",
            "\n",
            "TRAING f1 class0: 0.8680851063829788 TRAING f1 class1: 0.4380952380952381 TRAING f1 class2: 0.4222222222222222 TRAING f1 class3: 0.49612403100775193 \n",
            "\n",
            "TRAINIG LOSS: 199 0.7717425340600311\n",
            "TRAING recall class0: 0.9045454545454545 TRAING recall class1: 0.4666666666666667 TRAING recall class2: 0.40384615384615385 TRAING recall class3: 0.5897435897435898 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8432203389830508 TRAING prec class1: 0.5490196078431373 TRAING prec class2: 0.5384615384615384 TRAING prec class3: 0.46621621621621623 \n",
            "\n",
            "TRAING f1 class0: 0.8728070175438597 TRAING f1 class1: 0.5045045045045046 TRAING f1 class2: 0.46153846153846156 TRAING f1 class3: 0.520754716981132 \n",
            "\n",
            "TRAINIG LOSS: 299 0.7965076101198793\n",
            "TRAING recall class0: 0.8990825688073395 TRAING recall class1: 0.4093567251461988 TRAING recall class2: 0.3821656050955414 TRAING recall class3: 0.6108108108108108 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8436154949784792 TRAING prec class1: 0.5035971223021583 TRAING prec class2: 0.48 TRAING prec class3: 0.47280334728033474 \n",
            "\n",
            "TRAING f1 class0: 0.8704663212435233 TRAING f1 class1: 0.45161290322580644 TRAING f1 class2: 0.425531914893617 TRAING f1 class3: 0.5330188679245282 \n",
            "\n",
            "TRAINIG LOSS: 399 0.8124184439983219\n",
            "TRAING recall class0: 0.8939929328621908 TRAING recall class1: 0.3922413793103448 TRAING recall class2: 0.39545454545454545 TRAING recall class3: 0.6007905138339921 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8442714126807565 TRAING prec class1: 0.4666666666666667 TRAING prec class2: 0.4915254237288136 TRAING prec class3: 0.46200607902735563 \n",
            "\n",
            "TRAING f1 class0: 0.8684210526315791 TRAING f1 class1: 0.4262295081967213 TRAING f1 class2: 0.4382871536523929 TRAING f1 class3: 0.5223367697594502 \n",
            "\n",
            "TRAINIG LOSS: 499 0.8177180061489344\n",
            "TRAING recall class0: 0.8919431279620853 TRAING recall class1: 0.39403973509933776 TRAING recall class2: 0.4312267657992565 TRAING recall class3: 0.5801282051282052 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8454627133872417 TRAING prec class1: 0.4817813765182186 TRAING prec class2: 0.5 TRAING prec class3: 0.44362745098039214 \n",
            "\n",
            "TRAING f1 class0: 0.8680811808118082 TRAING f1 class1: 0.4335154826958106 TRAING f1 class2: 0.46307385229540915 TRAING f1 class3: 0.5027777777777779 \n",
            "\n",
            "TRAINIG LOSS: 599 0.8190192707876365\n",
            "TRAING recall class0: 0.8947784810126582 TRAING recall class1: 0.407202216066482 TRAING recall class2: 0.4369230769230769 TRAING recall class3: 0.552 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8446601941747572 TRAING prec class1: 0.4883720930232558 TRAING prec class2: 0.4965034965034965 TRAING prec class3: 0.43670886075949367 \n",
            "\n",
            "TRAING f1 class0: 0.8689973107952362 TRAING f1 class1: 0.4441087613293051 TRAING f1 class2: 0.46481178396072015 TRAING f1 class3: 0.4876325088339223 \n",
            "\n",
            "TRAINIG LOSS: 699 0.8149828311401819\n",
            "TRAING recall class0: 0.8906882591093117 TRAING recall class1: 0.40425531914893614 TRAING recall class2: 0.4363143631436314 TRAING recall class3: 0.54421768707483 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.849967804249839 TRAING prec class1: 0.4776536312849162 TRAING prec class2: 0.47214076246334313 TRAING prec class3: 0.43795620437956206 \n",
            "\n",
            "TRAING f1 class0: 0.8698517298187808 TRAING f1 class1: 0.4379001280409731 TRAING f1 class2: 0.4535211267605634 TRAING f1 class3: 0.4853387259858443 \n",
            "\n",
            "TRAINIG LOSS: 799 0.8199501699558459\n",
            "TRAING recall class0: 0.8904761904761904 TRAING recall class1: 0.418 TRAING recall class2: 0.43498817966903075 TRAING recall class3: 0.5384615384615384 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8504832291074474 TRAING prec class1: 0.47392290249433106 TRAING prec class2: 0.4804177545691906 TRAING prec class3: 0.43111831442463533 \n",
            "\n",
            "TRAING f1 class0: 0.8700203547542891 TRAING f1 class1: 0.4442082890541976 TRAING f1 class2: 0.456575682382134 TRAING f1 class3: 0.4788478847884788 \n",
            "\n",
            "3\n",
            "eval recall class0: 0.9180327868852459 eval recall class1: 0.5692307692307692 eval recall class2: 0.5660377358490566 eval recall class3: 0.5660377358490566 eval recall class4: 0.09090909090909091 \n",
            "\n",
            "eval prec class0: 0.8615384615384616 eval prec class1: 0.5692307692307692 eval prec class2: 0.6521739130434783 eval prec class3: 0.5172413793103449 eval prec class4: 1.0 \n",
            "\n",
            "eval f1 class0: 0.8888888888888888 eval f1 class1: 0.5692307692307692 eval f1 class2: 0.6060606060606061 eval f1 class3: 0.5405405405405406 eval f1 class4: 0.16666666666666669 \n",
            "\n",
            "eval LOSS: 0.7300390692857596\n",
            "epoch3_global-step2972  SAVED \n",
            "TRAINIG LOSS: 99 0.7797659478336573\n",
            "TRAING recall class0: 0.9090909090909091 TRAING recall class1: 0.35714285714285715 TRAING recall class2: 0.5263157894736842 TRAING recall class3: 0.5909090909090909 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8715596330275229 TRAING prec class1: 0.4166666666666667 TRAING prec class2: 0.5555555555555556 TRAING prec class3: 0.4875 \n",
            "\n",
            "TRAING f1 class0: 0.8899297423887588 TRAING f1 class1: 0.3846153846153846 TRAING f1 class2: 0.5405405405405405 TRAING f1 class3: 0.5342465753424658 \n",
            "\n",
            "TRAINIG LOSS: 199 0.7605942056328058\n",
            "TRAING recall class0: 0.8968824940047961 TRAING recall class1: 0.4074074074074074 TRAING recall class2: 0.5585585585585585 TRAING recall class3: 0.562962962962963 TRAING recall class4: 0.0 \n",
            "\n",
            "TRAING prec class0: 0.8883610451306413 TRAING prec class1: 0.4536082474226804 TRAING prec class2: 0.5299145299145299 TRAING prec class3: 0.46060606060606063 \n",
            "\n",
            "TRAING f1 class0: 0.8926014319809068 TRAING f1 class1: 0.4292682926829268 TRAING f1 class2: 0.5438596491228069 TRAING f1 class3: 0.5066666666666667 \n",
            "\n",
            "TRAINIG LOSS: 299 0.7596214368753135\n",
            "TRAING recall class0: 0.9015873015873016 TRAING recall class1: 0.4176470588235294 TRAING recall class2: 0.5625 TRAING recall class3: 0.5846153846153846 TRAING recall class4: 0.022222222222222223 \n",
            "\n",
            "TRAING prec class0: 0.8944881889763779 TRAING prec class1: 0.4896551724137931 TRAING prec class2: 0.5487804878048781 TRAING prec class3: 0.4470588235294118 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8980237154150198 TRAING f1 class1: 0.4507936507936508 TRAING f1 class2: 0.5555555555555556 TRAING f1 class3: 0.5066666666666666 TRAING f1 class4: 0.04347826086956522 \n",
            "\n",
            "TRAINIG LOSS: 399 0.7460525404755026\n",
            "TRAING recall class0: 0.9052631578947369 TRAING recall class1: 0.4291845493562232 TRAING recall class2: 0.5454545454545454 TRAING recall class3: 0.5619834710743802 TRAING recall class4: 0.09836065573770492 \n",
            "\n",
            "TRAING prec class0: 0.9 TRAING prec class1: 0.5 TRAING prec class2: 0.5181818181818182 TRAING prec class3: 0.43312101910828027 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9026239067055394 TRAING f1 class1: 0.4618937644341801 TRAING f1 class2: 0.5314685314685315 TRAING f1 class3: 0.4892086330935252 TRAING f1 class4: 0.1791044776119403 \n",
            "\n",
            "TRAINIG LOSS: 499 0.759361639495939\n",
            "TRAING recall class0: 0.9035004730368968 TRAING recall class1: 0.4290322580645161 TRAING recall class2: 0.5169811320754717 TRAING recall class3: 0.5525423728813559 TRAING recall class4: 0.1232876712328767 \n",
            "\n",
            "TRAING prec class0: 0.8891992551210428 TRAING prec class1: 0.5 TRAING prec class2: 0.5018315018315018 TRAING prec class3: 0.4312169312169312 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8962928202721727 TRAING f1 class1: 0.4618055555555556 TRAING f1 class2: 0.5092936802973977 TRAING f1 class3: 0.48439821693907875 TRAING f1 class4: 0.2195121951219512 \n",
            "\n",
            "TRAINIG LOSS: 599 0.7686078273908545\n",
            "TRAING recall class0: 0.8950715421303657 TRAING recall class1: 0.4465753424657534 TRAING recall class2: 0.5303030303030303 TRAING recall class3: 0.5469613259668509 TRAING recall class4: 0.10588235294117647 \n",
            "\n",
            "TRAING prec class0: 0.8908227848101266 TRAING prec class1: 0.4851190476190476 TRAING prec class2: 0.5028735632183908 TRAING prec class3: 0.4469525959367946 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8929421094369546 TRAING f1 class1: 0.46504992867332384 TRAING f1 class2: 0.5162241887905605 TRAING f1 class3: 0.4919254658385093 TRAING f1 class4: 0.19148936170212766 \n",
            "\n",
            "TRAINIG LOSS: 699 0.7693497221145247\n",
            "TRAING recall class0: 0.8941655359565808 TRAING recall class1: 0.4295612009237875 TRAING recall class2: 0.5388739946380697 TRAING recall class3: 0.5471698113207547 TRAING recall class4: 0.10416666666666667 \n",
            "\n",
            "TRAING prec class0: 0.8887390424814565 TRAING prec class1: 0.49732620320855614 TRAING prec class2: 0.4902439024390244 TRAING prec class3: 0.4435946462715105 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8914440311126142 TRAING f1 class1: 0.46096654275092935 TRAING f1 class2: 0.5134099616858238 TRAING f1 class3: 0.48996832101372756 TRAING f1 class4: 0.18867924528301885 \n",
            "\n",
            "TRAINIG LOSS: 799 0.7686120535805822\n",
            "TRAING recall class0: 0.8934769599042489 TRAING recall class1: 0.458 TRAING recall class2: 0.5342789598108747 TRAING recall class3: 0.5421686746987951 TRAING recall class4: 0.1111111111111111 \n",
            "\n",
            "TRAING prec class0: 0.8902802623732856 TRAING prec class1: 0.5077605321507761 TRAING prec class2: 0.4860215053763441 TRAING prec class3: 0.45531197301854975 TRAING prec class4: 0.8571428571428571 \n",
            "\n",
            "TRAING f1 class0: 0.8918757467144564 TRAING f1 class1: 0.48159831756046273 TRAING f1 class2: 0.509009009009009 TRAING f1 class3: 0.49495875343721357 TRAING f1 class4: 0.19672131147540986 \n",
            "\n",
            "4\n",
            "eval recall class0: 0.9289617486338798 eval recall class1: 0.5538461538461539 eval recall class2: 0.5660377358490566 eval recall class3: 0.6226415094339622 eval recall class4: 0.5454545454545454 \n",
            "\n",
            "eval prec class0: 0.8717948717948718 eval prec class1: 0.7058823529411765 eval prec class2: 0.6976744186046512 eval prec class3: 0.4852941176470588 eval prec class4: 0.75 \n",
            "\n",
            "eval f1 class0: 0.8994708994708994 eval f1 class1: 0.6206896551724139 eval f1 class2: 0.625 eval f1 class3: 0.5454545454545455 eval f1 class4: 0.631578947368421 \n",
            "\n",
            "eval LOSS: 0.6828437247160044\n",
            "epoch4_global-step3932  SAVED \n",
            "TRAINIG LOSS: 99 0.6927669018134475\n",
            "TRAING recall class0: 0.8794642857142857 TRAING recall class1: 0.39285714285714285 TRAING recall class2: 0.5681818181818182 TRAING recall class3: 0.5185185185185185 TRAING recall class4: 0.3181818181818182 \n",
            "\n",
            "TRAING prec class0: 0.8995433789954338 TRAING prec class1: 0.46808510638297873 TRAING prec class2: 0.5102040816326531 TRAING prec class3: 0.358974358974359 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.8893905191873589 TRAING f1 class1: 0.4271844660194175 TRAING f1 class2: 0.5376344086021506 TRAING f1 class3: 0.42424242424242425 TRAING f1 class4: 0.4827586206896552 \n",
            "\n",
            "TRAINIG LOSS: 199 0.7332773994654417\n",
            "TRAING recall class0: 0.8844339622641509 TRAING recall class1: 0.4095238095238095 TRAING recall class2: 0.5980392156862745 TRAING recall class3: 0.5461538461538461 TRAING recall class4: 0.3333333333333333 \n",
            "\n",
            "TRAING prec class0: 0.8886255924170616 TRAING prec class1: 0.46236559139784944 TRAING prec class2: 0.5980392156862745 TRAING prec class3: 0.4226190476190476 TRAING prec class4: 0.8666666666666667 \n",
            "\n",
            "TRAING f1 class0: 0.8865248226950355 TRAING f1 class1: 0.4343434343434343 TRAING f1 class2: 0.5980392156862745 TRAING f1 class3: 0.476510067114094 TRAING f1 class4: 0.48148148148148145 \n",
            "\n",
            "TRAINIG LOSS: 299 0.7233215379094085\n",
            "TRAING recall class0: 0.8939873417721519 TRAING recall class1: 0.4508670520231214 TRAING recall class2: 0.6089743589743589 TRAING recall class3: 0.5721925133689839 TRAING recall class4: 0.3269230769230769 \n",
            "\n",
            "TRAING prec class0: 0.8996815286624203 TRAING prec class1: 0.5234899328859061 TRAING prec class2: 0.5828220858895705 TRAING prec class3: 0.44398340248962653 TRAING prec class4: 0.8947368421052632 \n",
            "\n",
            "TRAING f1 class0: 0.8968253968253969 TRAING f1 class1: 0.48447204968944096 TRAING f1 class2: 0.5956112852664577 TRAING f1 class3: 0.5 TRAING f1 class4: 0.4788732394366197 \n",
            "\n",
            "TRAINIG LOSS: 399 0.7347342872619629\n",
            "TRAING recall class0: 0.8924731182795699 TRAING recall class1: 0.4525862068965517 TRAING recall class2: 0.5952380952380952 TRAING recall class3: 0.562015503875969 TRAING recall class4: 0.2857142857142857 \n",
            "\n",
            "TRAING prec class0: 0.8946107784431138 TRAING prec class1: 0.5357142857142857 TRAING prec class2: 0.5506607929515418 TRAING prec class3: 0.4503105590062112 TRAING prec class4: 0.9 \n",
            "\n",
            "TRAING f1 class0: 0.8935406698564593 TRAING f1 class1: 0.4906542056074766 TRAING f1 class2: 0.5720823798627002 TRAING f1 class3: 0.5 TRAING f1 class4: 0.4337349397590361 \n",
            "\n",
            "TRAINIG LOSS: 499 0.7338861480094493\n",
            "TRAING recall class0: 0.8926174496644296 TRAING recall class1: 0.45666666666666667 TRAING recall class2: 0.5795454545454546 TRAING recall class3: 0.577639751552795 TRAING recall class4: 0.28169014084507044 \n",
            "\n",
            "TRAING prec class0: 0.890057361376673 TRAING prec class1: 0.556910569105691 TRAING prec class2: 0.5425531914893617 TRAING prec class3: 0.46153846153846156 TRAING prec class4: 0.8695652173913043 \n",
            "\n",
            "TRAING f1 class0: 0.8913355672570609 TRAING f1 class1: 0.5018315018315017 TRAING f1 class2: 0.5604395604395604 TRAING f1 class3: 0.5131034482758621 TRAING f1 class4: 0.425531914893617 \n",
            "\n",
            "TRAINIG LOSS: 599 0.730697272621716\n",
            "TRAING recall class0: 0.8967434471803019 TRAING recall class1: 0.43175487465181056 TRAING recall class2: 0.5619047619047619 TRAING recall class3: 0.5936675461741425 TRAING recall class4: 0.2727272727272727 \n",
            "\n",
            "TRAING prec class0: 0.8946117274167987 TRAING prec class1: 0.543859649122807 TRAING prec class2: 0.5331325301204819 TRAING prec class3: 0.4563894523326572 TRAING prec class4: 0.8571428571428571 \n",
            "\n",
            "TRAING f1 class0: 0.8956763189210631 TRAING f1 class1: 0.48136645962732916 TRAING f1 class2: 0.5471406491499227 TRAING f1 class3: 0.5160550458715596 TRAING f1 class4: 0.41379310344827586 \n",
            "\n",
            "TRAINIG LOSS: 699 0.7297623871240233\n",
            "TRAING recall class0: 0.8942701227830833 TRAING recall class1: 0.4369158878504673 TRAING recall class2: 0.5679347826086957 TRAING recall class3: 0.5954545454545455 TRAING recall class4: 0.2755102040816326 \n",
            "\n",
            "TRAING prec class0: 0.8961038961038961 TRAING prec class1: 0.5532544378698225 TRAING prec class2: 0.5238095238095238 TRAING prec class3: 0.4628975265017668 TRAING prec class4: 0.7941176470588235 \n",
            "\n",
            "TRAING f1 class0: 0.895186070331171 TRAING f1 class1: 0.4882506527415144 TRAING f1 class2: 0.5449804432855281 TRAING f1 class3: 0.5208747514910537 TRAING f1 class4: 0.40909090909090906 \n",
            "\n",
            "TRAINIG LOSS: 799 0.7292917078360915\n",
            "TRAING recall class0: 0.8931980906921241 TRAING recall class1: 0.4309278350515464 TRAING recall class2: 0.5757575757575758 TRAING recall class3: 0.5912698412698413 TRAING recall class4: 0.2830188679245283 \n",
            "\n",
            "TRAING prec class0: 0.8969442780107849 TRAING prec class1: 0.5414507772020726 TRAING prec class2: 0.5210970464135021 TRAING prec class3: 0.47003154574132494 TRAING prec class4: 0.8108108108108109 \n",
            "\n",
            "TRAING f1 class0: 0.895067264573991 TRAING f1 class1: 0.4799081515499426 TRAING f1 class2: 0.5470653377630121 TRAING f1 class3: 0.523725834797891 TRAING f1 class4: 0.4195804195804196 \n",
            "\n",
            "5\n",
            "eval recall class0: 0.9234972677595629 eval recall class1: 0.6307692307692307 eval recall class2: 0.5660377358490566 eval recall class3: 0.5660377358490566 eval recall class4: 0.5454545454545454 \n",
            "\n",
            "eval prec class0: 0.8711340206185567 eval prec class1: 0.6307692307692307 eval prec class2: 0.6666666666666666 eval prec class3: 0.5660377358490566 eval prec class4: 0.75 \n",
            "\n",
            "eval f1 class0: 0.8965517241379312 eval f1 class1: 0.6307692307692307 eval f1 class2: 0.6122448979591837 eval f1 class3: 0.5660377358490566 eval f1 class4: 0.631578947368421 \n",
            "\n",
            "eval LOSS: 0.6383381537315282\n",
            "epoch5_global-step4892  SAVED \n",
            "TRAINIG LOSS: 99 0.678891799673438\n",
            "TRAING recall class0: 0.8975609756097561 TRAING recall class1: 0.5423728813559322 TRAING recall class2: 0.5555555555555556 TRAING recall class3: 0.6060606060606061 TRAING recall class4: 0.375 \n",
            "\n",
            "TRAING prec class0: 0.9246231155778895 TRAING prec class1: 0.5079365079365079 TRAING prec class2: 0.5 TRAING prec class3: 0.5633802816901409 TRAING prec class4: 0.8571428571428571 \n",
            "\n",
            "TRAING f1 class0: 0.9108910891089109 TRAING f1 class1: 0.5245901639344263 TRAING f1 class2: 0.5263157894736842 TRAING f1 class3: 0.5839416058394161 TRAING f1 class4: 0.5217391304347825 \n",
            "\n",
            "TRAINIG LOSS: 199 0.6893086338788271\n",
            "TRAING recall class0: 0.8932038834951457 TRAING recall class1: 0.4956521739130435 TRAING recall class2: 0.6226415094339622 TRAING recall class3: 0.6 TRAING recall class4: 0.3125 \n",
            "\n",
            "TRAING prec class0: 0.9223057644110275 TRAING prec class1: 0.504424778761062 TRAING prec class2: 0.4925373134328358 TRAING prec class3: 0.5664335664335665 TRAING prec class4: 0.9090909090909091 \n",
            "\n",
            "TRAING f1 class0: 0.9075215782983971 TRAING f1 class1: 0.5000000000000001 TRAING f1 class2: 0.55 TRAING f1 class3: 0.5827338129496403 TRAING f1 class4: 0.4651162790697674 \n",
            "\n",
            "TRAINIG LOSS: 299 0.6884208116245767\n",
            "TRAING recall class0: 0.889763779527559 TRAING recall class1: 0.47368421052631576 TRAING recall class2: 0.564935064935065 TRAING recall class3: 0.635 TRAING recall class4: 0.325 \n",
            "\n",
            "TRAING prec class0: 0.9157212317666127 TRAING prec class1: 0.5192307692307693 TRAING prec class2: 0.5028901734104047 TRAING prec class3: 0.5336134453781513 TRAING prec class4: 0.8125 \n",
            "\n",
            "TRAING f1 class0: 0.902555910543131 TRAING f1 class1: 0.4954128440366972 TRAING f1 class2: 0.5321100917431193 TRAING f1 class3: 0.5799086757990867 TRAING f1 class4: 0.46428571428571436 \n",
            "\n",
            "TRAINIG LOSS: 399 0.6879673646320589\n",
            "TRAING recall class0: 0.8983644859813084 TRAING recall class1: 0.48444444444444446 TRAING recall class2: 0.5688073394495413 TRAING recall class3: 0.6245059288537549 TRAING recall class4: 0.3333333333333333 \n",
            "\n",
            "TRAING prec class0: 0.9068396226415094 TRAING prec class1: 0.5505050505050505 TRAING prec class2: 0.543859649122807 TRAING prec class3: 0.5197368421052632 TRAING prec class4: 0.7272727272727273 \n",
            "\n",
            "TRAING f1 class0: 0.9025821596244132 TRAING f1 class1: 0.5153664302600474 TRAING f1 class2: 0.5560538116591929 TRAING f1 class3: 0.5673249551166964 TRAING f1 class4: 0.4571428571428572 \n",
            "\n",
            "TRAINIG LOSS: 499 0.6868160623144358\n",
            "TRAING recall class0: 0.8964879852125693 TRAING recall class1: 0.4659498207885305 TRAING recall class2: 0.5801526717557252 TRAING recall class3: 0.6206896551724138 TRAING recall class4: 0.3103448275862069 \n",
            "\n",
            "TRAING prec class0: 0.9073900841908326 TRAING prec class1: 0.5485232067510548 TRAING prec class2: 0.5170068027210885 TRAING prec class3: 0.526595744680851 TRAING prec class4: 0.75 \n",
            "\n",
            "TRAING f1 class0: 0.9019060901906091 TRAING f1 class1: 0.5038759689922481 TRAING f1 class2: 0.5467625899280575 TRAING f1 class3: 0.5697841726618704 TRAING f1 class4: 0.4390243902439025 \n",
            "\n",
            "TRAINIG LOSS: 599 0.7061108950137471\n",
            "TRAING recall class0: 0.8958168902920284 TRAING recall class1: 0.4782608695652174 TRAING recall class2: 0.5774193548387097 TRAING recall class3: 0.608 TRAING recall class4: 0.275 \n",
            "\n",
            "TRAING prec class0: 0.9043824701195219 TRAING prec class1: 0.5641025641025641 TRAING prec class2: 0.5173410404624278 TRAING prec class3: 0.4978165938864629 TRAING prec class4: 0.7586206896551724 \n",
            "\n",
            "TRAING f1 class0: 0.9000793021411578 TRAING f1 class1: 0.5176470588235293 TRAING f1 class2: 0.5457317073170731 TRAING f1 class3: 0.5474189675870348 TRAING f1 class4: 0.40366972477064217 \n",
            "\n",
            "TRAINIG LOSS: 699 0.700373982878934\n",
            "TRAING recall class0: 0.8976964769647696 TRAING recall class1: 0.48491879350348027 TRAING recall class2: 0.5977961432506887 TRAING recall class3: 0.6055045871559633 TRAING recall class4: 0.2872340425531915 \n",
            "\n",
            "TRAING prec class0: 0.9069130732375086 TRAING prec class1: 0.5679347826086957 TRAING prec class2: 0.5191387559808612 TRAING prec class3: 0.5086705202312138 TRAING prec class4: 0.7941176470588235 \n",
            "\n",
            "TRAING f1 class0: 0.902281239359891 TRAING f1 class1: 0.523153942428035 TRAING f1 class2: 0.5556978233034571 TRAING f1 class3: 0.5528795811518324 TRAING f1 class4: 0.421875 \n",
            "\n",
            "TRAINIG LOSS: 799 0.6955508949793875\n",
            "TRAING recall class0: 0.8990498812351544 TRAING recall class1: 0.4939271255060729 TRAING recall class2: 0.5995260663507109 TRAING recall class3: 0.6052631578947368 TRAING recall class4: 0.32075471698113206 \n",
            "\n",
            "TRAING prec class0: 0.9114990969295605 TRAING prec class1: 0.5687645687645687 TRAING prec class2: 0.524896265560166 TRAING prec class3: 0.5093696763202725 TRAING prec class4: 0.8292682926829268 \n",
            "\n",
            "TRAING f1 class0: 0.9052316890881914 TRAING f1 class1: 0.5287107258938245 TRAING f1 class2: 0.5597345132743362 TRAING f1 class3: 0.553191489361702 TRAING f1 class4: 0.4625850340136054 \n",
            "\n",
            "6\n",
            "eval recall class0: 0.9289617486338798 eval recall class1: 0.6 eval recall class2: 0.6792452830188679 eval recall class3: 0.6037735849056604 eval recall class4: 0.5454545454545454 \n",
            "\n",
            "eval prec class0: 0.8947368421052632 eval prec class1: 0.6724137931034483 eval prec class2: 0.6666666666666666 eval prec class3: 0.5818181818181818 eval prec class4: 0.75 \n",
            "\n",
            "eval f1 class0: 0.9115281501340483 eval f1 class1: 0.6341463414634146 eval f1 class2: 0.6728971962616822 eval f1 class3: 0.5925925925925926 eval f1 class4: 0.631578947368421 \n",
            "\n",
            "eval LOSS: 0.5919966113518228\n",
            "epoch6_global-step5852  SAVED \n",
            "TRAINIG LOSS: 99 0.7379300153255463\n",
            "TRAING recall class0: 0.8910891089108911 TRAING recall class1: 0.5303030303030303 TRAING recall class2: 0.6491228070175439 TRAING recall class3: 0.5333333333333333 TRAING recall class4: 0.06666666666666667 \n",
            "\n",
            "TRAING prec class0: 0.9137055837563451 TRAING prec class1: 0.546875 TRAING prec class2: 0.5068493150684932 TRAING prec class3: 0.5161290322580645 TRAING prec class4: 0.25 \n",
            "\n",
            "TRAING f1 class0: 0.9022556390977443 TRAING f1 class1: 0.5384615384615384 TRAING f1 class2: 0.5692307692307693 TRAING f1 class3: 0.5245901639344263 TRAING f1 class4: 0.10526315789473685 \n",
            "\n",
            "TRAINIG LOSS: 199 0.6826864440273493\n",
            "TRAING recall class0: 0.8985849056603774 TRAING recall class1: 0.5373134328358209 TRAING recall class2: 0.6831683168316832 TRAING recall class3: 0.5862068965517241 TRAING recall class4: 0.24 \n",
            "\n",
            "TRAING prec class0: 0.927007299270073 TRAING prec class1: 0.5853658536585366 TRAING prec class2: 0.5433070866141733 TRAING prec class3: 0.53125 TRAING prec class4: 0.5454545454545454 \n",
            "\n",
            "TRAING f1 class0: 0.9125748502994012 TRAING f1 class1: 0.5603112840466926 TRAING f1 class2: 0.6052631578947368 TRAING f1 class3: 0.5573770491803278 TRAING f1 class4: 0.3333333333333333 \n",
            "\n",
            "TRAINIG LOSS: 299 0.6814763206709177\n",
            "TRAING recall class0: 0.9036918138041734 TRAING recall class1: 0.525 TRAING recall class2: 0.6451612903225806 TRAING recall class3: 0.5879120879120879 TRAING recall class4: 0.3 \n",
            "\n",
            "TRAING prec class0: 0.9095315024232633 TRAING prec class1: 0.5898876404494382 TRAING prec class2: 0.5319148936170213 TRAING prec class3: 0.5487179487179488 TRAING prec class4: 0.6 \n",
            "\n",
            "TRAING f1 class0: 0.9066022544283414 TRAING f1 class1: 0.5555555555555555 TRAING f1 class2: 0.5830903790087464 TRAING f1 class3: 0.5676392572944298 TRAING f1 class4: 0.4 \n",
            "\n",
            "TRAINIG LOSS: 399 0.6742836328945123\n",
            "TRAING recall class0: 0.9065868263473054 TRAING recall class1: 0.5038167938931297 TRAING recall class2: 0.6568627450980392 TRAING recall class3: 0.5959183673469388 TRAING recall class4: 0.37037037037037035 \n",
            "\n",
            "TRAING prec class0: 0.9131483715319663 TRAING prec class1: 0.5972850678733032 TRAING prec class2: 0.5275590551181102 TRAING prec class3: 0.5468164794007491 TRAING prec class4: 0.6896551724137931 \n",
            "\n",
            "TRAING f1 class0: 0.9098557692307692 TRAING f1 class1: 0.546583850931677 TRAING f1 class2: 0.5851528384279476 TRAING f1 class3: 0.5703125 TRAING f1 class4: 0.48192771084337355 \n",
            "\n",
            "TRAINIG LOSS: 499 0.6707463405076415\n",
            "TRAING recall class0: 0.9083969465648855 TRAING recall class1: 0.5170278637770898 TRAING recall class2: 0.63671875 TRAING recall class3: 0.5915032679738562 TRAING recall class4: 0.373134328358209 \n",
            "\n",
            "TRAING prec class0: 0.9153846153846154 TRAING prec class1: 0.5880281690140845 TRAING prec class2: 0.5275080906148867 TRAING prec class3: 0.5451807228915663 TRAING prec class4: 0.7142857142857143 \n",
            "\n",
            "TRAING f1 class0: 0.9118773946360154 TRAING f1 class1: 0.5502471169686984 TRAING f1 class2: 0.5769911504424778 TRAING f1 class3: 0.5673981191222571 TRAING f1 class4: 0.49019607843137253 \n",
            "\n",
            "TRAINIG LOSS: 599 0.6675903031214451\n",
            "TRAING recall class0: 0.9059288537549407 TRAING recall class1: 0.5065963060686016 TRAING recall class2: 0.6339869281045751 TRAING recall class3: 0.6135135135135135 TRAING recall class4: 0.3875 \n",
            "\n",
            "TRAING prec class0: 0.9182692307692307 TRAING prec class1: 0.5907692307692308 TRAING prec class2: 0.5243243243243243 TRAING prec class3: 0.5456730769230769 TRAING prec class4: 0.7560975609756098 \n",
            "\n",
            "TRAING f1 class0: 0.9120573020294469 TRAING f1 class1: 0.5454545454545455 TRAING f1 class2: 0.5739644970414202 TRAING f1 class3: 0.5776081424936387 TRAING f1 class4: 0.5123966942148761 \n",
            "\n",
            "TRAINIG LOSS: 699 0.6690501898859761\n",
            "TRAING recall class0: 0.9064406779661017 TRAING recall class1: 0.5056179775280899 TRAING recall class2: 0.6336088154269972 TRAING recall class3: 0.6150234741784038 TRAING recall class4: 0.4065934065934066 \n",
            "\n",
            "TRAING prec class0: 0.9151266255989049 TRAING prec class1: 0.6 TRAING prec class2: 0.5324074074074074 TRAING prec class3: 0.5413223140495868 TRAING prec class4: 0.7708333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.9107629427792916 TRAING f1 class1: 0.5487804878048781 TRAING f1 class2: 0.5786163522012578 TRAING f1 class3: 0.5758241758241759 TRAING f1 class4: 0.5323741007194245 \n",
            "\n",
            "TRAINIG LOSS: 799 0.666760240237927\n",
            "TRAING recall class0: 0.904931669637552 TRAING recall class1: 0.5111111111111111 TRAING recall class2: 0.6308411214953271 TRAING recall class3: 0.6073619631901841 TRAING recall class4: 0.41904761904761906 \n",
            "\n",
            "TRAING prec class0: 0.9180229053646776 TRAING prec class1: 0.5911214953271028 TRAING prec class2: 0.5357142857142857 TRAING prec class3: 0.5370705244122965 TRAING prec class4: 0.7857142857142857 \n",
            "\n",
            "TRAING f1 class0: 0.9114302812687013 TRAING f1 class1: 0.5482123510292525 TRAING f1 class2: 0.5793991416309012 TRAING f1 class3: 0.5700575815738963 TRAING f1 class4: 0.546583850931677 \n",
            "\n",
            "7\n",
            "eval recall class0: 0.9453551912568307 eval recall class1: 0.6 eval recall class2: 0.6981132075471698 eval recall class3: 0.7735849056603774 eval recall class4: 0.5454545454545454 \n",
            "\n",
            "eval prec class0: 0.9153439153439153 eval prec class1: 0.8478260869565217 eval prec class2: 0.6981132075471698 eval prec class3: 0.5942028985507246 eval prec class4: 0.75 \n",
            "\n",
            "eval f1 class0: 0.9301075268817205 eval f1 class1: 0.7027027027027027 eval f1 class2: 0.6981132075471698 eval f1 class3: 0.6721311475409837 eval f1 class4: 0.631578947368421 \n",
            "\n",
            "eval LOSS: 0.5562973170858491\n",
            "epoch7_global-step6812  SAVED \n",
            "TRAINIG LOSS: 99 0.5919263264723122\n",
            "TRAING recall class0: 0.915929203539823 TRAING recall class1: 0.5 TRAING recall class2: 0.46153846153846156 TRAING recall class3: 0.6363636363636364 TRAING recall class4: 0.4166666666666667 \n",
            "\n",
            "TRAING prec class0: 0.92 TRAING prec class1: 0.6538461538461539 TRAING prec class2: 0.45 TRAING prec class3: 0.4666666666666667 TRAING prec class4: 0.625 \n",
            "\n",
            "TRAING f1 class0: 0.9179600886917961 TRAING f1 class1: 0.5666666666666668 TRAING f1 class2: 0.45569620253164556 TRAING f1 class3: 0.5384615384615385 TRAING f1 class4: 0.5 \n",
            "\n",
            "TRAINIG LOSS: 199 0.6293333277478814\n",
            "TRAING recall class0: 0.9203747072599532 TRAING recall class1: 0.4573643410852713 TRAING recall class2: 0.5566037735849056 TRAING recall class3: 0.6460176991150443 TRAING recall class4: 0.36 \n",
            "\n",
            "TRAING prec class0: 0.9055299539170507 TRAING prec class1: 0.59 TRAING prec class2: 0.5221238938053098 TRAING prec class3: 0.5214285714285715 TRAING prec class4: 0.6923076923076923 \n",
            "\n",
            "TRAING f1 class0: 0.9128919860627178 TRAING f1 class1: 0.5152838427947598 TRAING f1 class2: 0.5388127853881279 TRAING f1 class3: 0.5770750988142292 TRAING f1 class4: 0.47368421052631576 \n",
            "\n",
            "TRAINIG LOSS: 299 0.6384158206855257\n",
            "TRAING recall class0: 0.9114906832298136 TRAING recall class1: 0.47692307692307695 TRAING recall class2: 0.5838926174496645 TRAING recall class3: 0.6022727272727273 TRAING recall class4: 0.3888888888888889 \n",
            "\n",
            "TRAING prec class0: 0.9058641975308642 TRAING prec class1: 0.5923566878980892 TRAING prec class2: 0.49714285714285716 TRAING prec class3: 0.527363184079602 TRAING prec class4: 0.7368421052631579 \n",
            "\n",
            "TRAING f1 class0: 0.9086687306501549 TRAING f1 class1: 0.5284090909090909 TRAING f1 class2: 0.5370370370370371 TRAING f1 class3: 0.5623342175066313 TRAING f1 class4: 0.509090909090909 \n",
            "\n",
            "TRAINIG LOSS: 399 0.6467399516049772\n",
            "TRAING recall class0: 0.9067296340023613 TRAING recall class1: 0.4801587301587302 TRAING recall class2: 0.5643564356435643 TRAING recall class3: 0.6382113821138211 TRAING recall class4: 0.41509433962264153 \n",
            "\n",
            "TRAING prec class0: 0.9088757396449704 TRAING prec class1: 0.5817307692307693 TRAING prec class2: 0.504424778761062 TRAING prec class3: 0.5376712328767124 TRAING prec class4: 0.7586206896551724 \n",
            "\n",
            "TRAING f1 class0: 0.9078014184397163 TRAING f1 class1: 0.5260869565217391 TRAING f1 class2: 0.5327102803738317 TRAING f1 class3: 0.5836431226765799 TRAING f1 class4: 0.5365853658536586 \n",
            "\n",
            "TRAINIG LOSS: 499 0.6369464006312191\n",
            "TRAING recall class0: 0.909689557855127 TRAING recall class1: 0.48220064724919093 TRAING recall class2: 0.6068702290076335 TRAING recall class3: 0.6490066225165563 TRAING recall class4: 0.4375 \n",
            "\n",
            "TRAING prec class0: 0.909689557855127 TRAING prec class1: 0.5912698412698413 TRAING prec class2: 0.5463917525773195 TRAING prec class3: 0.5459610027855153 TRAING prec class4: 0.8 \n",
            "\n",
            "TRAING f1 class0: 0.9096895578551271 TRAING f1 class1: 0.5311942959001782 TRAING f1 class2: 0.5750452079566004 TRAING f1 class3: 0.5930408472012102 TRAING f1 class4: 0.5656565656565656 \n",
            "\n",
            "TRAINIG LOSS: 599 0.6400948990043253\n",
            "TRAING recall class0: 0.9100236779794791 TRAING recall class1: 0.4960835509138381 TRAING recall class2: 0.6221498371335505 TRAING recall class3: 0.6456043956043956 TRAING recall class4: 0.4430379746835443 \n",
            "\n",
            "TRAING prec class0: 0.9121835443037974 TRAING prec class1: 0.6070287539936102 TRAING prec class2: 0.5472779369627507 TRAING prec class3: 0.5477855477855478 TRAING prec class4: 0.7777777777777778 \n",
            "\n",
            "TRAING f1 class0: 0.9111023310944291 TRAING f1 class1: 0.5459770114942529 TRAING f1 class2: 0.5823170731707318 TRAING f1 class3: 0.5926860025220682 TRAING f1 class4: 0.564516129032258 \n",
            "\n",
            "TRAINIG LOSS: 699 0.6359468998680158\n",
            "TRAING recall class0: 0.911504424778761 TRAING recall class1: 0.5067567567567568 TRAING recall class2: 0.6170798898071626 TRAING recall class3: 0.6505747126436782 TRAING recall class4: 0.449438202247191 \n",
            "\n",
            "TRAING prec class0: 0.9146174863387978 TRAING prec class1: 0.6048387096774194 TRAING prec class2: 0.5614035087719298 TRAING prec class3: 0.5505836575875487 TRAING prec class4: 0.7843137254901961 \n",
            "\n",
            "TRAING f1 class0: 0.9130583020797818 TRAING f1 class1: 0.5514705882352942 TRAING f1 class2: 0.5879265091863517 TRAING f1 class3: 0.5964172813487882 TRAING f1 class4: 0.5714285714285713 \n",
            "\n",
            "TRAINIG LOSS: 799 0.6350069895293564\n",
            "TRAING recall class0: 0.9087656529516994 TRAING recall class1: 0.5149105367793241 TRAING recall class2: 0.6253041362530414 TRAING recall class3: 0.6561264822134387 TRAING recall class4: 0.46601941747572817 \n",
            "\n",
            "TRAING prec class0: 0.9164161154539988 TRAING prec class1: 0.6137440758293838 TRAING prec class2: 0.5562770562770563 TRAING prec class3: 0.5608108108108109 TRAING prec class4: 0.7868852459016393 \n",
            "\n",
            "TRAING f1 class0: 0.9125748502994012 TRAING f1 class1: 0.56 TRAING f1 class2: 0.588774341351661 TRAING f1 class3: 0.6047358834244081 TRAING f1 class4: 0.5853658536585367 \n",
            "\n",
            "8\n",
            "eval recall class0: 0.9562841530054644 eval recall class1: 0.6 eval recall class2: 0.6981132075471698 eval recall class3: 0.7547169811320755 eval recall class4: 0.6363636363636364 \n",
            "\n",
            "eval prec class0: 0.8883248730964467 eval prec class1: 0.8478260869565217 eval prec class2: 0.7254901960784313 eval prec class3: 0.6451612903225806 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.9210526315789473 eval f1 class1: 0.7027027027027027 eval f1 class2: 0.7115384615384615 eval f1 class3: 0.6956521739130435 eval f1 class4: 0.7000000000000001 \n",
            "\n",
            "eval LOSS: 0.5307887095988697\n",
            "epoch8_global-step7772  SAVED \n",
            "TRAINIG LOSS: 99 0.591017413334921\n",
            "TRAING recall class0: 0.9166666666666666 TRAING recall class1: 0.6491228070175439 TRAING recall class2: 0.6896551724137931 TRAING recall class3: 0.6521739130434783 TRAING recall class4: 0.5833333333333334 \n",
            "\n",
            "TRAING prec class0: 0.9033816425120773 TRAING prec class1: 0.6851851851851852 TRAING prec class2: 0.6666666666666666 TRAING prec class3: 0.6428571428571429 TRAING prec class4: 0.7777777777777778 \n",
            "\n",
            "TRAING f1 class0: 0.9099756690997568 TRAING f1 class1: 0.6666666666666666 TRAING f1 class2: 0.6779661016949153 TRAING f1 class3: 0.6474820143884892 TRAING f1 class4: 0.6666666666666666 \n",
            "\n",
            "TRAINIG LOSS: 199 0.6008383886795491\n",
            "TRAING recall class0: 0.9093137254901961 TRAING recall class1: 0.5688073394495413 TRAING recall class2: 0.6991869918699187 TRAING recall class3: 0.6590909090909091 TRAING recall class4: 0.6071428571428571 \n",
            "\n",
            "TRAING prec class0: 0.9048780487804878 TRAING prec class1: 0.6666666666666666 TRAING prec class2: 0.6323529411764706 TRAING prec class3: 0.6170212765957447 TRAING prec class4: 0.85 \n",
            "\n",
            "TRAING f1 class0: 0.9070904645476773 TRAING f1 class1: 0.6138613861386139 TRAING f1 class2: 0.6640926640926641 TRAING f1 class3: 0.6373626373626374 TRAING f1 class4: 0.7083333333333333 \n",
            "\n",
            "TRAINIG LOSS: 299 0.5921932250944276\n",
            "TRAING recall class0: 0.9202551834130781 TRAING recall class1: 0.5402298850574713 TRAING recall class2: 0.7176470588235294 TRAING recall class3: 0.6595744680851063 TRAING recall class4: 0.4878048780487805 \n",
            "\n",
            "TRAING prec class0: 0.9232 TRAING prec class1: 0.6811594202898551 TRAING prec class2: 0.6069651741293532 TRAING prec class3: 0.5904761904761905 TRAING prec class4: 0.7692307692307693 \n",
            "\n",
            "TRAING f1 class0: 0.9217252396166135 TRAING f1 class1: 0.6025641025641026 TRAING f1 class2: 0.6576819407008085 TRAING f1 class3: 0.6231155778894472 TRAING f1 class4: 0.5970149253731344 \n",
            "\n",
            "TRAINIG LOSS: 399 0.5977535459841602\n",
            "TRAING recall class0: 0.9127272727272727 TRAING recall class1: 0.5416666666666666 TRAING recall class2: 0.7149321266968326 TRAING recall class3: 0.6589147286821705 TRAING recall class4: 0.4642857142857143 \n",
            "\n",
            "TRAING prec class0: 0.9273399014778325 TRAING prec class1: 0.6701030927835051 TRAING prec class2: 0.5939849624060151 TRAING prec class3: 0.5743243243243243 TRAING prec class4: 0.8125 \n",
            "\n",
            "TRAING f1 class0: 0.919975565058033 TRAING f1 class1: 0.5990783410138248 TRAING f1 class2: 0.648870636550308 TRAING f1 class3: 0.6137184115523465 TRAING f1 class4: 0.5909090909090908 \n",
            "\n",
            "TRAINIG LOSS: 499 0.5998504129108041\n",
            "TRAING recall class0: 0.9124157844080847 TRAING recall class1: 0.5348837209302325 TRAING recall class2: 0.6951672862453532 TRAING recall class3: 0.6749226006191951 TRAING recall class4: 0.47058823529411764 \n",
            "\n",
            "TRAING prec class0: 0.92578125 TRAING prec class1: 0.6764705882352942 TRAING prec class2: 0.5880503144654088 TRAING prec class3: 0.5721784776902887 TRAING prec class4: 0.8205128205128205 \n",
            "\n",
            "TRAING f1 class0: 0.9190499272903538 TRAING f1 class1: 0.5974025974025974 TRAING f1 class2: 0.6371379897785349 TRAING f1 class3: 0.6193181818181819 TRAING f1 class4: 0.5981308411214953 \n",
            "\n",
            "TRAINIG LOSS: 599 0.5977855629737799\n",
            "TRAING recall class0: 0.915079365079365 TRAING recall class1: 0.5254691689008043 TRAING recall class2: 0.6881028938906752 TRAING recall class3: 0.6798941798941799 TRAING recall class4: 0.5128205128205128 \n",
            "\n",
            "TRAING prec class0: 0.9275945293644409 TRAING prec class1: 0.6805555555555556 TRAING prec class2: 0.5799457994579946 TRAING prec class3: 0.5711111111111111 TRAING prec class4: 0.8 \n",
            "\n",
            "TRAING f1 class0: 0.9212944466640032 TRAING f1 class1: 0.5930408472012102 TRAING f1 class2: 0.6294117647058823 TRAING f1 class3: 0.6207729468599033 TRAING f1 class4: 0.625 \n",
            "\n",
            "TRAINIG LOSS: 699 0.6030980797604258\n",
            "TRAING recall class0: 0.9137577002053389 TRAING recall class1: 0.5184331797235023 TRAING recall class2: 0.6815642458100558 TRAING recall class3: 0.6755555555555556 TRAING recall class4: 0.5154639175257731 \n",
            "\n",
            "TRAING prec class0: 0.9251559251559252 TRAING prec class1: 0.6521739130434783 TRAING prec class2: 0.580952380952381 TRAING prec class3: 0.5746691871455577 TRAING prec class4: 0.7936507936507936 \n",
            "\n",
            "TRAING f1 class0: 0.9194214876033059 TRAING f1 class1: 0.5776636713735558 TRAING f1 class2: 0.6272493573264781 TRAING f1 class3: 0.6210418794688458 TRAING f1 class4: 0.625 \n",
            "\n",
            "TRAINIG LOSS: 799 0.6067325308703585\n",
            "TRAING recall class0: 0.9121540312876053 TRAING recall class1: 0.5219123505976095 TRAING recall class2: 0.6746411483253588 TRAING recall class3: 0.6731898238747553 TRAING recall class4: 0.514018691588785 \n",
            "\n",
            "TRAING prec class0: 0.9227023737066342 TRAING prec class1: 0.6582914572864321 TRAING prec class2: 0.5826446280991735 TRAING prec class3: 0.5676567656765676 TRAING prec class4: 0.7971014492753623 \n",
            "\n",
            "TRAING f1 class0: 0.9173978819969743 TRAING f1 class1: 0.5822222222222223 TRAING f1 class2: 0.6252771618625278 TRAING f1 class3: 0.6159355416293644 TRAING f1 class4: 0.625 \n",
            "\n",
            "9\n",
            "eval recall class0: 0.9562841530054644 eval recall class1: 0.7384615384615385 eval recall class2: 0.7358490566037735 eval recall class3: 0.7358490566037735 eval recall class4: 0.6363636363636364 \n",
            "\n",
            "eval prec class0: 0.9210526315789473 eval prec class1: 0.7741935483870968 eval prec class2: 0.78 eval prec class3: 0.7222222222222222 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.938337801608579 eval f1 class1: 0.7559055118110236 eval f1 class2: 0.7572815533980584 eval f1 class3: 0.7289719626168224 eval f1 class4: 0.7000000000000001 \n",
            "\n",
            "eval LOSS: 0.49093367276549504\n",
            "epoch9_global-step8732  SAVED \n",
            "TRAINIG LOSS: 99 0.5316595340520144\n",
            "TRAING recall class0: 0.9383886255924171 TRAING recall class1: 0.5964912280701754 TRAING recall class2: 0.7547169811320755 TRAING recall class3: 0.6666666666666666 TRAING recall class4: 0.7 \n",
            "\n",
            "TRAING prec class0: 0.9383886255924171 TRAING prec class1: 0.6666666666666666 TRAING prec class2: 0.625 TRAING prec class3: 0.6865671641791045 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9383886255924171 TRAING f1 class1: 0.6296296296296297 TRAING f1 class2: 0.6837606837606838 TRAING f1 class3: 0.676470588235294 TRAING f1 class4: 0.8235294117647058 \n",
            "\n",
            "TRAINIG LOSS: 199 0.5464394873380661\n",
            "TRAING recall class0: 0.9399538106235565 TRAING recall class1: 0.5963302752293578 TRAING recall class2: 0.7247706422018348 TRAING recall class3: 0.64 TRAING recall class4: 0.5416666666666666 \n",
            "\n",
            "TRAING prec class0: 0.9271070615034168 TRAING prec class1: 0.6632653061224489 TRAING prec class2: 0.6528925619834711 TRAING prec class3: 0.6504065040650406 TRAING prec class4: 0.6842105263157895 \n",
            "\n",
            "TRAING f1 class0: 0.9334862385321101 TRAING f1 class1: 0.6280193236714976 TRAING f1 class2: 0.6869565217391305 TRAING f1 class3: 0.6451612903225806 TRAING f1 class4: 0.6046511627906976 \n",
            "\n",
            "TRAINIG LOSS: 299 0.5449825534565995\n",
            "TRAING recall class0: 0.9350850077279753 TRAING recall class1: 0.5674157303370787 TRAING recall class2: 0.7272727272727273 TRAING recall class3: 0.6902173913043478 TRAING recall class4: 0.5675675675675675 \n",
            "\n",
            "TRAING prec class0: 0.9322033898305084 TRAING prec class1: 0.7112676056338029 TRAING prec class2: 0.6222222222222222 TRAING prec class3: 0.6318407960199005 TRAING prec class4: 0.75 \n",
            "\n",
            "TRAING f1 class0: 0.933641975308642 TRAING f1 class1: 0.63125 TRAING f1 class2: 0.6706586826347306 TRAING f1 class3: 0.6597402597402597 TRAING f1 class4: 0.6461538461538462 \n",
            "\n",
            "TRAINIG LOSS: 399 0.555176218370907\n",
            "TRAING recall class0: 0.9292343387470998 TRAING recall class1: 0.608 TRAING recall class2: 0.7171717171717171 TRAING recall class3: 0.6869918699186992 TRAING recall class4: 0.5454545454545454 \n",
            "\n",
            "TRAING prec class0: 0.9346557759626605 TRAING prec class1: 0.7238095238095238 TRAING prec class2: 0.6147186147186147 TRAING prec class3: 0.6329588014981273 TRAING prec class4: 0.6857142857142857 \n",
            "\n",
            "TRAING f1 class0: 0.9319371727748691 TRAING f1 class1: 0.6608695652173913 TRAING f1 class2: 0.662004662004662 TRAING f1 class3: 0.6588693957115009 TRAING f1 class4: 0.6075949367088607 \n",
            "\n",
            "TRAINIG LOSS: 499 0.5636394434291869\n",
            "TRAING recall class0: 0.9298578199052133 TRAING recall class1: 0.6277602523659306 TRAING recall class2: 0.7054263565891473 TRAING recall class3: 0.6840390879478827 TRAING recall class4: 0.5873015873015873 \n",
            "\n",
            "TRAING prec class0: 0.9325095057034221 TRAING prec class1: 0.7236363636363636 TRAING prec class2: 0.6275862068965518 TRAING prec class3: 0.6287425149700598 TRAING prec class4: 0.7551020408163265 \n",
            "\n",
            "TRAING f1 class0: 0.9311817750355956 TRAING f1 class1: 0.6722972972972973 TRAING f1 class2: 0.6642335766423358 TRAING f1 class3: 0.6552262090483619 TRAING f1 class4: 0.6607142857142858 \n",
            "\n",
            "TRAINIG LOSS: 599 0.5621517290283615\n",
            "TRAING recall class0: 0.927843137254902 TRAING recall class1: 0.6196808510638298 TRAING recall class2: 0.7156549520766773 TRAING recall class3: 0.6935933147632312 TRAING recall class4: 0.5454545454545454 \n",
            "\n",
            "TRAING prec class0: 0.9351778656126483 TRAING prec class1: 0.7236024844720497 TRAING prec class2: 0.6292134831460674 TRAING prec class3: 0.6178660049627791 TRAING prec class4: 0.7777777777777778 \n",
            "\n",
            "TRAING f1 class0: 0.931496062992126 TRAING f1 class1: 0.667621776504298 TRAING f1 class2: 0.6696562032884903 TRAING f1 class3: 0.6535433070866141 TRAING f1 class4: 0.6412213740458015 \n",
            "\n",
            "TRAINIG LOSS: 699 0.5733035240056259\n",
            "TRAING recall class0: 0.9210347174948945 TRAING recall class1: 0.6213151927437641 TRAING recall class2: 0.7216216216216216 TRAING recall class3: 0.6960556844547564 TRAING recall class4: 0.5280898876404494 \n",
            "\n",
            "TRAING prec class0: 0.940236275191105 TRAING prec class1: 0.7191601049868767 TRAING prec class2: 0.6223776223776224 TRAING prec class3: 0.6134969325153374 TRAING prec class4: 0.7580645161290323 \n",
            "\n",
            "TRAING f1 class0: 0.9305364511691886 TRAING f1 class1: 0.6666666666666667 TRAING f1 class2: 0.6683354192740926 TRAING f1 class3: 0.6521739130434784 TRAING f1 class4: 0.6225165562913907 \n",
            "\n",
            "TRAINIG LOSS: 799 0.5744771227875026\n",
            "TRAING recall class0: 0.9219308700834327 TRAING recall class1: 0.6155378486055777 TRAING recall class2: 0.715311004784689 TRAING recall class3: 0.7074148296593187 TRAING recall class4: 0.5631067961165048 \n",
            "\n",
            "TRAING prec class0: 0.9353083434099153 TRAING prec class1: 0.7236533957845434 TRAING prec class2: 0.6242171189979123 TRAING prec class3: 0.625886524822695 TRAING prec class4: 0.7631578947368421 \n",
            "\n",
            "TRAING f1 class0: 0.9285714285714285 TRAING f1 class1: 0.6652314316469322 TRAING f1 class2: 0.6666666666666666 TRAING f1 class3: 0.6641580432737536 TRAING f1 class4: 0.64804469273743 \n",
            "\n",
            "10\n",
            "eval recall class0: 0.9617486338797814 eval recall class1: 0.7538461538461538 eval recall class2: 0.7358490566037735 eval recall class3: 0.8301886792452831 eval recall class4: 0.6363636363636364 \n",
            "\n",
            "eval prec class0: 0.9214659685863874 eval prec class1: 0.8166666666666667 eval prec class2: 0.8478260869565217 eval prec class3: 0.7457627118644068 eval prec class4: 0.7777777777777778 \n",
            "\n",
            "eval f1 class0: 0.9411764705882353 eval f1 class1: 0.784 eval f1 class2: 0.7878787878787878 eval f1 class3: 0.7857142857142858 eval f1 class4: 0.7000000000000001 \n",
            "\n",
            "eval LOSS: 0.460818295853192\n",
            "epoch10_global-step9692  SAVED \n",
            "TRAINIG LOSS: 99 0.5603036426473409\n",
            "TRAING recall class0: 0.9245283018867925 TRAING recall class1: 0.515625 TRAING recall class2: 0.6 TRAING recall class3: 0.810126582278481 TRAING recall class4: 0.6 \n",
            "\n",
            "TRAING prec class0: 0.937799043062201 TRAING prec class1: 0.7333333333333333 TRAING prec class2: 0.525 TRAING prec class3: 0.6530612244897959 TRAING prec class4: 0.75 \n",
            "\n",
            "TRAING f1 class0: 0.9311163895486936 TRAING f1 class1: 0.6055045871559632 TRAING f1 class2: 0.56 TRAING f1 class3: 0.7231638418079096 TRAING f1 class4: 0.6666666666666665 \n",
            "\n",
            "TRAINIG LOSS: 199 0.5624165542377159\n",
            "TRAING recall class0: 0.9312796208530806 TRAING recall class1: 0.6141732283464567 TRAING recall class2: 0.6444444444444445 TRAING recall class3: 0.7333333333333333 TRAING recall class4: 0.5769230769230769 \n",
            "\n",
            "TRAING prec class0: 0.9203747072599532 TRAING prec class1: 0.7428571428571429 TRAING prec class2: 0.6236559139784946 TRAING prec class3: 0.6305732484076433 TRAING prec class4: 0.8333333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.9257950530035336 TRAING f1 class1: 0.6724137931034483 TRAING f1 class2: 0.6338797814207651 TRAING f1 class3: 0.6780821917808219 TRAING f1 class4: 0.6818181818181818 \n",
            "\n",
            "TRAINIG LOSS: 299 0.5627243782983472\n",
            "TRAING recall class0: 0.9298245614035088 TRAING recall class1: 0.6129032258064516 TRAING recall class2: 0.6382978723404256 TRAING recall class3: 0.7438423645320197 TRAING recall class4: 0.5813953488372093 \n",
            "\n",
            "TRAING prec class0: 0.9342948717948718 TRAING prec class1: 0.7169811320754716 TRAING prec class2: 0.6 TRAING prec class3: 0.6371308016877637 TRAING prec class4: 0.8333333333333334 \n",
            "\n",
            "TRAING f1 class0: 0.9320543565147882 TRAING f1 class1: 0.6608695652173913 TRAING f1 class2: 0.6185567010309279 TRAING f1 class3: 0.6863636363636364 TRAING f1 class4: 0.684931506849315 \n",
            "\n",
            "TRAINIG LOSS: 399 0.5579887583199888\n",
            "TRAING recall class0: 0.9263285024154589 TRAING recall class1: 0.5856573705179283 TRAING recall class2: 0.6717948717948717 TRAING recall class3: 0.7389705882352942 TRAING recall class4: 0.6111111111111112 \n",
            "\n",
            "TRAING prec class0: 0.9365079365079365 TRAING prec class1: 0.6966824644549763 TRAING prec class2: 0.6093023255813953 TRAING prec class3: 0.638095238095238 TRAING prec class4: 0.825 \n",
            "\n",
            "TRAING f1 class0: 0.9313904068002428 TRAING f1 class1: 0.6363636363636364 TRAING f1 class2: 0.6390243902439025 TRAING f1 class3: 0.6848381601362862 TRAING f1 class4: 0.7021276595744681 \n",
            "\n",
            "TRAINIG LOSS: 499 0.5557510211560875\n",
            "TRAING recall class0: 0.9232245681381958 TRAING recall class1: 0.5789473684210527 TRAING recall class2: 0.6798418972332015 TRAING recall class3: 0.7417417417417418 TRAING recall class4: 0.5588235294117647 \n",
            "\n",
            "TRAING prec class0: 0.9403714565004888 TRAING prec class1: 0.6743295019157088 TRAING prec class2: 0.6277372262773723 TRAING prec class3: 0.6253164556962025 TRAING prec class4: 0.8085106382978723 \n",
            "\n",
            "TRAING f1 class0: 0.9317191283292978 TRAING f1 class1: 0.6230088495575222 TRAING f1 class2: 0.6527514231499051 TRAING f1 class3: 0.6785714285714286 TRAING f1 class4: 0.6608695652173914 \n",
            "\n",
            "TRAINIG LOSS: 599 0.5577211602994552\n",
            "TRAING recall class0: 0.9201612903225806 TRAING recall class1: 0.5710382513661202 TRAING recall class2: 0.7060702875399361 TRAING recall class3: 0.7361809045226131 TRAING recall class4: 0.5421686746987951 \n",
            "\n",
            "TRAING prec class0: 0.9437551695616212 TRAING prec class1: 0.6807817589576547 TRAING prec class2: 0.6296296296296297 TRAING prec class3: 0.6181434599156118 TRAING prec class4: 0.7627118644067796 \n",
            "\n",
            "TRAING f1 class0: 0.9318089015924866 TRAING f1 class1: 0.6210995542347697 TRAING f1 class2: 0.6656626506024097 TRAING f1 class3: 0.6720183486238531 TRAING f1 class4: 0.6338028169014085 \n",
            "\n",
            "TRAINIG LOSS: 699 0.5484769063682429\n",
            "TRAING recall class0: 0.9251373626373627 TRAING recall class1: 0.5866050808314087 TRAING recall class2: 0.7044198895027625 TRAING recall class3: 0.7439824945295405 TRAING recall class4: 0.5760869565217391 \n",
            "\n",
            "TRAING prec class0: 0.9465917076598735 TRAING prec class1: 0.6920980926430518 TRAING prec class2: 0.6327543424317618 TRAING prec class3: 0.633147113594041 TRAING prec class4: 0.7571428571428571 \n",
            "\n",
            "TRAING f1 class0: 0.9357415769364363 TRAING f1 class1: 0.6349999999999999 TRAING f1 class2: 0.6666666666666667 TRAING f1 class3: 0.6841046277665996 TRAING f1 class4: 0.6543209876543209 \n",
            "\n",
            "TRAINIG LOSS: 799 0.5504604884108995\n",
            "TRAING recall class0: 0.9270771069934249 TRAING recall class1: 0.596039603960396 TRAING recall class2: 0.6958637469586375 TRAING recall class3: 0.7416173570019724 TRAING recall class4: 0.5576923076923077 \n",
            "\n",
            "TRAING prec class0: 0.9451553930530164 TRAING prec class1: 0.6903669724770642 TRAING prec class2: 0.6313465783664459 TRAING prec class3: 0.6351351351351351 TRAING prec class4: 0.7435897435897436 \n",
            "\n",
            "TRAING f1 class0: 0.9360289680144839 TRAING f1 class1: 0.6397449521785334 TRAING f1 class2: 0.6620370370370371 TRAING f1 class3: 0.6842584167424931 TRAING f1 class4: 0.6373626373626373 \n",
            "\n",
            "11\n",
            "eval recall class0: 0.9672131147540983 eval recall class1: 0.7538461538461538 eval recall class2: 0.8301886792452831 eval recall class3: 0.8301886792452831 eval recall class4: 0.7272727272727273 \n",
            "\n",
            "eval prec class0: 0.9516129032258065 eval prec class1: 0.8305084745762712 eval prec class2: 0.8301886792452831 eval prec class3: 0.7719298245614035 eval prec class4: 0.8 \n",
            "\n",
            "eval f1 class0: 0.959349593495935 eval f1 class1: 0.7903225806451613 eval f1 class2: 0.8301886792452831 eval f1 class3: 0.8 eval f1 class4: 0.761904761904762 \n",
            "\n",
            "eval LOSS: 0.42032821835683926\n",
            "epoch11_global-step10652  SAVED \n",
            "TRAINIG LOSS: 99 0.5290739976242185\n",
            "TRAING recall class0: 0.9191919191919192 TRAING recall class1: 0.6716417910447762 TRAING recall class2: 0.7758620689655172 TRAING recall class3: 0.7049180327868853 TRAING recall class4: 0.625 \n",
            "\n",
            "TRAING prec class0: 0.9430051813471503 TRAING prec class1: 0.6818181818181818 TRAING prec class2: 0.6617647058823529 TRAING prec class3: 0.6825396825396826 TRAING prec class4: 1.0 \n",
            "\n",
            "TRAING f1 class0: 0.9309462915601022 TRAING f1 class1: 0.6766917293233083 TRAING f1 class2: 0.7142857142857142 TRAING f1 class3: 0.6935483870967742 TRAING f1 class4: 0.7692307692307693 \n",
            "\n",
            "TRAINIG LOSS: 199 0.5274077163590118\n",
            "TRAING recall class0: 0.9193154034229829 TRAING recall class1: 0.6641221374045801 TRAING recall class2: 0.7692307692307693 TRAING recall class3: 0.6785714285714286 TRAING recall class4: 0.5483870967741935 \n",
            "\n",
            "TRAING prec class0: 0.9447236180904522 TRAING prec class1: 0.7131147540983607 TRAING prec class2: 0.6474820143884892 TRAING prec class3: 0.6333333333333333 TRAING prec class4: 0.8095238095238095 \n",
            "\n",
            "TRAING f1 class0: 0.9318463444857498 TRAING f1 class1: 0.6877470355731226 TRAING f1 class2: 0.7031250000000001 TRAING f1 class3: 0.6551724137931035 TRAING f1 class4: 0.6538461538461537 \n",
            "\n",
            "TRAINIG LOSS: 299 0.5291459405282513\n",
            "TRAING recall class0: 0.9217252396166135 TRAING recall class1: 0.6185567010309279 TRAING recall class2: 0.740506329113924 TRAING recall class3: 0.7215909090909091 TRAING recall class4: 0.5869565217391305 \n",
            "\n",
            "TRAING prec class0: 0.9428104575163399 TRAING prec class1: 0.7100591715976331 TRAING prec class2: 0.6256684491978609 TRAING prec class3: 0.6446700507614214 TRAING prec class4: 0.7714285714285715 \n",
            "\n",
            "TRAING f1 class0: 0.9321486268174476 TRAING f1 class1: 0.6611570247933883 TRAING f1 class2: 0.6782608695652174 TRAING f1 class3: 0.6809651474530831 TRAING f1 class4: 0.6666666666666667 \n",
            "\n",
            "TRAINIG LOSS: 399 0.531752641616622\n",
            "TRAING recall class0: 0.9249394673123487 TRAING recall class1: 0.6515151515151515 TRAING recall class2: 0.7254901960784313 TRAING recall class3: 0.7346938775510204 TRAING recall class4: 0.6065573770491803 \n",
            "\n",
            "TRAING prec class0: 0.9478908188585607 TRAING prec class1: 0.7226890756302521 TRAING prec class2: 0.6218487394957983 TRAING prec class3: 0.6617647058823529 TRAING prec class4: 0.8043478260869565 \n",
            "\n",
            "TRAING f1 class0: 0.9362745098039216 TRAING f1 class1: 0.6852589641434262 TRAING f1 class2: 0.669683257918552 TRAING f1 class3: 0.6963249516441006 TRAING f1 class4: 0.691588785046729 \n",
            "\n",
            "TRAINIG LOSS: 499 0.5234593953108415\n",
            "TRAING recall class0: 0.9269932756964457 TRAING recall class1: 0.6615384615384615 TRAING recall class2: 0.7354085603112841 TRAING recall class3: 0.7392739273927392 TRAING recall class4: 0.6216216216216216 \n",
            "\n",
            "TRAING prec class0: 0.9516765285996055 TRAING prec class1: 0.7214765100671141 TRAING prec class2: 0.6363636363636364 TRAING prec class3: 0.6686567164179105 TRAING prec class4: 0.8214285714285714 \n",
            "\n",
            "TRAING f1 class0: 0.9391727493917275 TRAING f1 class1: 0.6902086677367576 TRAING f1 class2: 0.6823104693140795 TRAING f1 class3: 0.7021943573667712 TRAING f1 class4: 0.7076923076923075 \n",
            "\n",
            "TRAINIG LOSS: 599 0.5307266211688209\n",
            "TRAING recall class0: 0.9285714285714286 TRAING recall class1: 0.6641604010025063 TRAING recall class2: 0.7161290322580646 TRAING recall class3: 0.7209944751381215 TRAING recall class4: 0.6024096385542169 \n",
            "\n",
            "TRAING prec class0: 0.9475839475839476 TRAING prec class1: 0.7142857142857143 TRAING prec class2: 0.6379310344827587 TRAING prec class3: 0.6557788944723618 TRAING prec class4: 0.8064516129032258 \n",
            "\n",
            "TRAING f1 class0: 0.9379813538710985 TRAING f1 class1: 0.6883116883116883 TRAING f1 class2: 0.6747720364741643 TRAING f1 class3: 0.6868421052631579 TRAING f1 class4: 0.689655172413793 \n",
            "\n",
            "TRAINIG LOSS: 699 0.5240040542438094\n",
            "TRAING recall class0: 0.9268626110731374 TRAING recall class1: 0.6681514476614699 TRAING recall class2: 0.73224043715847 TRAING recall class3: 0.7302325581395349 TRAING recall class4: 0.5760869565217391 \n",
            "\n",
            "TRAING prec class0: 0.9495798319327731 TRAING prec class1: 0.7142857142857143 TRAING prec class2: 0.639618138424821 TRAING prec class3: 0.6738197424892703 TRAING prec class4: 0.7910447761194029 \n",
            "\n",
            "TRAING f1 class0: 0.9380837080594948 TRAING f1 class1: 0.6904487917146145 TRAING f1 class2: 0.6828025477707007 TRAING f1 class3: 0.7008928571428572 TRAING f1 class4: 0.6666666666666666 \n",
            "\n",
            "TRAINIG LOSS: 799 0.52260077291372\n",
            "TRAING recall class0: 0.9290399522957663 TRAING recall class1: 0.6428571428571429 TRAING recall class2: 0.7506053268765133 TRAING recall class3: 0.7345309381237525 TRAING recall class4: 0.5904761904761905 \n",
            "\n",
            "TRAING prec class0: 0.9476885644768857 TRAING prec class1: 0.7184035476718403 TRAING prec class2: 0.6444906444906445 TRAING prec class3: 0.6752293577981652 TRAING prec class4: 0.7848101265822784 \n",
            "\n",
            "TRAING f1 class0: 0.9382716049382717 TRAING f1 class1: 0.6785340314136126 TRAING f1 class2: 0.6935123042505593 TRAING f1 class3: 0.7036328871892926 TRAING f1 class4: 0.6739130434782609 \n",
            "\n",
            "12\n",
            "eval recall class0: 0.9672131147540983 eval recall class1: 0.7538461538461538 eval recall class2: 0.8301886792452831 eval recall class3: 0.9433962264150944 eval recall class4: 0.7272727272727273 \n",
            "\n",
            "eval prec class0: 0.9833333333333333 eval prec class1: 0.8909090909090909 eval prec class2: 0.8301886792452831 eval prec class3: 0.746268656716418 eval prec class4: 0.8 \n",
            "\n",
            "eval f1 class0: 0.9752066115702478 eval f1 class1: 0.8166666666666667 eval f1 class2: 0.8301886792452831 eval f1 class3: 0.8333333333333333 eval f1 class4: 0.761904761904762 \n",
            "\n",
            "eval LOSS: 0.38916239592727725\n",
            "epoch12_global-step11612  SAVED \n",
            "TRAINIG LOSS: 99 0.4602824905700982\n",
            "TRAING recall class0: 0.9485981308411215 TRAING recall class1: 0.5384615384615384 TRAING recall class2: 0.8076923076923077 TRAING recall class3: 0.8529411764705882 TRAING recall class4: 0.5 \n",
            "\n",
            "TRAING prec class0: 0.9575471698113207 TRAING prec class1: 0.8 TRAING prec class2: 0.7241379310344828 TRAING prec class3: 0.7073170731707317 TRAING prec class4: 0.5384615384615384 \n",
            "\n",
            "TRAING f1 class0: 0.9530516431924883 TRAING f1 class1: 0.6436781609195402 TRAING f1 class2: 0.7636363636363636 TRAING f1 class3: 0.7733333333333334 TRAING f1 class4: 0.5185185185185186 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-14f84ecb8e90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-b44b22f43736>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m# if (step + 1) % grad_accumulation_steps == 0:)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                     group['eps'])\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP6M0uJnxVLK"
      },
      "source": [
        "## entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0kzBvFuxYez"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class Inference_dataset(Dataset):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.preprocess_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        with open('purpose_classification/common_country_sample_idx.json', 'r') as f:\n",
        "            self.idxes = json.load(f)\n",
        "\n",
        "        print('DATA AMOUNT:', len(self.idxes))\n",
        "            \n",
        "        # self.idx_target = list(self.data) # data_idx, target\n",
        "\n",
        "        path2newfiles = 'purpose_classification'\n",
        "        file_handle = open('/'.join([path2newfiles, 'global_country_idx2data.json']), 'r')\n",
        "        self.idx2data = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        file_handle = open('/'.join([path2newfiles, 'common_country_sample_idx.json']), 'r')\n",
        "        self.common_country_sample_idx = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        file_handle = open('/'.join([path2newfiles, 'global_country_token.json']), 'r')\n",
        "        self.token = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        file_handle = open('/'.join([path2newfiles, 'global_country_data.json']), 'r')\n",
        "        self.pos = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        file_handle = open('/'.join([path2newfiles, 'global_country2idx.json']), 'r')\n",
        "        self.global_country2idx = json.load(file_handle)\n",
        "        file_handle.close()\n",
        "\n",
        "        # get full list of country for each cat_name+row_idx\n",
        "        done = [\"reviews_Musical_Instruments_5\",  \"reviews_Automotive_5\",\n",
        "                \"reviews_Cell_Phones_and_Accessories_5\", \"reviews_Toys_and_Games_5\", \n",
        "                \"reviews_Office_Products_5\", \"reviews_Pet_Supplies_5\",\n",
        "                \"reviews_Digital_Music_5\", 'reviews_Tools_and_Home_Improvement_5',\n",
        "                \"reviews_Clothing_Shoes_and_Jewelry_5\", \"reviews_Amazon_Instant_Video_5\"]\n",
        "\n",
        "        try:\n",
        "            with open('purpose_classification_new/fail.json', 'w') as f:\n",
        "                self.data = json.load(f)\n",
        "        except:\n",
        "            print('FAIL TO LOAD EXISTING DATA')\n",
        "            find_complete_country_list = {}\n",
        "            incomplete_data = []\n",
        "            for index in self.idxes:\n",
        "                # import pdb\n",
        "                # pdb.set_trace()\n",
        "                info = self.idx2data[str(index)]\n",
        "                text = self.token[str(info['category'])][str(info['row_idx'])][1]\n",
        "                incomplete_data.append((index, text, info))\n",
        "                find_complete_country_list[(info['category'], int(info['row_idx']))] = 0\n",
        "            \n",
        "            for name in done:\n",
        "                file_path = '/'.join([name+'_all', name])\n",
        "                for row_idx, data in enumerate(open(file_path+'_feature.json', 'r')):\n",
        "                    if (name, row_idx) in find_complete_country_list:\n",
        "                        feature = json.loads(data)\n",
        "                        assert len(feature['COUNTRY']) > 0\n",
        "                        find_complete_country_list[(name, row_idx)] = feature['COUNTRY']\n",
        "            \n",
        "            # combine all data to self.data\n",
        "            raw_data = []\n",
        "            skip_list = []\n",
        "            skip_c = 0\n",
        "            for data in incomplete_data:\n",
        "                index, text, info = data\n",
        "                assert len(find_complete_country_list[(info['category'], int(info['row_idx']))]) != 0\n",
        "                country_list = find_complete_country_list[(info['category'], info['row_idx'])]\n",
        "\n",
        "                # if  self.pos[str(index)]['end'] >= 400 or int(info['country_idx']) >= 6 or index in ['22154', '26418', '29455', '7214', '500', '7284', '24013', '6981', '2850']:\n",
        "                if self.pos[str(index)]['end'] >= 400 or index in ['22154', '26418', '29455', '7214', '500', '7284', '24013', '6981', '2850']:\n",
        "                    skip_list.append(index)\n",
        "                    skip_c += 1\n",
        "                    continue\n",
        "                raw_data.append((index, 0, text, info, country_list, self.pos[str(index)]['start'], self.pos[str(index)]['end'])) # data_idx, target, text, info(info['country_idx'], info['country_name']), country_list, start, end\n",
        "            print(skip_c)\n",
        "\n",
        "            self.data = []\n",
        "\n",
        "            for r_d in raw_data:\n",
        "                if self.check_if_tokenize_can_find_country(r_d, r_d[2]):\n",
        "                    self.data.append(r_d)\n",
        "                else:\n",
        "                    skip_c += 1\n",
        "                    skip_list.append(r_d[0])\n",
        "            \n",
        "            print('SKIP NUMBER', skip_c)\n",
        "            print('SKIP LIST', skip_list)\n",
        "\n",
        "            with open('purpose_classification_new/entire_training/all_dataset_complete_data.json', 'w') as f:\n",
        "                json.dump(self.data, f)\n",
        "    \n",
        "            with open('purpose_classification_new/entire_training/all_dataset_skip_list.json', 'w') as f:\n",
        "                json.dump(skip_list, f)\n",
        "\n",
        "    def check_if_tokenize_can_find_country(self, r_d, text):\n",
        "            d = {}\n",
        "            d['country_name'] = r_d[3]['country_name']\n",
        "            d['country_idx'] = r_d[3]['country_idx']\n",
        "            d['country_list'] = r_d[-3]\n",
        "            d['start'] = r_d[-2]\n",
        "            d['end'] = r_d[-1]\n",
        "            tokens = self.preprocess_tokenizer(text.lower(), return_tensors=\"pt\", padding='max_length', max_length=512, truncation=True)\n",
        "            input_ids, _, _ = tokens['input_ids'], tokens['token_type_ids'], tokens['attention_mask']\n",
        "            start, end = find_country_idx(d, self.preprocess_tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
        "\n",
        "            if start == 1000 and end == 1000:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # (text, target, country_name, country_idx, country_list)\n",
        "        return {'index':self.data[idx][0], 'text':self.data[idx][2], 'target':int(self.data[idx][1])-1, 'country_name':self.data[idx][3]['country_name'], \n",
        "                'country_idx':self.data[idx][3]['country_idx'], 'country_list':self.data[idx][-3], 'start':self.data[idx][-2], 'end':self.data[idx][-1]}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def find_country_idx(d, tokenized_sent):\n",
        "    country_name, country_idx, country_list, start, end = d['country_name'], d['country_idx'], d['country_list'], d['start'], d['end']\n",
        "    words_length = len(country_name.split(' '))\n",
        "    record = []\n",
        "    new_sent = []\n",
        "    # print(tokenized_sent)\n",
        "    for idx, t in enumerate(tokenized_sent):\n",
        "        if idx == 0:\n",
        "            last_t = t\n",
        "            record.append(1)\n",
        "            new_sent.append(t)\n",
        "            continue\n",
        "        \n",
        "        if t.startswith('##'):\n",
        "            record[-1] += 1\n",
        "            new_sent[-1] = new_sent[-1] + t[2:]\n",
        "        else:\n",
        "            record.append(1)\n",
        "            new_sent.append(t)\n",
        "\n",
        "    assert len(record) == len(new_sent)\n",
        "\n",
        "    pointer = 0\n",
        "    start = 0\n",
        "    loop = 0\n",
        "    while 1:\n",
        "        cur_words = country_list[pointer].lower().split(' ')\n",
        "        cur_words_len = len(cur_words)\n",
        "        # print('WORD TO FIND:', cur_words)\n",
        "        for i in range(start, len(new_sent)-cur_words_len, 1):\n",
        "\n",
        "            if len(re.sub(\"[^A-Za-z']+\", '', new_sent[i])) == 0:\n",
        "                continue\n",
        "\n",
        "            w2compare = [re.sub(\"[^A-Za-z']+\", '', new_sent[i])]\n",
        "        \n",
        "            j = i+1\n",
        "            while len(w2compare) < cur_words_len:\n",
        "                if len(re.sub(\"[^A-Za-z']+\", '', new_sent[j])) > 0:\n",
        "                    w2compare.append(new_sent[j])\n",
        "                j += 1\n",
        "            \n",
        "            if w2compare == cur_words:\n",
        "                if country_idx != pointer:\n",
        "                    # print('CURRENT WORD==WORD TO FIND', new_sent[i:j], cur_words, country_idx, pointer, i)\n",
        "                    pointer += 1\n",
        "                    start = i+cur_words_len\n",
        "                    break\n",
        "                else:\n",
        "                    # print('CURRENT WORD==WORD TO FIND and RETURN', new_sent[i:j], cur_words, country_idx, pointer, i)\n",
        "                    start_ = sum(record[:i])\n",
        "                    end_ = sum(record[:j])\n",
        "                    # print(tokenized_sent[start_:end_], new_sent[i:j])\n",
        "                    return start_, end_\n",
        "\n",
        "        if i == len(new_sent)-cur_words_len-1:\n",
        "            print('SKIP')\n",
        "            return 1000, 1000\n",
        "\n",
        "    if start_ == 0 and end_ == 0:\n",
        "        print('ERROR')\n",
        "        import pdb\n",
        "        pdb.set_trace()\n",
        "    \n",
        "    return start_, end_\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
        "def collate_fn(data):\n",
        "    # tokenized sentence batchxsequence x3\n",
        "    # target batchx1\n",
        "    # country_mask batchxsequence\n",
        "    input_ids_list = []\n",
        "    token_type_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    targets = []\n",
        "    country_mask = torch.zeros((len(data), 512))\n",
        "    indexes = []\n",
        "    for idx,d in enumerate(data):\n",
        "        # print(d['target'])\n",
        "        tokens = tokenizer(d['text'].lower(), return_tensors=\"pt\", padding='max_length', max_length=512, truncation=True)\n",
        "        input_ids, token_type_ids, attention_mask = tokens['input_ids'], tokens['token_type_ids'], tokens['attention_mask']\n",
        "        start, end = find_country_idx(d, tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
        "        if start == 1000 and end == 1000:\n",
        "            print('ERROR')\n",
        "            import pdb\n",
        "            pdb.set_trace()\n",
        "        country_mask[idx, start:end] = 1\n",
        "        indexes.append(d['index'])\n",
        "\n",
        "        input_ids_list.append(input_ids)\n",
        "        token_type_ids_list.append(token_type_ids)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "\n",
        "    input_ids_list = torch.cat(input_ids_list, dim=0)\n",
        "    token_type_ids_list = torch.cat(token_type_ids_list, dim=0)\n",
        "    attention_mask_list = torch.cat(attention_mask_list, dim=0)\n",
        "    \n",
        "    return country_mask, input_ids_list, token_type_ids_list, attention_mask_list, indexes\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PHhayua_sy7",
        "outputId": "cf9ae14e-067a-42f7-b6cc-c728826a39eb"
      },
      "source": [
        "all_dataset = Inference_dataset()\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "all_data_loader = DataLoader(dataset=all_dataset, batch_size=batch_size, \n",
        "                               shuffle=False, collate_fn=collate_fn)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA AMOUNT: 31278\n",
            "FAIL TO LOAD EXISTING DATA\n",
            "2207\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP\n",
            "SKIP NUMBER 2318\n",
            "SKIP LIST [144, 305, 537, 544, 986, 1032, 1186, 1187, 1226, 1247, 1248, 1249, 1505, 2258, 2366, 2376, 2610, 2661, 2738, 2741, 2783, 2784, 2816, 2817, 2837, 2866, 2895, 2987, 3042, 3145, 3146, 3179, 3380, 3752, 3865, 3922, 4131, 4146, 4153, 4158, 4205, 4233, 4278, 4289, 4290, 4301, 4537, 4593, 4605, 4639, 4642, 4909, 4925, 5133, 5141, 5330, 5781, 5884, 6328, 6421, 6688, 6819, 6840, 6850, 6857, 6879, 7085, 7162, 7166, 8005, 8070, 8186, 8199, 9005, 9514, 10325, 11424, 11590, 11591, 11814, 11815, 11817, 11819, 12438, 12672, 12961, 13407, 13524, 13657, 13658, 13800, 13801, 13859, 13986, 14304, 14310, 14529, 14530, 14531, 14756, 14791, 14794, 14795, 14796, 14885, 14888, 14890, 14891, 14892, 14893, 15356, 15364, 15365, 15578, 15689, 15968, 16232, 16301, 16334, 16358, 16510, 16562, 16608, 16610, 16688, 16694, 16739, 16740, 16756, 16869, 16995, 16998, 17000, 17001, 17043, 17044, 17116, 17129, 17131, 17224, 17441, 17481, 17633, 17634, 17639, 17703, 17847, 17849, 17942, 17943, 18131, 18133, 18176, 18178, 18193, 18225, 18240, 18411, 18438, 18439, 18442, 18455, 18646, 18704, 18899, 18951, 18953, 19024, 19029, 19212, 19403, 19410, 19545, 19558, 19576, 19581, 19632, 19650, 19665, 19684, 19706, 19758, 19759, 19847, 19856, 19991, 20033, 20038, 20099, 20124, 20144, 20146, 20147, 20150, 20162, 20236, 20377, 20391, 20397, 20398, 20399, 20402, 20413, 20423, 20430, 20433, 20439, 20658, 20818, 20820, 20821, 20845, 20981, 20984, 20985, 20990, 21121, 22462, 22463, 22504, 23053, 23054, 23055, 23056, 23057, 23153, 23167, 23698, 23706, 27780, 28365, 28366, 28368, 29109, 29657, 29664, 31203, 31206, 31207, 31208, 31209, 31210, 31212, 31213, 31215, 31216, 31218, 31247, 31271, 31545, 31583, 31709, 31760, 32076, 32605, 33437, 33438, 33440, 33577, 33578, 33579, 33617, 33676, 33875, 33883, 34077, 34096, 35028, 35500, 35974, 35993, 36191, 36192, 36193, 36268, 36773, 37235, 37762, 38725, 39009, 49, 137, 138, 180, 214, 237, 258, 311, 315, 394, 420, 479, 521, 536, 859, 1150, 1180, 1184, 1616, 1719, 1948, 2133, 2134, 2396, 2603, 2890, 2924, 2986, 3023, 3027, 3041, 3046, 3059, 3256, 3308, 3309, 3310, 3440, 3445, 3524, 3684, 3722, 3860, 3861, 4015, 4107, 4108, 4120, 4147, 4148, 4149, 4173, 4418, 4769, 5522, 5571, 5747, 5856, 6420, 6422, 6667, 6813, 6941, 7075, 7551, 7553, 8188, 8190, 8393, 8842, 9633, 9818, 10118, 10339, 10587, 10934, 11068, 11470, 11813, 11818, 11912, 11913, 12183, 12584, 12585, 12627, 12783, 13427, 13428, 13472, 13733, 13770, 14174, 14520, 14532, 14609, 14681, 14792, 14793, 15067, 15347, 15469, 15483, 15680, 15682, 15818, 15850, 16070, 16096, 16243, 16325, 16333, 16335, 16336, 16337, 16389, 16511, 16513, 16718, 16877, 16901, 16982, 16983, 16985, 17042, 17072, 17122, 17128, 17501, 17548, 17608, 17670, 17749, 17846, 17848, 17850, 17984, 18132, 18164, 18454, 18456, 18952, 19131, 19160, 19189, 19224, 19237, 19337, 19338, 19373, 19490, 19526, 19559, 19662, 19682, 19731, 19733, 19734, 19735, 19738, 19740, 19741, 19742, 19743, 19744, 19757, 19852, 19876, 19902, 20094, 20100, 20145, 20317, 20375, 20376, 20819, 21258, 21445, 21453, 21810, 22082, 22085, 22105, 22191, 22663, 22683, 22723, 22878, 22879, 22886, 22894, 23754, 23755, 23778, 24068, 24455, 24978, 24979, 24980, 24981, 24985, 25066, 25281, 26127, 26288, 26649, 26661, 27152, 27419, 27523, 27525, 27536, 27545, 27546, 27736, 28342, 28506, 28906, 28934, 29305, 29494, 29496, 30083, 31733, 31734, 31735, 31736, 31774, 31837, 31838, 32606, 32607, 32834, 35403, 37171, 37196, 37310, 37311, 37312, 37379, 37380, 37438, 37514, 38155, 39073, 142, 145, 146, 147, 148, 150, 170, 171, 172, 175, 176, 177, 182, 184, 199, 201, 202, 209, 238, 260, 261, 275, 285, 286, 298, 313, 318, 321, 322, 324, 325, 328, 363, 364, 365, 366, 367, 371, 386, 467, 468, 480, 481, 547, 606, 615, 617, 629, 655, 663, 701, 798, 801, 858, 907, 944, 946, 953, 960, 961, 968, 971, 972, 1062, 1069, 1136, 1137, 1140, 1141, 1153, 1160, 1163, 1167, 1168, 1169, 1172, 1173, 1175, 1181, 1182, 1206, 1207, 1208, 1209, 1210, 1211, 1228, 1229, 1253, 1254, 1318, 1369, 1380, 1381, 1420, 1468, 1469, 1604, 1618, 1622, 1758, 1759, 1760, 1833, 1872, 2050, 2068, 2157, 2167, 2170, 2183, 2207, 2218, 2219, 2375, 2389, 2574, 2642, 2643, 2655, 2678, 2690, 2767, 2888, 2903, 2911, 2915, 3005, 3006, 3148, 3264, 3325, 3326, 3327, 3668, 3677, 3733, 4150, 4302, 4347, 4435, 4597, 4598, 4826, 4828, 5063, 5064, 5114, 5115, 5145, 5146, 5147, 5417, 6160, 6173, 6563, 6626, 6661, 6728, 6814, 6841, 6982, 7064, 7068, 7308, 7425, 7512, 7682, 7683, 7758, 8150, 8937, 9013, 12027, 12087, 12121, 12521, 12522, 12641, 12643, 14168, 14183, 14311, 15488, 15489, 15865, 15881, 16341, 16813, 16984, 17520, 17780, 17920, 18192, 18269, 18518, 18717, 19035, 19126, 19190, 19454, 19546, 19579, 19839, 19840, 19841, 19843, 19977, 19987, 20039, 20115, 20308, 20381, 20382, 20388, 20440, 20444, 20575, 20576, 21001, 21046, 21142, 21143, 21154, 21156, 21157, 21199, 21202, 21284, 21316, 21416, 21435, 21438, 21463, 21516, 21688, 21734, 21735, 21745, 21746, 21791, 21792, 21817, 21823, 21871, 22092, 22094, 22192, 22204, 22241, 22267, 22271, 22287, 22310, 22365, 22376, 22413, 22497, 22531, 22534, 22535, 22546, 22558, 22559, 22564, 22613, 22622, 22624, 22699, 22741, 22884, 22925, 23031, 23036, 23043, 23065, 23066, 23075, 23077, 23078, 23099, 23100, 23101, 23102, 23103, 23104, 23136, 23137, 23138, 23139, 23140, 23240, 23247, 23273, 23394, 23395, 23426, 23427, 23583, 23599, 23628, 23630, 23633, 23638, 23639, 23640, 23641, 23651, 23742, 23892, 23907, 24016, 24029, 24048, 24096, 24097, 24098, 24113, 24130, 24158, 24238, 24334, 24347, 24389, 24390, 24400, 24441, 24476, 24524, 24705, 24744, 24889, 24977, 24982, 25141, 25142, 25143, 25247, 25256, 25257, 25275, 25753, 25836, 25948, 25998, 26188, 26753, 26773, 26774, 27317, 27347, 27415, 27460, 27461, 27462, 27465, 27575, 27608, 27612, 27722, 27770, 27791, 27868, 27869, 27953, 27969, 28124, 28208, 28554, 28632, 28715, 28716, 28779, 28841, 28864, 28980, 29080, 29210, 29211, 29222, 29260, 29302, 29382, 29383, 29421, 29464, 29512, 29549, 29727, 29728, 29729, 29730, 29775, 29796, 29800, 29801, 29802, 30269, 30515, 31244, 31370, 31629, 31630, 31747, 31751, 31836, 32086, 33507, 34076, 34933, 35973, 36079, 36294, 36295, 37066, 37163, 37287, 37485, 38149, 38368, 38523, 38723, 38917, 38929, 39152, 24, 27, 57, 296, 554, 623, 1319, 2300, 2390, 2559, 2659, 4267, 7804, 7945, 20319, 21080, 21471, 22007, 22081, 22095, 22104, 24912, 24914, 25339, 25340, 25341, 25342, 27789, 28040, 30516, 34934, 37615, 37980, 38326, 38451, 1358, 1359, 6003, 6152, 6821, 15936, 25081, 28327, 29812, 29813, 29814, 31201, 31202, 31204, 31205, 31211, 31214, 31217, 31219, 32550, 37108, 4159, 7490, 10933, 11522, 19372, 31109, 33433, 33618, 37350, 284, 439, 440, 543, 994, 1149, 3376, 4459, 5296, 5301, 5313, 5314, 5525, 7422, 8722, 12629, 22068, 24281, 25186, 26192, 26790, 26791, 26817, 26818, 26819, 26820, 26821, 26822, 26823, 26824, 26842, 26843, 26844, 26845, 26846, 26847, 26848, 26849, 26850, 26870, 26896, 26897, 27115, 27116, 27120, 27130, 27131, 27181, 27436, 28075, 29297, 33380, 37237, 37320, 38534, 39322, 22, 25, 9634, 19140, 21841, 25268, 31749, 31753, 37521, 38301, 39236, 23, 26, 530, 3907, 21611, 23003, 23222, 23241, 23863, 24796, 24805, 24806, 25010, 25018, 25020, 26984, 26989, 27335, 27336, 29324, 29418, 29452, 32877, 34291, 39054, 15522, 17471, 21387, 22205, 24828, 27542, 27738, 27739, 28048, 28396, 28543, 28560, 28978, 37193, 37194, 37243, 143, 213, 257, 1478, 2091, 2142, 2889, 2910, 3353, 4060, 4569, 4570, 4595, 4596, 5113, 5293, 5294, 5295, 5297, 6116, 6168, 6269, 6851, 6852, 7319, 7552, 7696, 7720, 15554, 16717, 16755, 16999, 17130, 17683, 17748, 18523, 19258, 20983, 21124, 21417, 22103, 22424, 22826, 22871, 22923, 22937, 22938, 22939, 23345, 23370, 23378, 23392, 23393, 23396, 23411, 23446, 23447, 23779, 24088, 24242, 24243, 24244, 24447, 24572, 24935, 24983, 24986, 24987, 25222, 25223, 25224, 26045, 26374, 27178, 27289, 27725, 27726, 28574, 29060, 29223, 30082, 31537, 31646, 31741, 33675, 37095, 37220, 37360, 37361, 37421, 37579, 37753, 48, 59, 299, 319, 373, 477, 478, 607, 656, 657, 658, 1164, 1165, 1166, 1170, 1171, 1174, 1230, 1231, 2156, 2166, 2418, 2652, 3255, 7685, 7686, 7688, 12642, 15834, 16503, 19836, 19837, 19838, 19842, 21120, 21141, 21152, 21153, 21155, 21158, 21201, 21266, 21270, 21434, 21517, 21528, 21529, 21530, 21532, 21731, 21732, 21733, 21736, 21737, 21738, 21740, 21816, 21822, 21895, 21938, 21958, 22060, 22061, 22062, 22067, 22080, 22084, 22086, 22093, 22145, 22193, 22194, 22230, 22232, 22263, 22268, 22288, 22390, 22391, 22496, 22508, 22509, 22530, 22532, 22533, 22536, 22557, 22560, 22562, 22563, 22565, 22566, 22610, 22611, 22612, 22623, 22625, 22636, 22724, 22738, 22796, 22885, 22895, 23052, 23064, 23076, 23210, 23221, 23235, 23239, 23269, 23296, 23479, 23629, 23652, 23739, 23740, 23741, 23743, 23861, 23904, 23905, 23906, 23996, 23997, 23998, 24015, 24017, 24030, 24105, 24112, 24114, 24115, 24116, 24167, 24345, 24348, 24401, 24440, 24442, 24469, 24545, 24546, 24640, 24792, 24890, 24908, 24948, 25017, 25019, 25021, 25164, 25208, 25215, 25258, 25298, 25326, 25472, 25488, 25489, 25751, 25768, 25802, 25867, 25949, 25950, 26754, 27179, 27267, 27463, 27464, 27466, 27558, 27574, 27700, 27701, 27788, 27790, 27809, 27836, 27867, 27870, 27890, 27970, 28064, 28065, 28141, 28142, 28200, 28236, 28335, 28336, 28397, 28494, 28552, 28553, 28633, 28699, 28714, 28717, 28741, 28742, 28747, 28748, 28751, 28780, 28881, 29079, 29193, 29314, 29316, 29352, 29355, 29435, 29479, 31107, 31746, 31748, 31750, 31752, 33614, 33615, 33616, 33619, 34337, 34338, 34339, 36080, 37162, 37170, 37197, 37207, 37236, 37513, 37554, 37689, 38154, 38163, 38164, 38333, 1891, 15470, 15485, 15848, 15862, 15984, 16168, 16514, 16640, 16693, 17045, 17445, 17482, 17914, 19580, 19855, 20158, 20230, 20378, 2195, 3345, 10689, 10690, 10691, 10692, 10693, 18441, 24533, 24534, 28361, 139, 140, 141, 208, 212, 567, 1220, 1265, 1390, 1506, 1623, 2184, 2185, 2231, 2397, 2600, 2799, 2809, 2810, 2825, 2849, 2867, 2868, 2869, 3295, 3395, 3669, 4522, 4714, 4990, 5300, 5408, 6016, 6367, 6368, 6546, 10887, 12811, 13128, 15348, 15614, 16018, 16233, 16234, 16576, 16609, 16646, 16994, 17034, 17226, 18060, 18290, 18435, 19202, 19238, 19379, 19871, 19872, 19873, 19874, 19875, 20169, 20229, 20400, 20401, 21009, 21107, 21356, 21359, 21399, 21407, 21408, 21411, 21412, 21415, 21495, 21531, 21645, 21662, 21863, 21868, 21902, 21928, 22035, 22112, 22335, 22395, 22482, 22583, 22598, 22599, 22632, 22635, 22659, 22664, 22678, 22694, 22848, 22987, 23024, 23121, 23288, 23456, 23508, 23605, 23691, 23788, 24173, 24286, 24439, 24457, 24463, 24496, 24535, 24588, 24738, 24746, 24760, 24771, 24909, 24967, 25146, 25242, 25249, 25271, 25321, 25336, 25501, 25657, 25725, 25868, 25961, 25977, 25978, 26102, 26238, 26466, 26476, 26486, 26491, 26507, 26635, 26660, 26713, 26737, 26757, 26762, 26929, 26977, 27024, 27203, 27204, 27205, 27206, 27209, 27211, 27230, 27232, 27233, 27235, 27265, 27367, 27381, 27425, 27426, 27427, 27607, 27630, 27682, 27690, 27744, 27773, 27904, 27910, 27939, 28030, 28114, 28154, 28172, 28178, 28326, 28337, 28343, 28344, 28360, 28466, 28486, 28487, 28498, 28507, 28520, 28615, 28618, 28639, 28781, 28895, 28898, 29107, 29198, 29235, 29255, 29258, 29353, 29429, 29490, 29528, 29541, 29553, 29559, 29997, 30766, 31108, 31245, 31534, 33205, 37071, 37098, 37113, 37172, 37198, 37254, 37288, 37290, 37412, 37413, 37414, 37432, 37766, 37967, 38289, 38330, 38363, 38366, 38461, 38478, 38479, 38505, 38705, 38751, 38823, 38879, 38980, 39027, 39050, 39137, 39363, 39397, 39398, 39399, 39431, 39432, 3375, 6627, 21892, 30081, 30514, 320, 895, 5167, 7165, 15125, 21695, 21806, 22386, 25522, 27382, 27394, 27396, 27399, 27405, 27406, 28433, 28658, 29066, 29323, 29428, 30071, 38292, 39075, 398, 441, 624, 1945, 2220, 2230, 3296, 3732, 3751, 3824, 3938, 4161, 4452, 4641, 6544, 6597, 6598, 6817, 6818, 13369, 13429, 15690, 15823, 16993, 17480, 17483, 17662, 17680, 17684, 17819, 18279, 18886, 19017, 19023, 19557, 20207, 20223, 20431, 20434, 21739, 22244, 25272, 27628, 29656, 29663, 36305, 37783, 21610, 23132, 25299, 25658, 26963, 28463, 29390, 37130, 37455, 316, 317, 646, 2331, 2437, 2601, 2602, 3825, 3840, 4276, 6582, 7065, 7307, 8192, 10932, 10955, 11816, 14889, 15681, 16512, 17033, 17054, 17702, 19800, 20132, 20133, 20293, 21328, 21450, 21510, 22393, 22633, 22887, 23037, 23042, 23490, 23767, 24219, 24645, 25415, 25976, 26193, 26398, 27279, 27877, 27878, 28013, 28358, 28437, 28438, 28496, 28499, 28624, 28625, 28626, 28628, 28635, 28636, 28644, 29145, 29146, 29149, 29168, 29169, 37059, 37292, 38365, 38367, 38369, 38447, 39067, 39103, 39321, 200, 424, 616, 3290, 4152, 4434, 4754, 6754, 6795, 8800, 8801, 8902, 16035, 18436, 21556, 21581, 22828, 22829, 22941, 24575, 24822, 25192, 25193, 25197, 26306, 27439, 28761, 29480, 29536, 30359, 31536, 37060, 37195, 37283, 37784, 38217, 38281, 38282, 38283, 38284, 38285, 38286, 38290, 38493, 2505, 4151, 4234, 4277, 4712, 4755, 6596, 6815, 6820, 7167, 12977, 18268, 18437, 18440, 18545, 18546, 19975, 20221, 20285, 20392, 21960, 22151, 22942, 25122, 26219, 26220, 29173, 29381, 29526, 31535, 34653, 35404, 37061, 37451, 37642, 37785, 37786, 38532, 38916, 38932, 307, 2658, 5044, 11491, 16231, 17225, 17866, 17922, 21200, 21490, 21609, 21614, 21615, 21646, 21670, 22481, 23203, 23455, 23769, 23770, 24036, 24363, 24470, 24745, 24759, 25047, 25101, 25248, 25270, 25436, 25622, 25887, 26152, 26239, 26250, 26983, 27038, 27039, 27040, 27053, 27054, 27305, 27306, 27587, 27588, 27798, 27799, 27839, 27840, 27841, 27844, 28044, 28198, 28235, 28436, 28444, 28451, 29082, 29315, 29356, 31246, 37099, 37127, 37128, 37129, 37233, 37234, 37468, 37480, 37868, 38198, 38211, 38331, 38332, 38656, 39038, 39194, 39195, 39504, 22298, 22819, 22872, 22940, 22992, 22993, 22994, 22995, 25225, 33877, 33882, 37280, 37385, 37386, 38880, 3182, 3439, 21413, 24502, 25670, 28831, 28832, 38087, 38737, 2114, 3982, 21125, 21498, 21515, 21869, 22709, 25690, 26022, 27107, 29221, 35257, 37824, 13364, 26124, 26495, 26973, 37094, 37560, 37858, 3798, 6515, 6516, 23642, 24907, 25269, 25659, 28656, 29298, 37511, 37545, 37706, 2558, 21176, 21357, 21358, 21369, 21386, 21400, 22020, 22034, 22079, 22083, 22087, 22088, 22090, 22091, 22414, 22518, 22654, 23020, 23507, 23509, 24335, 24444, 24642, 28099, 29354, 31248, 33432, 33434, 33436, 33439, 37238, 38724, 38771, 3297, 3306, 3307, 5292, 5315, 15688, 19012, 19943, 21555, 21795, 23441, 25133, 30073, 33876, 34075, 34078, 35972, 35976, 37284, 4940, 5077, 5365, 5366, 5591, 5592, 5894, 6263, 6319, 6320, 15636, 16695, 16696, 17024, 26272, 26280, 27156, 29333, 29762, 32560, 36280, 37285, 38080, 16456, 21083, 21441, 23204, 23410, 23727, 24478, 24752, 24913, 24915, 25334, 25335, 27266, 28171, 29088, 29108, 37512, 37522, 39157, 206, 2850, 14247, 18755, 20775, 20866, 23048, 31200, 10112, 19614, 20387, 21948, 23688, 25582, 27544, 160, 205, 269, 442, 443, 497, 498, 499, 500, 501, 502, 773, 825, 866, 940, 1213, 1364, 2315, 6327, 6707, 6752, 6981, 6983, 6984, 7209, 7210, 7211, 7212, 7213, 7214, 7647, 20495, 22190, 22567, 28058, 28060, 29351, 29455, 30513, 35411, 35443, 36338, 24013, 26832, 26887, 26895, 27504, 22869, 22870, 1385, 2085, 25487, 2309, 2371, 20865, 22154, 22669, 23689, 26266, 26274, 26321, 5636, 27505, 397, 27503, 11593, 28631, 28643, 28677, 31130, 39263, 6753, 26418, 29177, 22153, 23409, 21590, 27838, 3977, 24053, 27506, 38252, 22019, 19010, 5059, 7206, 7207, 7208, 7283, 7284, 7296, 7464, 16588, 18520, 20283, 20296]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn9lBpNXwOJG"
      },
      "source": [
        "## entire inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOGCecKLe1wh",
        "outputId": "20ed8fd8-05e0-44d2-ec9d-5ff05898c60f"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf8D3q0tdavn",
        "outputId": "33e6c824-5182-4235-c844-f6a0bb2df457"
      },
      "source": [
        "bert = HDCTModel.from_pretrained('bert-base-uncased').to(device)\n",
        "metadata = torch.load('purpose_classification_new/entire_training/epoch12_global-step11612')\n",
        "bert.load_state_dict(metadata['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of HDCTModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.fc.weight', 'bert.fc.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OdqN3xwoUP9"
      },
      "source": [
        "def inference_entire_dataset(model, all_dataloader):\n",
        "    model.eval()\n",
        "    data_results = {}\n",
        "    stats = {0:0, 1:0, 2:0, 3:0, 4:0}\n",
        "    for step, example in enumerate(all_dataloader):\n",
        "        # get outputs\n",
        "        country_mask, input_ids, token_type_ids, attention_mask, indexes = example\n",
        "        country_mask = country_mask.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        # labels = labels.to(device)\n",
        "        outputs = model(country_mask=country_mask, input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # print(outputs.size())\n",
        "        prediction = torch.argmax(outputs, dim=-1).squeeze().cpu().data.numpy()\n",
        "\n",
        "        for index, pred in zip(indexes, prediction):\n",
        "            if pred != 0:\n",
        "                print(pred)\n",
        "            data_results[index] = int(pred)\n",
        "            stats[int(pred)] += 1\n",
        "\n",
        "        del country_mask, input_ids, token_type_ids, attention_mask, indexes\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if (step+1)%1000 == 0:\n",
        "            print(step)\n",
        "            with open('purpose_classification_new/entire_training/entire_dataset_inference_results.json', 'w') as f:\n",
        "                json.dump(data_results, f)\n",
        "    \n",
        "    with open('purpose_classification_new/entire_training/entire_dataset_inference_results.json', 'w') as f:\n",
        "        json.dump(data_results, f)\n",
        "\n",
        "    print(stats)\n",
        "    print(data_results)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDOh_gTxAeGf"
      },
      "source": [
        "## Stats analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mALA9vP2AdHR"
      },
      "source": [
        "country_worth2lookat = {'china':['china', 'ch'], 'us':['usa', 'us', 'america', 'united states', 'the states'], \n",
        "                      'uk':['uk', 'england', 'britain', 'ireland', 'scotland'], \n",
        "                      'japan':['japan'], 'mexico':['mexico'], 'germany': ['germany'], \n",
        "                      'canada':['canada'], 'india':['india'],  'australia':['australia'], \n",
        "                      'italy':['italy'], 'france':['france'], 'taiwan':['taiwan'], 'vietnam':['vietnam'], \n",
        "                      'thailand':['thailand'], 'brazil':['brazil'], 'korea':['korea'], \n",
        "                      'afghanistan':['afghanistan'], 'spain':['spain'], 'new zealand':['new zealand'], \n",
        "                      'singapore':['singapore'], 'iraq':['iraq'], 'russia':['russia'], 'switzerland':['switzerland']}\n",
        "\n",
        "\n",
        "nationality_worth2lookat = {'american': ['american'], 'english': ['english'], 'german':['german'], \n",
        "                          'chinese':['chinese'], 'british':['british', 'scottish', 'irish'], \n",
        "                          'spanish':['spanish'], 'french':['french'], 'japanese':['japanese'], \n",
        "                          'european':['european'], 'australian':['australian'], \n",
        "                          'african':['african'], 'asian':['asian'], 'italian':['italian'], \n",
        "                          'canadian':['canadian'],  'indian':['indian'], \n",
        "                          'russian':['russian'], 'mexican':['mexican']} # 17\n",
        "\n",
        "# build word2label\n",
        "word2label_country = {}\n",
        "word2label_nationality = {}\n",
        "for key, item in country_worth2lookat.items():\n",
        "    for word in item:\n",
        "        word2label_country[word] = key\n",
        "for key, item in nationality_worth2lookat.items():\n",
        "    for word in item:\n",
        "        word2label_nationality[word] = key\n",
        "\n",
        "path2newfiles = 'purpose_classification'\n",
        "file_handle = open('/'.join([path2newfiles, 'global_country_idx2data.json']), 'r')\n",
        "idx2data = json.load(file_handle)\n",
        "file_handle.close()\n",
        "\n",
        "file_handle = open('/'.join([path2newfiles, 'common_country_sample_idx.json']), 'r')\n",
        "common_country_sample_idx = json.load(file_handle)\n",
        "file_handle.close()\n",
        "\n",
        "file_handle = open('/'.join([path2newfiles, 'global_country_token.json']), 'r')\n",
        "token = json.load(file_handle)\n",
        "file_handle.close()\n",
        "\n",
        "file_handle = open('/'.join([path2newfiles, 'global_country_data.json']), 'r')\n",
        "pos = json.load(file_handle)\n",
        "file_handle.close()\n",
        "\n",
        "file_handle = open('/'.join([path2newfiles, 'global_country2idx.json']), 'r')\n",
        "global_country2idx = json.load(file_handle)\n",
        "file_handle.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFnnAqVDAnx_",
        "outputId": "d252974b-df55-4282-d884-cd16f7e8564b"
      },
      "source": [
        "\n",
        "with open('purpose_classification_new/entire_training/entire_dataset_inference_results.json', 'r') as f:\n",
        "    ruiqi = json.load(f)\n",
        "\n",
        "# idx2country:\n",
        "idx2country = {}\n",
        "for k, item in global_country2idx.items():\n",
        "    for i in item:\n",
        "        idx2country[int(i)] = k\n",
        "    \n",
        "total = copy.deepcopy(ruiqi)\n",
        "\n",
        "print(len(total))\n",
        "\n",
        "dict_ = {}\n",
        "for k, item in total.items():\n",
        "    print(k, item)\n",
        "    if idx2country[int(k)] not in word2label_country:\n",
        "        print(idx2country[int(k)])\n",
        "        continue\n",
        "    country = word2label_country[idx2country[int(k)]]\n",
        "    if country not in dict_:\n",
        "        dict_[country] = {item:1}\n",
        "    else:\n",
        "        if item not in dict_[country]:\n",
        "            dict_[country][item] = 1\n",
        "        else:\n",
        "            dict_[country][item] += 1\n",
        "\n",
        "print(dict_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "370 3\n",
            "674 2\n",
            "735 2\n",
            "1333 2\n",
            "1832 3\n",
            "2334 2\n",
            "2539 0\n",
            "2628 2\n",
            "2629 2\n",
            "2651 3\n",
            "2988 2\n",
            "3553 1\n",
            "3760 1\n",
            "3953 3\n",
            "4230 2\n",
            "4248 2\n",
            "4264 3\n",
            "4300 3\n",
            "4469 3\n",
            "4993 3\n",
            "4994 3\n",
            "5001 2\n",
            "5047 1\n",
            "5180 2\n",
            "5334 2\n",
            "5335 2\n",
            "5336 3\n",
            "5337 3\n",
            "5653 0\n",
            "5774 2\n",
            "5776 3\n",
            "6485 0\n",
            "6611 0\n",
            "6806 0\n",
            "6807 0\n",
            "6859 2\n",
            "6947 2\n",
            "7004 0\n",
            "7005 0\n",
            "7225 0\n",
            "7257 2\n",
            "8237 0\n",
            "8238 0\n",
            "9017 0\n",
            "9018 0\n",
            "10000 2\n",
            "10127 0\n",
            "11754 3\n",
            "11761 3\n",
            "14686 0\n",
            "15105 0\n",
            "15331 0\n",
            "15600 3\n",
            "15844 2\n",
            "15846 0\n",
            "17625 2\n",
            "18799 0\n",
            "18943 2\n",
            "19956 0\n",
            "20349 3\n",
            "20351 1\n",
            "20720 0\n",
            "21160 0\n",
            "21195 3\n",
            "21208 2\n",
            "21489 0\n",
            "21638 0\n",
            "21693 0\n",
            "21694 0\n",
            "21696 0\n",
            "21697 0\n",
            "21702 0\n",
            "21978 0\n",
            "22239 0\n",
            "22259 2\n",
            "22693 2\n",
            "23187 2\n",
            "23513 3\n",
            "23678 0\n",
            "23679 0\n",
            "23692 2\n",
            "23880 3\n",
            "24181 3\n",
            "24199 2\n",
            "24204 2\n",
            "24550 3\n",
            "24552 3\n",
            "24671 2\n",
            "24925 3\n",
            "25120 2\n",
            "25151 3\n",
            "25293 2\n",
            "25550 2\n",
            "25942 3\n",
            "25974 2\n",
            "26256 3\n",
            "26407 3\n",
            "26954 0\n",
            "27379 3\n",
            "27385 3\n",
            "27386 3\n",
            "27387 3\n",
            "27389 3\n",
            "27390 3\n",
            "27391 0\n",
            "27393 3\n",
            "27395 3\n",
            "27397 3\n",
            "27398 3\n",
            "27400 3\n",
            "27401 3\n",
            "27402 3\n",
            "27403 3\n",
            "27404 3\n",
            "27407 3\n",
            "27493 3\n",
            "27572 3\n",
            "27728 3\n",
            "27729 3\n",
            "27730 3\n",
            "27731 3\n",
            "27897 3\n",
            "27923 1\n",
            "27926 3\n",
            "28002 3\n",
            "28046 3\n",
            "28224 0\n",
            "28423 3\n",
            "28708 2\n",
            "28769 3\n",
            "29063 0\n",
            "29064 0\n",
            "29065 0\n",
            "29510 3\n",
            "29804 2\n",
            "29805 2\n",
            "29807 2\n",
            "29818 2\n",
            "29819 2\n",
            "29821 2\n",
            "29965 2\n",
            "30072 1\n",
            "30270 2\n",
            "30313 2\n",
            "30360 2\n",
            "30770 2\n",
            "30800 2\n",
            "30969 1\n",
            "31006 0\n",
            "31059 1\n",
            "31092 3\n",
            "31136 2\n",
            "31228 2\n",
            "31314 1\n",
            "31315 1\n",
            "31316 1\n",
            "31317 1\n",
            "31318 3\n",
            "31319 0\n",
            "31320 0\n",
            "31325 2\n",
            "31334 2\n",
            "31380 0\n",
            "31657 2\n",
            "31963 1\n",
            "31969 3\n",
            "31985 1\n",
            "31986 1\n",
            "32016 2\n",
            "32136 2\n",
            "32316 2\n",
            "32506 1\n",
            "32660 1\n",
            "32689 1\n",
            "32690 0\n",
            "32819 2\n",
            "32867 2\n",
            "33108 2\n",
            "33400 3\n",
            "33407 2\n",
            "33828 2\n",
            "34327 0\n",
            "34399 2\n",
            "34461 2\n",
            "34566 2\n",
            "34587 0\n",
            "34714 2\n",
            "34901 2\n",
            "34902 2\n",
            "35203 2\n",
            "35363 3\n",
            "35426 2\n",
            "35558 0\n",
            "35823 2\n",
            "35824 2\n",
            "36253 2\n",
            "36568 2\n",
            "36569 2\n",
            "37085 1\n",
            "37125 2\n",
            "37136 2\n",
            "37140 1\n",
            "37166 1\n",
            "37169 1\n",
            "37401 2\n",
            "37482 2\n",
            "37886 2\n",
            "37887 2\n",
            "37889 2\n",
            "37891 2\n",
            "37892 1\n",
            "37893 1\n",
            "37894 2\n",
            "37895 0\n",
            "37896 0\n",
            "37897 0\n",
            "37899 1\n",
            "37900 0\n",
            "37901 2\n",
            "37902 0\n",
            "38009 3\n",
            "38019 3\n",
            "38127 0\n",
            "38199 0\n",
            "38271 2\n",
            "38288 0\n",
            "38633 1\n",
            "38732 2\n",
            "38734 1\n",
            "38813 2\n",
            "38876 3\n",
            "38878 0\n",
            "38883 3\n",
            "38884 2\n",
            "39079 1\n",
            "39109 2\n",
            "39127 2\n",
            "39128 1\n",
            "39129 1\n",
            "39130 2\n",
            "39364 1\n",
            "39365 2\n",
            "39446 2\n",
            "122 2\n",
            "128 2\n",
            "226 2\n",
            "388 2\n",
            "395 2\n",
            "621 2\n",
            "633 2\n",
            "706 2\n",
            "707 2\n",
            "719 2\n",
            "782 0\n",
            "805 0\n",
            "1003 2\n",
            "1004 3\n",
            "1080 2\n",
            "1188 2\n",
            "1217 1\n",
            "2063 2\n",
            "2736 0\n",
            "3057 2\n",
            "3302 1\n",
            "3349 0\n",
            "3371 2\n",
            "3373 2\n",
            "3386 2\n",
            "3410 2\n",
            "3437 0\n",
            "3443 0\n",
            "3492 2\n",
            "3493 2\n",
            "3494 2\n",
            "3514 0\n",
            "3532 0\n",
            "3587 0\n",
            "3590 3\n",
            "3591 0\n",
            "3658 0\n",
            "3679 0\n",
            "3680 1\n",
            "3681 3\n",
            "3682 0\n",
            "3683 0\n",
            "3698 0\n",
            "3723 0\n",
            "3724 0\n",
            "3726 0\n",
            "3737 0\n",
            "3738 2\n",
            "3739 3\n",
            "3740 2\n",
            "3741 3\n",
            "3742 0\n",
            "3743 2\n",
            "3744 2\n",
            "3746 0\n",
            "3747 0\n",
            "3749 0\n",
            "3750 0\n",
            "3755 0\n",
            "3756 0\n",
            "3759 1\n",
            "3787 2\n",
            "3803 0\n",
            "3810 3\n",
            "3822 2\n",
            "3828 2\n",
            "3829 3\n",
            "3831 3\n",
            "3846 3\n",
            "3873 2\n",
            "3891 0\n",
            "3913 0\n",
            "3928 0\n",
            "3930 2\n",
            "3951 3\n",
            "3952 0\n",
            "3954 2\n",
            "3958 1\n",
            "3965 3\n",
            "3967 3\n",
            "3998 0\n",
            "3999 0\n",
            "4074 2\n",
            "4075 2\n",
            "4126 0\n",
            "4127 0\n",
            "4129 0\n",
            "4163 0\n",
            "4177 2\n",
            "4228 0\n",
            "4231 0\n",
            "4240 0\n",
            "4241 0\n",
            "4243 0\n",
            "4255 0\n",
            "4291 0\n",
            "4336 0\n",
            "4338 2\n",
            "4392 0\n",
            "4424 0\n",
            "4453 0\n",
            "4455 2\n",
            "4470 0\n",
            "4472 0\n",
            "4504 0\n",
            "4528 0\n",
            "4534 0\n",
            "4546 0\n",
            "4553 0\n",
            "4556 0\n",
            "4627 2\n",
            "4631 2\n",
            "4677 0\n",
            "4679 0\n",
            "4683 0\n",
            "4766 2\n",
            "4781 2\n",
            "4782 2\n",
            "4783 2\n",
            "4784 0\n",
            "4787 0\n",
            "4849 2\n",
            "4850 0\n",
            "4856 0\n",
            "4858 0\n",
            "4901 0\n",
            "4928 2\n",
            "4963 2\n",
            "4982 2\n",
            "5000 3\n",
            "5048 0\n",
            "5050 0\n",
            "5051 0\n",
            "5062 2\n",
            "5122 0\n",
            "5181 0\n",
            "5247 2\n",
            "5259 2\n",
            "5263 0\n",
            "5284 0\n",
            "5379 0\n",
            "5442 0\n",
            "5454 2\n",
            "5493 2\n",
            "5494 2\n",
            "5495 2\n",
            "5496 0\n",
            "5498 0\n",
            "5502 2\n",
            "5507 2\n",
            "5528 2\n",
            "5530 2\n",
            "5534 2\n",
            "5535 0\n",
            "5537 0\n",
            "5539 0\n",
            "5541 0\n",
            "5543 0\n",
            "5548 0\n",
            "5572 2\n",
            "5579 0\n",
            "5581 0\n",
            "5605 2\n",
            "5834 0\n",
            "5851 3\n",
            "5861 2\n",
            "5943 0\n",
            "5999 0\n",
            "6040 1\n",
            "6141 0\n",
            "6161 0\n",
            "6162 0\n",
            "6222 3\n",
            "6313 2\n",
            "6486 0\n",
            "6550 0\n",
            "6559 0\n",
            "6561 2\n",
            "6584 0\n",
            "6585 0\n",
            "6612 0\n",
            "6644 0\n",
            "6671 0\n",
            "6717 0\n",
            "6718 0\n",
            "6720 0\n",
            "6721 0\n",
            "6729 0\n",
            "6838 0\n",
            "6870 0\n",
            "6954 0\n",
            "6957 0\n",
            "7258 0\n",
            "7322 2\n",
            "7449 0\n",
            "7485 2\n",
            "7642 0\n",
            "8442 2\n",
            "8569 0\n",
            "8655 0\n",
            "8656 0\n",
            "8856 2\n",
            "9088 2\n",
            "9089 3\n",
            "9206 0\n",
            "9819 0\n",
            "10497 2\n",
            "10929 2\n",
            "11297 0\n",
            "12029 0\n",
            "12067 0\n",
            "12176 0\n",
            "12704 0\n",
            "12827 0\n",
            "12891 0\n",
            "13365 0\n",
            "13371 0\n",
            "13389 0\n",
            "13517 0\n",
            "13518 0\n",
            "13521 0\n",
            "13628 0\n",
            "13629 0\n",
            "13630 0\n",
            "13631 0\n",
            "13633 0\n",
            "13635 0\n",
            "13848 0\n",
            "14534 0\n",
            "14535 0\n",
            "14537 0\n",
            "14734 2\n",
            "15373 0\n",
            "15439 2\n",
            "15487 0\n",
            "15621 0\n",
            "15627 0\n",
            "15683 0\n",
            "15684 0\n",
            "15691 0\n",
            "15711 0\n",
            "15712 0\n",
            "15734 0\n",
            "15736 0\n",
            "15739 0\n",
            "15775 3\n",
            "15837 0\n",
            "15841 0\n",
            "16007 0\n",
            "16041 0\n",
            "16078 0\n",
            "16090 0\n",
            "16097 0\n",
            "16124 0\n",
            "16133 0\n",
            "16151 0\n",
            "16159 0\n",
            "16212 0\n",
            "16415 2\n",
            "16430 0\n",
            "16496 0\n",
            "16497 0\n",
            "16534 0\n",
            "16553 0\n",
            "16669 0\n",
            "16674 0\n",
            "16700 0\n",
            "16820 0\n",
            "16822 0\n",
            "16827 0\n",
            "16848 0\n",
            "16858 0\n",
            "16996 0\n",
            "17077 0\n",
            "17078 0\n",
            "17079 0\n",
            "17170 0\n",
            "17257 0\n",
            "17306 0\n",
            "17309 0\n",
            "17313 0\n",
            "17314 0\n",
            "17315 0\n",
            "17448 0\n",
            "17484 0\n",
            "17485 0\n",
            "17486 0\n",
            "17487 0\n",
            "17489 0\n",
            "17494 0\n",
            "17517 0\n",
            "17518 0\n",
            "17524 0\n",
            "17570 0\n",
            "17571 0\n",
            "17601 0\n",
            "17660 0\n",
            "17661 0\n",
            "17663 0\n",
            "17664 0\n",
            "17665 0\n",
            "17679 0\n",
            "17735 0\n",
            "17758 0\n",
            "17809 3\n",
            "17810 0\n",
            "17821 0\n",
            "17878 0\n",
            "17879 0\n",
            "17880 0\n",
            "17881 0\n",
            "17882 0\n",
            "17883 0\n",
            "17897 0\n",
            "17944 0\n",
            "17945 0\n",
            "17949 0\n",
            "17950 0\n",
            "18261 0\n",
            "18296 0\n",
            "18298 2\n",
            "18535 0\n",
            "18610 0\n",
            "18623 0\n",
            "18633 0\n",
            "18778 0\n",
            "18800 0\n",
            "18801 0\n",
            "18802 0\n",
            "18803 0\n",
            "18877 0\n",
            "18881 0\n",
            "18882 0\n",
            "18919 0\n",
            "18920 0\n",
            "19022 0\n",
            "19143 2\n",
            "19152 0\n",
            "19171 0\n",
            "19173 2\n",
            "19239 0\n",
            "19253 0\n",
            "19361 0\n",
            "19402 0\n",
            "19447 0\n",
            "19459 0\n",
            "19552 0\n",
            "19825 2\n",
            "19974 0\n",
            "20183 0\n",
            "20196 0\n",
            "20197 0\n",
            "20220 0\n",
            "20231 0\n",
            "20232 0\n",
            "20233 0\n",
            "20234 0\n",
            "20490 0\n",
            "20535 0\n",
            "20543 0\n",
            "20618 2\n",
            "20620 0\n",
            "20647 0\n",
            "20760 0\n",
            "20776 0\n",
            "20784 2\n",
            "20793 0\n",
            "20807 2\n",
            "20848 2\n",
            "20853 0\n",
            "20859 0\n",
            "20882 0\n",
            "20888 1\n",
            "20896 0\n",
            "20903 2\n",
            "20923 0\n",
            "20929 0\n",
            "20936 0\n",
            "20937 0\n",
            "21197 0\n",
            "21212 0\n",
            "21221 2\n",
            "21272 2\n",
            "21274 0\n",
            "21279 3\n",
            "21280 3\n",
            "21281 3\n",
            "21419 3\n",
            "21535 2\n",
            "21540 0\n",
            "21711 2\n",
            "21727 2\n",
            "21728 0\n",
            "21925 0\n",
            "22130 0\n",
            "22169 2\n",
            "22243 0\n",
            "22251 2\n",
            "22253 0\n",
            "22361 3\n",
            "23225 3\n",
            "23713 0\n",
            "23838 2\n",
            "24175 3\n",
            "24203 0\n",
            "24678 3\n",
            "24680 3\n",
            "24685 3\n",
            "24688 0\n",
            "24689 2\n",
            "24691 2\n",
            "24856 0\n",
            "24878 0\n",
            "24895 0\n",
            "25037 0\n",
            "25499 0\n",
            "25705 3\n",
            "25862 0\n",
            "25943 0\n",
            "26132 0\n",
            "26137 3\n",
            "26967 3\n",
            "27590 2\n",
            "27625 1\n",
            "27687 0\n",
            "27733 2\n",
            "27866 2\n",
            "27881 3\n",
            "27896 3\n",
            "27908 3\n",
            "28212 2\n",
            "28608 1\n",
            "29209 3\n",
            "29347 2\n",
            "29612 0\n",
            "29640 0\n",
            "29650 0\n",
            "29659 0\n",
            "29684 0\n",
            "29763 0\n",
            "29772 0\n",
            "29806 2\n",
            "29904 2\n",
            "29991 2\n",
            "30107 2\n",
            "30122 2\n",
            "30182 3\n",
            "30266 0\n",
            "30268 0\n",
            "30577 2\n",
            "30610 2\n",
            "30781 0\n",
            "30782 3\n",
            "30783 0\n",
            "30816 2\n",
            "30864 2\n",
            "30866 2\n",
            "30905 2\n",
            "31014 0\n",
            "31335 2\n",
            "31404 0\n",
            "31439 2\n",
            "31663 0\n",
            "31728 0\n",
            "31786 0\n",
            "31906 2\n",
            "31949 0\n",
            "32021 2\n",
            "32075 2\n",
            "32194 2\n",
            "32242 2\n",
            "32256 0\n",
            "32459 2\n",
            "32462 2\n",
            "32498 2\n",
            "32733 2\n",
            "32776 2\n",
            "32804 0\n",
            "32820 2\n",
            "33305 0\n",
            "33348 2\n",
            "33353 2\n",
            "33523 2\n",
            "33537 2\n",
            "33695 0\n",
            "33821 2\n",
            "33822 0\n",
            "33931 0\n",
            "33967 2\n",
            "34060 2\n",
            "34242 2\n",
            "34568 2\n",
            "34592 2\n",
            "34947 2\n",
            "35065 2\n",
            "35155 2\n",
            "35490 0\n",
            "35748 0\n",
            "35822 2\n",
            "36021 2\n",
            "36229 0\n",
            "36311 0\n",
            "36408 2\n",
            "36667 0\n",
            "37105 3\n",
            "37124 2\n",
            "37224 1\n",
            "37393 1\n",
            "37517 2\n",
            "37519 3\n",
            "37890 0\n",
            "37898 0\n",
            "38138 2\n",
            "38204 1\n",
            "38492 3\n",
            "38681 2\n",
            "38713 3\n",
            "38721 1\n",
            "38749 3\n",
            "38799 1\n",
            "38801 1\n",
            "38875 3\n",
            "38882 3\n",
            "38956 1\n",
            "38957 2\n",
            "39015 2\n",
            "39077 3\n",
            "39098 1\n",
            "39464 1\n",
            "39513 1\n",
            "126 2\n",
            "1823 2\n",
            "2122 2\n",
            "2179 3\n",
            "2263 3\n",
            "2363 3\n",
            "3560 0\n",
            "3561 3\n",
            "3562 3\n",
            "3564 1\n",
            "3565 3\n",
            "3566 3\n",
            "3567 3\n",
            "3568 1\n",
            "3673 2\n",
            "3968 3\n",
            "5357 3\n",
            "5358 3\n",
            "12102 0\n",
            "13828 3\n",
            "15605 2\n",
            "16140 0\n",
            "18836 0\n",
            "18841 0\n",
            "19826 2\n",
            "20979 2\n",
            "21117 2\n",
            "21624 3\n",
            "21917 3\n",
            "23126 3\n",
            "23134 3\n",
            "23233 0\n",
            "23480 2\n",
            "23756 3\n",
            "23914 3\n",
            "23915 3\n",
            "24174 3\n",
            "24329 3\n",
            "24694 3\n",
            "24695 0\n",
            "24697 3\n",
            "25278 2\n",
            "25302 0\n",
            "25493 3\n",
            "25525 3\n",
            "25685 3\n",
            "25688 3\n",
            "25689 2\n",
            "25788 0\n",
            "25792 3\n",
            "25793 3\n",
            "25799 3\n",
            "26377 1\n",
            "27483 3\n",
            "28143 3\n",
            "28317 2\n",
            "28475 3\n",
            "28768 2\n",
            "29098 2\n",
            "29131 2\n",
            "29174 0\n",
            "29215 3\n",
            "29232 1\n",
            "29233 3\n",
            "29341 3\n",
            "29456 3\n",
            "29457 3\n",
            "29458 3\n",
            "29465 3\n",
            "29816 2\n",
            "29977 3\n",
            "29978 0\n",
            "30693 2\n",
            "30993 2\n",
            "31137 2\n",
            "31301 2\n",
            "31307 2\n",
            "31308 3\n",
            "31313 2\n",
            "31526 3\n",
            "31661 2\n",
            "31662 2\n",
            "31849 2\n",
            "32461 2\n",
            "32502 2\n",
            "32527 2\n",
            "32588 2\n",
            "32762 2\n",
            "32859 2\n",
            "32886 2\n",
            "32981 0\n",
            "32982 0\n",
            "33061 2\n",
            "33354 3\n",
            "33355 3\n",
            "33362 0\n",
            "33397 3\n",
            "33719 2\n",
            "33720 2\n",
            "33903 0\n",
            "33961 2\n",
            "34007 2\n",
            "34322 2\n",
            "34405 2\n",
            "34688 2\n",
            "34946 2\n",
            "34948 3\n",
            "35157 2\n",
            "35181 2\n",
            "35280 2\n",
            "35348 2\n",
            "35795 2\n",
            "35852 0\n",
            "36093 3\n",
            "37181 3\n",
            "37183 2\n",
            "37185 3\n",
            "37416 0\n",
            "37418 1\n",
            "37420 3\n",
            "37450 3\n",
            "37452 3\n",
            "37453 3\n",
            "37457 3\n",
            "37458 3\n",
            "37724 3\n",
            "37725 3\n",
            "37726 3\n",
            "37727 3\n",
            "37728 3\n",
            "37729 0\n",
            "37730 3\n",
            "37732 2\n",
            "37733 3\n",
            "37734 3\n",
            "37735 3\n",
            "37736 3\n",
            "37737 3\n",
            "37738 2\n",
            "37739 3\n",
            "37740 3\n",
            "37741 3\n",
            "37742 3\n",
            "37743 3\n",
            "37744 0\n",
            "37745 3\n",
            "37746 3\n",
            "37847 2\n",
            "38067 3\n",
            "38078 3\n",
            "38165 3\n",
            "38166 2\n",
            "38171 2\n",
            "38172 3\n",
            "38173 3\n",
            "38174 3\n",
            "38176 3\n",
            "38177 0\n",
            "38178 3\n",
            "38179 3\n",
            "38180 3\n",
            "38181 3\n",
            "38182 3\n",
            "38183 2\n",
            "38184 3\n",
            "38185 3\n",
            "38186 2\n",
            "38187 3\n",
            "38188 3\n",
            "38191 0\n",
            "38192 3\n",
            "38193 3\n",
            "38194 3\n",
            "38229 3\n",
            "38231 3\n",
            "38232 3\n",
            "38236 3\n",
            "38341 3\n",
            "38342 3\n",
            "38641 0\n",
            "38647 0\n",
            "38669 2\n",
            "38877 3\n",
            "38885 3\n",
            "38988 3\n",
            "38989 0\n",
            "38990 3\n",
            "38991 3\n",
            "38992 3\n",
            "39018 2\n",
            "39019 2\n",
            "39022 2\n",
            "39025 2\n",
            "39026 3\n",
            "39170 2\n",
            "39177 3\n",
            "39258 3\n",
            "39259 3\n",
            "39260 2\n",
            "39261 3\n",
            "39265 3\n",
            "39267 3\n",
            "39313 2\n",
            "151 2\n",
            "183 2\n",
            "223 2\n",
            "312 2\n",
            "422 2\n",
            "644 2\n",
            "722 2\n",
            "752 1\n",
            "769 0\n",
            "775 2\n",
            "1316 0\n",
            "1376 2\n",
            "1592 2\n",
            "1765 2\n",
            "1873 0\n",
            "1959 1\n",
            "1963 2\n",
            "1964 2\n",
            "1988 2\n",
            "2023 0\n",
            "2180 3\n",
            "2264 3\n",
            "2330 2\n",
            "2364 3\n",
            "2706 0\n",
            "2713 2\n",
            "2877 0\n",
            "3082 2\n",
            "3083 1\n",
            "3164 2\n",
            "3294 2\n",
            "3311 1\n",
            "3316 1\n",
            "3321 2\n",
            "3324 3\n",
            "3331 2\n",
            "3342 3\n",
            "3351 2\n",
            "3352 2\n",
            "3372 0\n",
            "3389 2\n",
            "3392 0\n",
            "3394 2\n",
            "3495 2\n",
            "3540 1\n",
            "3569 2\n",
            "3586 2\n",
            "3600 1\n",
            "3644 1\n",
            "3645 1\n",
            "3646 1\n",
            "3826 2\n",
            "3859 3\n",
            "3868 3\n",
            "3992 1\n",
            "4027 3\n",
            "4030 0\n",
            "4462 2\n",
            "4606 0\n",
            "4727 1\n",
            "4729 1\n",
            "4735 1\n",
            "4822 3\n",
            "4864 0\n",
            "5036 0\n",
            "5176 3\n",
            "5367 1\n",
            "5425 2\n",
            "5503 3\n",
            "5542 3\n",
            "5556 0\n",
            "5689 1\n",
            "5725 3\n",
            "5800 0\n",
            "5804 2\n",
            "5818 3\n",
            "5854 0\n",
            "5954 1\n",
            "6078 0\n",
            "6217 0\n",
            "6326 0\n",
            "6378 0\n",
            "6426 0\n",
            "6467 0\n",
            "6495 0\n",
            "6496 0\n",
            "6504 0\n",
            "6572 3\n",
            "6664 0\n",
            "6674 0\n",
            "7016 0\n",
            "7017 0\n",
            "7033 0\n",
            "7048 0\n",
            "7185 3\n",
            "7256 2\n",
            "7440 3\n",
            "7442 3\n",
            "7443 2\n",
            "7743 0\n",
            "7757 3\n",
            "8081 3\n",
            "8216 0\n",
            "8222 0\n",
            "8230 0\n",
            "8388 0\n",
            "8681 0\n",
            "8686 2\n",
            "8702 2\n",
            "8785 2\n",
            "8987 0\n",
            "9025 0\n",
            "9071 0\n",
            "9126 2\n",
            "9190 1\n",
            "9191 0\n",
            "9273 0\n",
            "9277 0\n",
            "9407 0\n",
            "9420 0\n",
            "9518 2\n",
            "9815 0\n",
            "9817 0\n",
            "10060 0\n",
            "10131 0\n",
            "10134 2\n",
            "10140 0\n",
            "10342 0\n",
            "10439 2\n",
            "10788 0\n",
            "11013 2\n",
            "11111 0\n",
            "11164 2\n",
            "11189 0\n",
            "11472 0\n",
            "11561 0\n",
            "11563 0\n",
            "11564 0\n",
            "11567 0\n",
            "11628 2\n",
            "11683 0\n",
            "11688 0\n",
            "11712 2\n",
            "11721 2\n",
            "11742 0\n",
            "11765 2\n",
            "11808 0\n",
            "11821 0\n",
            "11949 0\n",
            "11971 0\n",
            "12045 2\n",
            "12137 2\n",
            "12174 0\n",
            "12190 0\n",
            "12250 0\n",
            "12252 0\n",
            "12256 0\n",
            "12272 2\n",
            "12352 2\n",
            "12357 3\n",
            "12359 0\n",
            "12464 0\n",
            "12470 0\n",
            "12499 0\n",
            "12553 0\n",
            "12592 0\n",
            "12608 0\n",
            "12816 0\n",
            "12955 0\n",
            "12971 2\n",
            "13132 2\n",
            "13193 2\n",
            "13229 2\n",
            "13314 0\n",
            "13316 0\n",
            "13444 2\n",
            "13465 0\n",
            "13478 0\n",
            "13572 0\n",
            "13705 0\n",
            "13710 0\n",
            "13748 0\n",
            "13760 0\n",
            "13768 0\n",
            "13786 0\n",
            "13963 0\n",
            "14109 0\n",
            "14110 2\n",
            "14308 0\n",
            "14319 0\n",
            "14356 0\n",
            "14460 0\n",
            "14464 0\n",
            "14516 0\n",
            "14591 0\n",
            "14737 0\n",
            "14738 0\n",
            "14798 0\n",
            "14842 0\n",
            "14887 3\n",
            "14927 2\n",
            "14974 0\n",
            "15048 0\n",
            "15073 0\n",
            "15080 0\n",
            "15096 0\n",
            "15119 2\n",
            "15275 0\n",
            "15342 2\n",
            "15366 0\n",
            "15630 0\n",
            "15720 0\n",
            "15981 0\n",
            "16013 3\n",
            "16581 3\n",
            "16605 2\n",
            "16617 0\n",
            "16765 0\n",
            "17032 0\n",
            "17668 0\n",
            "18489 0\n",
            "18589 2\n",
            "18645 0\n",
            "19014 0\n",
            "19122 0\n",
            "19374 2\n",
            "19645 2\n",
            "19795 1\n",
            "19797 3\n",
            "20112 2\n",
            "20114 2\n",
            "20118 0\n",
            "20284 0\n",
            "20320 2\n",
            "20350 3\n",
            "20361 0\n",
            "20362 0\n",
            "20395 0\n",
            "20422 0\n",
            "20548 2\n",
            "20630 2\n",
            "20643 2\n",
            "20657 2\n",
            "20752 0\n",
            "20757 0\n",
            "20861 0\n",
            "20917 2\n",
            "20918 2\n",
            "20947 2\n",
            "21189 3\n",
            "21219 2\n",
            "21245 2\n",
            "21297 1\n",
            "21317 3\n",
            "21318 3\n",
            "21492 3\n",
            "21539 2\n",
            "21542 3\n",
            "21648 2\n",
            "21661 0\n",
            "21667 0\n",
            "21862 3\n",
            "21923 2\n",
            "21944 3\n",
            "21992 2\n",
            "22117 3\n",
            "22161 3\n",
            "22224 1\n",
            "22319 2\n",
            "22360 2\n",
            "22368 3\n",
            "22503 1\n",
            "22645 2\n",
            "22686 0\n",
            "22736 3\n",
            "22779 2\n",
            "22847 2\n",
            "22861 1\n",
            "22905 0\n",
            "22908 0\n",
            "23021 2\n",
            "23281 1\n",
            "23282 3\n",
            "23483 3\n",
            "23484 3\n",
            "23487 3\n",
            "23488 3\n",
            "23491 3\n",
            "23496 3\n",
            "23516 2\n",
            "23520 0\n",
            "23594 2\n",
            "23635 2\n",
            "23654 0\n",
            "23655 0\n",
            "23685 2\n",
            "23694 0\n",
            "23723 2\n",
            "24035 2\n",
            "24313 3\n",
            "24357 3\n",
            "24407 3\n",
            "24430 2\n",
            "24434 0\n",
            "24477 3\n",
            "24514 1\n",
            "24531 3\n",
            "24605 0\n",
            "24698 3\n",
            "24699 2\n",
            "24765 3\n",
            "24770 3\n",
            "24831 3\n",
            "24840 1\n",
            "24841 0\n",
            "24891 2\n",
            "24892 2\n",
            "24917 2\n",
            "24938 2\n",
            "25175 0\n",
            "25539 3\n",
            "25672 2\n",
            "25794 2\n",
            "25804 3\n",
            "25845 0\n",
            "25922 2\n",
            "25967 1\n",
            "25968 2\n",
            "26002 3\n",
            "26134 2\n",
            "26150 3\n",
            "26246 2\n",
            "26260 2\n",
            "26335 2\n",
            "26359 2\n",
            "26432 3\n",
            "26716 1\n",
            "26725 1\n",
            "26731 1\n",
            "26733 1\n",
            "26747 1\n",
            "26976 1\n",
            "27080 0\n",
            "27416 0\n",
            "27618 0\n",
            "27871 2\n",
            "27872 2\n",
            "27873 2\n",
            "27922 2\n",
            "27925 3\n",
            "27927 2\n",
            "28033 0\n",
            "28156 0\n",
            "28242 2\n",
            "28300 1\n",
            "28302 2\n",
            "28315 2\n",
            "28431 2\n",
            "28440 3\n",
            "28441 1\n",
            "28443 1\n",
            "28449 3\n",
            "28450 3\n",
            "28534 2\n",
            "28566 2\n",
            "28620 3\n",
            "28621 3\n",
            "28622 3\n",
            "28623 3\n",
            "28629 3\n",
            "28630 3\n",
            "28634 3\n",
            "28637 3\n",
            "28640 3\n",
            "28641 3\n",
            "28642 3\n",
            "28645 3\n",
            "28646 3\n",
            "28654 2\n",
            "28796 1\n",
            "28797 2\n",
            "28808 1\n",
            "28904 1\n",
            "28988 2\n",
            "29140 3\n",
            "29141 3\n",
            "29142 1\n",
            "29143 3\n",
            "29144 3\n",
            "29148 3\n",
            "29150 3\n",
            "29151 3\n",
            "29152 3\n",
            "29153 3\n",
            "29154 3\n",
            "29155 1\n",
            "29156 1\n",
            "29157 1\n",
            "29158 3\n",
            "29159 3\n",
            "29160 3\n",
            "29161 3\n",
            "29163 3\n",
            "29164 3\n",
            "29165 3\n",
            "29166 3\n",
            "29167 1\n",
            "29171 3\n",
            "29218 1\n",
            "29254 2\n",
            "29277 2\n",
            "29280 2\n",
            "29343 0\n",
            "29371 2\n",
            "29372 2\n",
            "29401 2\n",
            "29406 2\n",
            "29958 2\n",
            "30012 2\n",
            "30079 2\n",
            "30225 1\n",
            "30327 2\n",
            "30530 0\n",
            "30918 2\n",
            "31152 0\n",
            "31253 2\n",
            "31350 0\n",
            "32137 0\n",
            "32173 2\n",
            "32346 0\n",
            "32490 3\n",
            "32724 3\n",
            "32874 3\n",
            "32929 0\n",
            "32941 0\n",
            "33329 2\n",
            "33428 0\n",
            "33481 0\n",
            "33636 3\n",
            "33661 0\n",
            "33790 0\n",
            "33791 0\n",
            "33887 2\n",
            "34570 2\n",
            "34600 1\n",
            "34754 2\n",
            "34825 0\n",
            "35120 1\n",
            "36603 2\n",
            "36957 1\n",
            "37062 1\n",
            "37086 1\n",
            "37096 1\n",
            "37107 1\n",
            "37111 3\n",
            "37222 0\n",
            "37267 2\n",
            "37330 3\n",
            "37446 1\n",
            "37467 0\n",
            "37599 1\n",
            "37600 3\n",
            "37605 1\n",
            "37620 1\n",
            "37624 1\n",
            "37654 1\n",
            "37656 1\n",
            "37683 0\n",
            "37705 3\n",
            "37749 2\n",
            "37773 3\n",
            "37860 0\n",
            "37905 3\n",
            "37917 1\n",
            "37947 1\n",
            "37955 1\n",
            "37968 0\n",
            "37986 1\n",
            "38068 2\n",
            "38111 1\n",
            "38141 1\n",
            "38151 1\n",
            "38202 0\n",
            "38219 1\n",
            "38226 3\n",
            "38276 2\n",
            "38291 3\n",
            "38310 2\n",
            "38314 2\n",
            "38360 1\n",
            "38375 1\n",
            "38377 3\n",
            "38392 0\n",
            "38394 3\n",
            "38417 1\n",
            "38424 3\n",
            "38434 3\n",
            "38445 2\n",
            "38519 3\n",
            "38521 3\n",
            "38527 3\n",
            "38561 1\n",
            "38570 1\n",
            "38596 1\n",
            "38597 1\n",
            "38610 3\n",
            "38673 3\n",
            "38704 1\n",
            "38752 1\n",
            "38758 2\n",
            "38800 2\n",
            "38814 3\n",
            "38815 3\n",
            "38937 1\n",
            "39006 1\n",
            "39087 1\n",
            "39088 3\n",
            "39099 0\n",
            "39100 1\n",
            "39101 1\n",
            "39102 1\n",
            "39111 2\n",
            "39149 1\n",
            "39166 2\n",
            "39268 3\n",
            "39269 1\n",
            "39272 3\n",
            "39273 3\n",
            "39274 1\n",
            "39275 3\n",
            "39276 3\n",
            "39277 3\n",
            "39284 3\n",
            "39340 3\n",
            "39357 1\n",
            "39462 3\n",
            "39474 2\n",
            "39477 1\n",
            "39493 3\n",
            "220 2\n",
            "282 2\n",
            "345 2\n",
            "380 2\n",
            "570 2\n",
            "796 2\n",
            "920 2\n",
            "1093 0\n",
            "1107 2\n",
            "1110 1\n",
            "1111 0\n",
            "1185 2\n",
            "1282 1\n",
            "1861 2\n",
            "1863 2\n",
            "1883 2\n",
            "1950 2\n",
            "2346 0\n",
            "2347 0\n",
            "2489 2\n",
            "2549 0\n",
            "3312 2\n",
            "3343 2\n",
            "3555 2\n",
            "3572 2\n",
            "3580 2\n",
            "3596 2\n",
            "3615 0\n",
            "3672 2\n",
            "3685 0\n",
            "3727 0\n",
            "3776 2\n",
            "3777 2\n",
            "3780 0\n",
            "3781 0\n",
            "3782 2\n",
            "3854 2\n",
            "3920 2\n",
            "3948 2\n",
            "4002 2\n",
            "4014 2\n",
            "4028 2\n",
            "4112 1\n",
            "4210 2\n",
            "4271 2\n",
            "4287 0\n",
            "4370 2\n",
            "4373 0\n",
            "4432 2\n",
            "4661 2\n",
            "4689 0\n",
            "4690 2\n",
            "4691 0\n",
            "4770 2\n",
            "4791 2\n",
            "5168 2\n",
            "5386 2\n",
            "5870 0\n",
            "5873 0\n",
            "5877 0\n",
            "5909 1\n",
            "6202 2\n",
            "6240 2\n",
            "6322 2\n",
            "6433 2\n",
            "6440 2\n",
            "6498 0\n",
            "6499 0\n",
            "6500 0\n",
            "6508 0\n",
            "6749 2\n",
            "6750 2\n",
            "6751 2\n",
            "6847 2\n",
            "7061 2\n",
            "7071 2\n",
            "7179 0\n",
            "7312 2\n",
            "7314 2\n",
            "7405 0\n",
            "7416 2\n",
            "7418 2\n",
            "7420 2\n",
            "7424 2\n",
            "7463 0\n",
            "7646 2\n",
            "8010 0\n",
            "8276 0\n",
            "8643 2\n",
            "8645 2\n",
            "8899 0\n",
            "8966 0\n",
            "8979 0\n",
            "9014 0\n",
            "9043 0\n",
            "9061 0\n",
            "9127 2\n",
            "9372 0\n",
            "9456 2\n",
            "9501 0\n",
            "9505 0\n",
            "9558 0\n",
            "9621 1\n",
            "9623 2\n",
            "9769 0\n",
            "10049 2\n",
            "10281 0\n",
            "10288 0\n",
            "10291 0\n",
            "10292 0\n",
            "10296 0\n",
            "10534 0\n",
            "10699 2\n",
            "10784 0\n",
            "10892 2\n",
            "10911 0\n",
            "11104 2\n",
            "11178 2\n",
            "11483 0\n",
            "11682 0\n",
            "11751 2\n",
            "11927 0\n",
            "12086 2\n",
            "12403 0\n",
            "12404 0\n",
            "12462 0\n",
            "13163 0\n",
            "13355 2\n",
            "13509 2\n",
            "13515 2\n",
            "13595 2\n",
            "13632 2\n",
            "13698 2\n",
            "13707 0\n",
            "13849 0\n",
            "13860 2\n",
            "13964 0\n",
            "13966 2\n",
            "14158 0\n",
            "14352 2\n",
            "14403 0\n",
            "14526 0\n",
            "14528 0\n",
            "15074 2\n",
            "15078 0\n",
            "15089 2\n",
            "15091 2\n",
            "15092 0\n",
            "15114 0\n",
            "15116 2\n",
            "15134 2\n",
            "15145 0\n",
            "15159 0\n",
            "15161 0\n",
            "15176 2\n",
            "15181 2\n",
            "15289 0\n",
            "15341 0\n",
            "15546 2\n",
            "15579 0\n",
            "15617 0\n",
            "15618 0\n",
            "15864 0\n",
            "15894 2\n",
            "15985 0\n",
            "16182 0\n",
            "16368 0\n",
            "16505 0\n",
            "16681 0\n",
            "16721 1\n",
            "16825 0\n",
            "17113 3\n",
            "17516 0\n",
            "17787 0\n",
            "18220 0\n",
            "18380 2\n",
            "18447 0\n",
            "18680 0\n",
            "19087 2\n",
            "19090 2\n",
            "19093 2\n",
            "19094 0\n",
            "19116 2\n",
            "19135 0\n",
            "19166 2\n",
            "19339 2\n",
            "19474 0\n",
            "19476 0\n",
            "19574 2\n",
            "19946 2\n",
            "19963 2\n",
            "20030 2\n",
            "20031 0\n",
            "20116 2\n",
            "20140 2\n",
            "20203 2\n",
            "20454 0\n",
            "20477 0\n",
            "20482 0\n",
            "20622 0\n",
            "20891 0\n",
            "20897 0\n",
            "20920 2\n",
            "20921 0\n",
            "20944 2\n",
            "21058 2\n",
            "21069 2\n",
            "21095 3\n",
            "21239 3\n",
            "21242 1\n",
            "21344 2\n",
            "21477 0\n",
            "21503 0\n",
            "21553 1\n",
            "21686 2\n",
            "22025 0\n",
            "22026 1\n",
            "22178 2\n",
            "22199 0\n",
            "22321 0\n",
            "22364 3\n",
            "22366 2\n",
            "22415 3\n",
            "22416 0\n",
            "22798 2\n",
            "22799 3\n",
            "22800 2\n",
            "22802 2\n",
            "22827 1\n",
            "22855 1\n",
            "22890 2\n",
            "22962 1\n",
            "22963 1\n",
            "23017 3\n",
            "23110 0\n",
            "23122 3\n",
            "23249 2\n",
            "23510 3\n",
            "23517 2\n",
            "23621 2\n",
            "23763 3\n",
            "23764 0\n",
            "23883 3\n",
            "23884 3\n",
            "23886 2\n",
            "23895 0\n",
            "23930 2\n",
            "24213 2\n",
            "24256 2\n",
            "24332 2\n",
            "24443 2\n",
            "24454 2\n",
            "24507 2\n",
            "24560 0\n",
            "24561 1\n",
            "24562 2\n",
            "24563 0\n",
            "24565 2\n",
            "24566 0\n",
            "24567 1\n",
            "24569 2\n",
            "24576 0\n",
            "24604 2\n",
            "24737 2\n",
            "24739 1\n",
            "24742 1\n",
            "25095 1\n",
            "25096 1\n",
            "25189 1\n",
            "25323 3\n",
            "25517 2\n",
            "25540 0\n",
            "25594 2\n",
            "25597 2\n",
            "25598 2\n",
            "25600 2\n",
            "25601 2\n",
            "25602 2\n",
            "25604 2\n",
            "25605 3\n",
            "25632 1\n",
            "25633 0\n",
            "25682 2\n",
            "25692 0\n",
            "25777 2\n",
            "25815 2\n",
            "25870 2\n",
            "25874 2\n",
            "25944 1\n",
            "26006 2\n",
            "26009 2\n",
            "26308 2\n",
            "26310 0\n",
            "26311 0\n",
            "26312 0\n",
            "26316 0\n",
            "26960 2\n",
            "26964 0\n",
            "27051 1\n",
            "27240 2\n",
            "27290 3\n",
            "27291 2\n",
            "27302 3\n",
            "27359 1\n",
            "27442 2\n",
            "27444 1\n",
            "27577 2\n",
            "27760 2\n",
            "27761 2\n",
            "27813 2\n",
            "27848 0\n",
            "27851 2\n",
            "27949 2\n",
            "27966 3\n",
            "28015 2\n",
            "28071 0\n",
            "28072 1\n",
            "28073 1\n",
            "28095 0\n",
            "28096 2\n",
            "28097 0\n",
            "28213 1\n",
            "28268 2\n",
            "28341 2\n",
            "28345 2\n",
            "28384 1\n",
            "28389 0\n",
            "28455 2\n",
            "28477 2\n",
            "28595 1\n",
            "28596 2\n",
            "28732 2\n",
            "28733 3\n",
            "28759 2\n",
            "28760 3\n",
            "28804 2\n",
            "28953 3\n",
            "28975 2\n",
            "28985 2\n",
            "28987 1\n",
            "29213 2\n",
            "29308 2\n",
            "29416 1\n",
            "29453 1\n",
            "29477 1\n",
            "29533 0\n",
            "29540 2\n",
            "29608 0\n",
            "29634 0\n",
            "29638 2\n",
            "29639 2\n",
            "29671 0\n",
            "29672 0\n",
            "29673 3\n",
            "29679 0\n",
            "29680 0\n",
            "29682 0\n",
            "29714 0\n",
            "29715 0\n",
            "29732 0\n",
            "29733 0\n",
            "29734 0\n",
            "29749 0\n",
            "29852 0\n",
            "29906 2\n",
            "29907 3\n",
            "29946 2\n",
            "30052 2\n",
            "30078 2\n",
            "30400 1\n",
            "30401 1\n",
            "30402 0\n",
            "30408 2\n",
            "30424 3\n",
            "30453 0\n",
            "30456 0\n",
            "30605 2\n",
            "30648 2\n",
            "30797 0\n",
            "30948 0\n",
            "31021 2\n",
            "31024 0\n",
            "31032 0\n",
            "31033 2\n",
            "31124 0\n",
            "31222 0\n",
            "31225 0\n",
            "31251 2\n",
            "31252 2\n",
            "31257 2\n",
            "31258 2\n",
            "31358 0\n",
            "31540 2\n",
            "31598 0\n",
            "31599 0\n",
            "31609 0\n",
            "31877 2\n",
            "31960 1\n",
            "32205 2\n",
            "32215 2\n",
            "32264 0\n",
            "32318 0\n",
            "32319 0\n",
            "32320 0\n",
            "32482 2\n",
            "32497 3\n",
            "32584 2\n",
            "32772 2\n",
            "32854 2\n",
            "32888 2\n",
            "32914 3\n",
            "33072 0\n",
            "33123 0\n",
            "33223 0\n",
            "33239 0\n",
            "33293 2\n",
            "33363 2\n",
            "33389 2\n",
            "33399 2\n",
            "33582 2\n",
            "33868 2\n",
            "33944 0\n",
            "33975 2\n",
            "33990 2\n",
            "34058 0\n",
            "34150 2\n",
            "34246 2\n",
            "34292 2\n",
            "34323 2\n",
            "34398 2\n",
            "34567 1\n",
            "34575 2\n",
            "34602 2\n",
            "34712 2\n",
            "34753 2\n",
            "34758 0\n",
            "34793 2\n",
            "34906 2\n",
            "34909 0\n",
            "34912 0\n",
            "34936 2\n",
            "34937 2\n",
            "34978 2\n",
            "35071 2\n",
            "35301 2\n",
            "35420 2\n",
            "35581 2\n",
            "35588 2\n",
            "35810 2\n",
            "35817 2\n",
            "35818 2\n",
            "35952 2\n",
            "36162 0\n",
            "36306 2\n",
            "36451 2\n",
            "36458 0\n",
            "36459 0\n",
            "36519 2\n",
            "36604 2\n",
            "36734 2\n",
            "36840 2\n",
            "36890 2\n",
            "37191 1\n",
            "37258 2\n",
            "37261 0\n",
            "37262 0\n",
            "37263 2\n",
            "37268 2\n",
            "37324 2\n",
            "37325 2\n",
            "37329 2\n",
            "37504 1\n",
            "37505 1\n",
            "37827 2\n",
            "37832 2\n",
            "37961 2\n",
            "37972 2\n",
            "37976 2\n",
            "37979 1\n",
            "38170 0\n",
            "38212 1\n",
            "38213 2\n",
            "38214 2\n",
            "38215 2\n",
            "38216 1\n",
            "38275 0\n",
            "38287 2\n",
            "38309 0\n",
            "38440 2\n",
            "38441 1\n",
            "38467 2\n",
            "38494 3\n",
            "38623 2\n",
            "38631 2\n",
            "38674 0\n",
            "38700 0\n",
            "38820 2\n",
            "38904 0\n",
            "39238 3\n",
            "39239 0\n",
            "39319 2\n",
            "39362 2\n",
            "39508 3\n",
            "39509 0\n",
            "251 2\n",
            "283 2\n",
            "463 2\n",
            "516 2\n",
            "551 3\n",
            "572 2\n",
            "620 2\n",
            "645 2\n",
            "652 2\n",
            "955 2\n",
            "1083 0\n",
            "1084 0\n",
            "1191 0\n",
            "1367 2\n",
            "1438 2\n",
            "1487 2\n",
            "1566 2\n",
            "1585 0\n",
            "2108 2\n",
            "2171 2\n",
            "2432 2\n",
            "2434 2\n",
            "2502 0\n",
            "2573 1\n",
            "2583 3\n",
            "2664 2\n",
            "3262 2\n",
            "3286 2\n",
            "3393 2\n",
            "3426 0\n",
            "3434 0\n",
            "3435 0\n",
            "3523 3\n",
            "3585 3\n",
            "3704 0\n",
            "3789 2\n",
            "4070 0\n",
            "4097 3\n",
            "4109 2\n",
            "4111 3\n",
            "4165 3\n",
            "4306 0\n",
            "4538 2\n",
            "4551 0\n",
            "4584 0\n",
            "4801 3\n",
            "4936 1\n",
            "4937 2\n",
            "5169 2\n",
            "5428 0\n",
            "5740 0\n",
            "5961 2\n",
            "6193 2\n",
            "6315 0\n",
            "6399 0\n",
            "6407 0\n",
            "6412 0\n",
            "6413 0\n",
            "6442 0\n",
            "6473 0\n",
            "6477 0\n",
            "6481 2\n",
            "6509 0\n",
            "6510 2\n",
            "6535 0\n",
            "6547 0\n",
            "6569 0\n",
            "6578 0\n",
            "6599 3\n",
            "6609 0\n",
            "6622 0\n",
            "6659 2\n",
            "6691 0\n",
            "6757 0\n",
            "6759 2\n",
            "6812 0\n",
            "6876 0\n",
            "6878 3\n",
            "6940 2\n",
            "6956 0\n",
            "6988 0\n",
            "7008 0\n",
            "7050 0\n",
            "7080 0\n",
            "7081 0\n",
            "7161 3\n",
            "7199 0\n",
            "7298 1\n",
            "7404 3\n",
            "7447 2\n",
            "7495 0\n",
            "7624 0\n",
            "8009 0\n",
            "8905 2\n",
            "8981 0\n",
            "9845 2\n",
            "10019 0\n",
            "10098 0\n",
            "10504 2\n",
            "10505 2\n",
            "10541 0\n",
            "11008 2\n",
            "11014 0\n",
            "11144 2\n",
            "11175 2\n",
            "11436 0\n",
            "11441 0\n",
            "11443 0\n",
            "11446 0\n",
            "11449 0\n",
            "11451 0\n",
            "11453 0\n",
            "11455 0\n",
            "11800 3\n",
            "12026 2\n",
            "12324 0\n",
            "12682 0\n",
            "12954 0\n",
            "12957 0\n",
            "12958 0\n",
            "12960 0\n",
            "12962 0\n",
            "12963 0\n",
            "12967 0\n",
            "12969 0\n",
            "12970 0\n",
            "12972 0\n",
            "12973 0\n",
            "12974 0\n",
            "12978 0\n",
            "12982 0\n",
            "12983 0\n",
            "12993 0\n",
            "12994 0\n",
            "13008 0\n",
            "14232 0\n",
            "15279 0\n",
            "15281 0\n",
            "15283 0\n",
            "15286 0\n",
            "15294 0\n",
            "15295 0\n",
            "15298 0\n",
            "15343 0\n",
            "15344 0\n",
            "15363 2\n",
            "15375 0\n",
            "15387 0\n",
            "15527 0\n",
            "15529 0\n",
            "15547 0\n",
            "15548 0\n",
            "15550 0\n",
            "15569 2\n",
            "15604 0\n",
            "15608 0\n",
            "15744 2\n",
            "15781 0\n",
            "15804 0\n",
            "15835 0\n",
            "15838 0\n",
            "15839 0\n",
            "15843 0\n",
            "15851 0\n",
            "15860 0\n",
            "15898 0\n",
            "15900 0\n",
            "15906 3\n",
            "15907 0\n",
            "15938 0\n",
            "15950 0\n",
            "16033 2\n",
            "16049 0\n",
            "16051 0\n",
            "16190 0\n",
            "16193 0\n",
            "16237 0\n",
            "16239 0\n",
            "16269 0\n",
            "16354 3\n",
            "16362 0\n",
            "16364 0\n",
            "16365 0\n",
            "16403 2\n",
            "16421 0\n",
            "16437 0\n",
            "16450 0\n",
            "16467 0\n",
            "16483 0\n",
            "16558 0\n",
            "16569 0\n",
            "16578 0\n",
            "16579 0\n",
            "16580 0\n",
            "16627 0\n",
            "16629 0\n",
            "16632 0\n",
            "16635 0\n",
            "16642 0\n",
            "16682 2\n",
            "16811 2\n",
            "16817 0\n",
            "16868 2\n",
            "16897 0\n",
            "16944 0\n",
            "16962 2\n",
            "16991 0\n",
            "17014 2\n",
            "17052 0\n",
            "17100 0\n",
            "17191 2\n",
            "17192 0\n",
            "17193 2\n",
            "17208 0\n",
            "17221 0\n",
            "17247 0\n",
            "17249 0\n",
            "17253 2\n",
            "17258 0\n",
            "17462 0\n",
            "17545 0\n",
            "17698 0\n",
            "17699 0\n",
            "17704 2\n",
            "17737 0\n",
            "17765 0\n",
            "17825 0\n",
            "17852 0\n",
            "17854 0\n",
            "17953 0\n",
            "18011 0\n",
            "18014 2\n",
            "18175 2\n",
            "18197 0\n",
            "18238 2\n",
            "18266 0\n",
            "18299 0\n",
            "18301 0\n",
            "18338 3\n",
            "18382 0\n",
            "18384 0\n",
            "18433 0\n",
            "18510 0\n",
            "18517 0\n",
            "18618 0\n",
            "18631 2\n",
            "18640 0\n",
            "18641 0\n",
            "18777 0\n",
            "18804 0\n",
            "18848 0\n",
            "18890 0\n",
            "18891 0\n",
            "19041 0\n",
            "19048 2\n",
            "19078 0\n",
            "19082 0\n",
            "19284 0\n",
            "19340 0\n",
            "19349 0\n",
            "19494 0\n",
            "19553 0\n",
            "19798 2\n",
            "19939 0\n",
            "19979 0\n",
            "20057 2\n",
            "20168 0\n",
            "20249 0\n",
            "20289 0\n",
            "20322 0\n",
            "20324 0\n",
            "20393 0\n",
            "20550 0\n",
            "20587 2\n",
            "20653 0\n",
            "20655 1\n",
            "20699 0\n",
            "20700 0\n",
            "20703 0\n",
            "20719 0\n",
            "20732 0\n",
            "20792 0\n",
            "21546 3\n",
            "22031 3\n",
            "22642 3\n",
            "22761 3\n",
            "23005 3\n",
            "23333 3\n",
            "23339 1\n",
            "23400 3\n",
            "23401 3\n",
            "23403 3\n",
            "23415 3\n",
            "23416 3\n",
            "24084 2\n",
            "24176 3\n",
            "24177 3\n",
            "24178 3\n",
            "24179 3\n",
            "24209 2\n",
            "24211 1\n",
            "24448 3\n",
            "24465 2\n",
            "25348 3\n",
            "25349 3\n",
            "25820 1\n",
            "25846 3\n",
            "25847 3\n",
            "25848 2\n",
            "25849 3\n",
            "25850 3\n",
            "26195 3\n",
            "26196 3\n",
            "26198 3\n",
            "26199 3\n",
            "26200 3\n",
            "26201 3\n",
            "26202 3\n",
            "26203 3\n",
            "26204 3\n",
            "26205 3\n",
            "26206 3\n",
            "26207 3\n",
            "26208 3\n",
            "26209 3\n",
            "26210 3\n",
            "26211 3\n",
            "26212 3\n",
            "26213 3\n",
            "26214 3\n",
            "26215 3\n",
            "26216 3\n",
            "26217 3\n",
            "26218 3\n",
            "26221 3\n",
            "26222 3\n",
            "26223 3\n",
            "26224 3\n",
            "26225 3\n",
            "26226 3\n",
            "26227 3\n",
            "26228 3\n",
            "26229 3\n",
            "26336 2\n",
            "26384 3\n",
            "26385 2\n",
            "26387 3\n",
            "27264 2\n",
            "27582 3\n",
            "27691 3\n",
            "27692 3\n",
            "28647 3\n",
            "28648 3\n",
            "28649 3\n",
            "28766 3\n",
            "28921 3\n",
            "28923 3\n",
            "29035 3\n",
            "29443 2\n",
            "29489 3\n",
            "29511 3\n",
            "29525 3\n",
            "29527 3\n",
            "29564 0\n",
            "29619 0\n",
            "29678 0\n",
            "29736 2\n",
            "29747 0\n",
            "29841 0\n",
            "29854 0\n",
            "29856 0\n",
            "29865 0\n",
            "29868 0\n",
            "29881 0\n",
            "29889 0\n",
            "29892 0\n",
            "29894 0\n",
            "29898 0\n",
            "29900 0\n",
            "29908 2\n",
            "29927 0\n",
            "29934 0\n",
            "29937 0\n",
            "29955 0\n",
            "29984 0\n",
            "29992 2\n",
            "30024 0\n",
            "30029 0\n",
            "30034 0\n",
            "30089 0\n",
            "30096 0\n",
            "30097 0\n",
            "30115 0\n",
            "30119 0\n",
            "30135 2\n",
            "30164 2\n",
            "30191 0\n",
            "30201 0\n",
            "30234 0\n",
            "30236 0\n",
            "30240 0\n",
            "30241 0\n",
            "30244 0\n",
            "30251 3\n",
            "30252 0\n",
            "30278 0\n",
            "30298 2\n",
            "30321 0\n",
            "30328 0\n",
            "30332 0\n",
            "30334 0\n",
            "30352 0\n",
            "30357 2\n",
            "30387 2\n",
            "30389 3\n",
            "30451 0\n",
            "30547 2\n",
            "30580 2\n",
            "30589 0\n",
            "30591 0\n",
            "30593 0\n",
            "30595 0\n",
            "30711 2\n",
            "30731 0\n",
            "30732 0\n",
            "30761 2\n",
            "30857 0\n",
            "30861 2\n",
            "30885 0\n",
            "30930 0\n",
            "30933 0\n",
            "30935 0\n",
            "30937 0\n",
            "30939 0\n",
            "30942 0\n",
            "30944 0\n",
            "30949 0\n",
            "30958 2\n",
            "31115 2\n",
            "31287 2\n",
            "31305 2\n",
            "31367 0\n",
            "31469 0\n",
            "31482 0\n",
            "31484 3\n",
            "31485 0\n",
            "31487 0\n",
            "31495 2\n",
            "31543 2\n",
            "31546 0\n",
            "31768 2\n",
            "31834 2\n",
            "31873 0\n",
            "31901 3\n",
            "31902 2\n",
            "31903 2\n",
            "31904 2\n",
            "31935 0\n",
            "32040 3\n",
            "32043 2\n",
            "32050 2\n",
            "32067 2\n",
            "32090 3\n",
            "32099 2\n",
            "32184 2\n",
            "32224 0\n",
            "32225 3\n",
            "32247 2\n",
            "32253 0\n",
            "32310 2\n",
            "32315 1\n",
            "32470 2\n",
            "32483 0\n",
            "32538 0\n",
            "32578 2\n",
            "32582 0\n",
            "32591 0\n",
            "32615 2\n",
            "32651 2\n",
            "32685 0\n",
            "32751 0\n",
            "32769 2\n",
            "32786 2\n",
            "32799 2\n",
            "32828 2\n",
            "32837 2\n",
            "32908 0\n",
            "32988 0\n",
            "32991 0\n",
            "32999 0\n",
            "33015 0\n",
            "33017 0\n",
            "33030 0\n",
            "33252 2\n",
            "33258 0\n",
            "33303 0\n",
            "33351 3\n",
            "33386 2\n",
            "33391 2\n",
            "33417 0\n",
            "33467 2\n",
            "33511 2\n",
            "33561 0\n",
            "33583 2\n",
            "33657 2\n",
            "33662 0\n",
            "33698 0\n",
            "33739 2\n",
            "33786 2\n",
            "33789 2\n",
            "33795 0\n",
            "33813 2\n",
            "33820 0\n",
            "33826 0\n",
            "33849 2\n",
            "33867 2\n",
            "33896 0\n",
            "33914 0\n",
            "33916 0\n",
            "33935 2\n",
            "33948 2\n",
            "33964 2\n",
            "33976 2\n",
            "33991 2\n",
            "34080 2\n",
            "34104 0\n",
            "34105 0\n",
            "34108 0\n",
            "34111 0\n",
            "34134 3\n",
            "34160 0\n",
            "34161 2\n",
            "34252 2\n",
            "34253 2\n",
            "34281 2\n",
            "34302 2\n",
            "34363 2\n",
            "34370 0\n",
            "34391 2\n",
            "34392 2\n",
            "34418 2\n",
            "34455 2\n",
            "34483 2\n",
            "34656 2\n",
            "34704 2\n",
            "34833 0\n",
            "34861 0\n",
            "34924 2\n",
            "34925 2\n",
            "34953 2\n",
            "35148 2\n",
            "35168 0\n",
            "35172 0\n",
            "35351 2\n",
            "35373 2\n",
            "35398 2\n",
            "35408 2\n",
            "35424 2\n",
            "35466 2\n",
            "35494 2\n",
            "35541 2\n",
            "35542 2\n",
            "35642 2\n",
            "35669 2\n",
            "35755 2\n",
            "35858 0\n",
            "35912 2\n",
            "35963 0\n",
            "35979 2\n",
            "35980 2\n",
            "36051 0\n",
            "36069 0\n",
            "36082 2\n",
            "36181 2\n",
            "36227 2\n",
            "36318 2\n",
            "36331 2\n",
            "36405 2\n",
            "36485 0\n",
            "36495 2\n",
            "36601 2\n",
            "36747 2\n",
            "36797 2\n",
            "36819 2\n",
            "37011 3\n",
            "37074 2\n",
            "37075 1\n",
            "37077 2\n",
            "37079 1\n",
            "37080 3\n",
            "37081 2\n",
            "37082 1\n",
            "37151 2\n",
            "37160 2\n",
            "37187 3\n",
            "37317 3\n",
            "37323 3\n",
            "37326 0\n",
            "37327 0\n",
            "37336 3\n",
            "37337 3\n",
            "37338 1\n",
            "37339 3\n",
            "37440 3\n",
            "37585 3\n",
            "37586 2\n",
            "37589 3\n",
            "37590 2\n",
            "37592 3\n",
            "37594 2\n",
            "37596 3\n",
            "37598 3\n",
            "37601 3\n",
            "37606 3\n",
            "37607 2\n",
            "37608 3\n",
            "37611 3\n",
            "37613 1\n",
            "37616 2\n",
            "37619 2\n",
            "37621 3\n",
            "37623 3\n",
            "37625 1\n",
            "37627 3\n",
            "37628 2\n",
            "37631 3\n",
            "37633 3\n",
            "37634 3\n",
            "37636 2\n",
            "37637 3\n",
            "37640 3\n",
            "37641 3\n",
            "37643 3\n",
            "37645 3\n",
            "37647 3\n",
            "37651 3\n",
            "37655 2\n",
            "37657 3\n",
            "37863 2\n",
            "38201 0\n",
            "38312 1\n",
            "38607 3\n",
            "38608 2\n",
            "38609 2\n",
            "38910 3\n",
            "38911 3\n",
            "38913 3\n",
            "38915 1\n",
            "38920 3\n",
            "38924 3\n",
            "38925 3\n",
            "38931 3\n",
            "38935 3\n",
            "38936 1\n",
            "38939 3\n",
            "38940 3\n",
            "38942 3\n",
            "38943 3\n",
            "38944 3\n",
            "39218 2\n",
            "39350 3\n",
            "310 3\n",
            "410 0\n",
            "585 2\n",
            "634 2\n",
            "720 2\n",
            "1099 2\n",
            "1525 0\n",
            "1549 3\n",
            "1626 0\n",
            "2083 0\n",
            "2562 2\n",
            "2689 3\n",
            "2932 2\n",
            "3430 3\n",
            "3464 3\n",
            "3762 1\n",
            "3786 2\n",
            "3900 2\n",
            "3924 3\n",
            "3969 1\n",
            "3970 2\n",
            "4181 3\n",
            "4190 3\n",
            "4193 3\n",
            "4343 1\n",
            "4428 3\n",
            "4468 0\n",
            "4540 0\n",
            "4628 4\n",
            "4645 2\n",
            "4908 2\n",
            "4917 3\n",
            "5220 0\n",
            "5265 0\n",
            "5338 3\n",
            "5606 2\n",
            "5607 3\n",
            "5792 2\n",
            "6119 2\n",
            "6194 3\n",
            "6195 2\n",
            "6272 2\n",
            "6471 2\n",
            "6680 3\n",
            "7499 3\n",
            "8654 0\n",
            "8662 2\n",
            "8700 3\n",
            "8701 2\n",
            "9140 3\n",
            "9141 3\n",
            "9469 3\n",
            "9516 2\n",
            "9524 2\n",
            "9734 2\n",
            "10119 2\n",
            "10120 1\n",
            "10193 0\n",
            "10477 2\n",
            "10698 2\n",
            "10928 2\n",
            "11109 2\n",
            "11262 2\n",
            "11334 2\n",
            "11466 2\n",
            "12375 2\n",
            "12396 2\n",
            "12700 0\n",
            "13443 2\n",
            "15332 0\n",
            "15447 2\n",
            "15476 3\n",
            "15555 0\n",
            "15745 0\n",
            "15747 0\n",
            "16039 0\n",
            "16170 2\n",
            "16198 0\n",
            "16245 0\n",
            "16523 0\n",
            "16525 0\n",
            "16571 0\n",
            "16664 2\n",
            "16794 2\n",
            "17095 0\n",
            "17096 0\n",
            "17097 0\n",
            "17338 0\n",
            "17431 0\n",
            "17434 0\n",
            "17712 0\n",
            "17774 2\n",
            "17786 3\n",
            "17900 2\n",
            "17936 3\n",
            "17977 0\n",
            "17979 2\n",
            "18155 0\n",
            "18292 0\n",
            "18383 0\n",
            "18408 2\n",
            "18570 2\n",
            "18571 2\n",
            "18586 3\n",
            "18713 3\n",
            "18827 2\n",
            "18838 0\n",
            "18839 0\n",
            "18864 0\n",
            "18898 2\n",
            "19060 0\n",
            "19199 3\n",
            "19344 2\n",
            "19375 2\n",
            "19524 3\n",
            "19988 3\n",
            "19998 2\n",
            "19999 2\n",
            "20032 3\n",
            "20130 3\n",
            "20336 3\n",
            "20408 2\n",
            "20579 2\n",
            "20586 2\n",
            "20594 2\n",
            "20596 3\n",
            "20713 3\n",
            "20722 2\n",
            "20734 2\n",
            "20737 2\n",
            "20738 2\n",
            "20758 0\n",
            "20804 2\n",
            "20805 2\n",
            "20835 2\n",
            "20836 1\n",
            "20837 3\n",
            "20863 2\n",
            "20864 2\n",
            "20934 2\n",
            "20935 2\n",
            "20966 0\n",
            "20973 3\n",
            "20998 3\n",
            "20999 0\n",
            "21015 2\n",
            "21016 0\n",
            "21040 2\n",
            "21053 2\n",
            "21070 2\n",
            "21076 3\n",
            "21077 3\n",
            "21082 2\n",
            "21099 2\n",
            "21137 2\n",
            "21172 3\n",
            "21246 0\n",
            "21273 0\n",
            "21276 0\n",
            "21277 0\n",
            "21278 3\n",
            "21329 3\n",
            "21493 0\n",
            "21497 3\n",
            "21507 3\n",
            "21508 1\n",
            "21513 3\n",
            "21541 3\n",
            "21587 0\n",
            "21588 0\n",
            "21589 0\n",
            "21591 0\n",
            "21592 0\n",
            "21593 0\n",
            "21594 0\n",
            "21595 0\n",
            "21596 0\n",
            "21597 0\n",
            "21598 0\n",
            "21599 2\n",
            "21600 0\n",
            "21601 0\n",
            "21602 0\n",
            "21603 0\n",
            "21604 0\n",
            "21605 0\n",
            "21606 0\n",
            "21608 0\n",
            "21612 0\n",
            "21613 0\n",
            "21616 0\n",
            "21617 0\n",
            "21618 0\n",
            "21619 0\n",
            "21620 0\n",
            "21621 0\n",
            "21622 0\n",
            "21623 0\n",
            "21625 0\n",
            "21626 0\n",
            "21628 0\n",
            "21629 0\n",
            "21630 0\n",
            "21639 0\n",
            "21644 3\n",
            "21666 2\n",
            "21671 3\n",
            "21672 0\n",
            "21673 2\n",
            "21675 0\n",
            "21676 3\n",
            "21679 0\n",
            "21682 0\n",
            "21684 0\n",
            "21691 2\n",
            "21753 0\n",
            "21754 3\n",
            "21755 0\n",
            "21756 0\n",
            "21757 0\n",
            "21759 0\n",
            "21760 0\n",
            "21761 0\n",
            "21762 0\n",
            "21763 0\n",
            "21771 2\n",
            "21774 2\n",
            "21787 3\n",
            "21788 3\n",
            "21789 0\n",
            "21790 2\n",
            "21793 3\n",
            "21809 2\n",
            "21833 1\n",
            "21859 2\n",
            "21872 2\n",
            "21901 3\n",
            "21920 2\n",
            "22013 1\n",
            "22015 2\n",
            "22016 2\n",
            "22018 2\n",
            "22040 3\n",
            "22197 2\n",
            "22214 1\n",
            "22216 3\n",
            "22218 0\n",
            "22225 2\n",
            "22226 3\n",
            "22228 3\n",
            "22280 2\n",
            "22358 3\n",
            "22362 3\n",
            "22476 3\n",
            "22478 2\n",
            "22542 0\n",
            "22730 2\n",
            "22731 2\n",
            "22782 2\n",
            "22834 3\n",
            "22837 1\n",
            "22849 2\n",
            "22863 3\n",
            "22891 3\n",
            "22968 3\n",
            "22975 2\n",
            "23039 3\n",
            "23201 2\n",
            "23231 2\n",
            "23258 3\n",
            "23272 2\n",
            "23309 3\n",
            "23420 2\n",
            "23421 2\n",
            "23434 2\n",
            "23437 2\n",
            "23440 2\n",
            "23489 3\n",
            "23498 2\n",
            "23634 2\n",
            "23703 3\n",
            "23725 1\n",
            "23726 1\n",
            "23768 3\n",
            "23771 3\n",
            "23772 3\n",
            "23773 3\n",
            "23776 3\n",
            "23777 3\n",
            "23780 3\n",
            "23781 3\n",
            "23959 2\n",
            "24031 3\n",
            "24032 3\n",
            "24037 3\n",
            "24038 3\n",
            "24039 1\n",
            "24041 3\n",
            "24042 3\n",
            "24043 3\n",
            "24045 3\n",
            "24046 3\n",
            "24069 3\n",
            "24073 0\n",
            "24074 3\n",
            "24075 1\n",
            "24094 1\n",
            "24110 2\n",
            "24122 2\n",
            "24134 2\n",
            "24162 1\n",
            "24163 2\n",
            "24170 2\n",
            "24171 2\n",
            "24172 0\n",
            "24190 3\n",
            "24195 2\n",
            "24196 2\n",
            "24200 2\n",
            "24252 2\n",
            "24254 2\n",
            "24268 0\n",
            "24294 1\n",
            "24328 2\n",
            "24331 2\n",
            "24344 0\n",
            "24373 1\n",
            "24396 1\n",
            "24399 2\n",
            "24427 3\n",
            "24467 3\n",
            "24488 3\n",
            "24489 3\n",
            "24509 1\n",
            "24578 3\n",
            "24657 3\n",
            "24667 1\n",
            "24749 2\n",
            "24775 3\n",
            "24778 2\n",
            "24779 1\n",
            "24781 3\n",
            "24872 3\n",
            "24918 2\n",
            "24924 3\n",
            "24937 2\n",
            "24994 0\n",
            "25007 0\n",
            "25009 0\n",
            "25022 0\n",
            "25028 2\n",
            "25034 2\n",
            "25051 2\n",
            "25058 2\n",
            "25119 2\n",
            "25166 3\n",
            "25211 3\n",
            "25267 0\n",
            "25279 1\n",
            "25315 2\n",
            "25331 3\n",
            "25333 2\n",
            "25337 3\n",
            "25514 2\n",
            "25520 3\n",
            "25647 1\n",
            "25652 3\n",
            "25653 3\n",
            "25655 3\n",
            "25679 3\n",
            "25717 3\n",
            "25882 0\n",
            "25923 0\n",
            "25925 0\n",
            "26027 0\n",
            "26069 2\n",
            "26092 2\n",
            "26108 2\n",
            "26111 2\n",
            "26126 1\n",
            "26139 0\n",
            "26140 0\n",
            "26141 3\n",
            "26142 0\n",
            "26144 0\n",
            "26145 0\n",
            "26146 0\n",
            "26147 0\n",
            "26148 0\n",
            "26149 0\n",
            "26153 0\n",
            "26154 0\n",
            "26155 0\n",
            "26158 0\n",
            "26159 0\n",
            "26241 2\n",
            "26319 3\n",
            "26320 3\n",
            "26322 3\n",
            "26323 3\n",
            "26325 3\n",
            "26326 3\n",
            "26327 3\n",
            "26328 3\n",
            "26329 3\n",
            "26330 3\n",
            "26331 3\n",
            "26334 3\n",
            "26346 2\n",
            "26678 3\n",
            "26681 3\n",
            "26695 2\n",
            "26915 2\n",
            "26931 2\n",
            "26933 1\n",
            "26935 3\n",
            "26936 3\n",
            "26939 3\n",
            "26947 3\n",
            "26958 2\n",
            "26980 0\n",
            "26981 0\n",
            "26985 0\n",
            "27037 3\n",
            "27041 3\n",
            "27044 3\n",
            "27045 3\n",
            "27047 3\n",
            "27048 3\n",
            "27049 3\n",
            "27050 3\n",
            "27052 3\n",
            "27055 3\n",
            "27059 3\n",
            "27060 3\n",
            "27061 3\n",
            "27062 3\n",
            "27063 3\n",
            "27064 3\n",
            "27065 3\n",
            "27066 3\n",
            "27068 3\n",
            "27070 3\n",
            "27071 3\n",
            "27112 1\n",
            "27185 0\n",
            "27189 0\n",
            "27190 2\n",
            "27236 2\n",
            "27258 0\n",
            "27269 0\n",
            "27284 3\n",
            "27349 3\n",
            "27350 0\n",
            "27355 3\n",
            "27356 3\n",
            "27357 2\n",
            "27595 2\n",
            "27598 0\n",
            "27732 2\n",
            "27786 3\n",
            "27792 2\n",
            "27795 3\n",
            "27797 3\n",
            "27815 3\n",
            "27835 0\n",
            "27837 0\n",
            "27842 0\n",
            "27843 0\n",
            "27845 0\n",
            "27920 3\n",
            "27921 1\n",
            "28014 2\n",
            "28037 2\n",
            "28038 3\n",
            "28122 0\n",
            "28173 0\n",
            "28232 3\n",
            "28233 3\n",
            "28234 3\n",
            "28237 3\n",
            "28239 3\n",
            "28299 1\n",
            "28379 3\n",
            "28405 2\n",
            "28457 2\n",
            "28464 2\n",
            "28470 3\n",
            "28471 3\n",
            "28472 3\n",
            "28473 3\n",
            "28474 3\n",
            "28490 2\n",
            "28505 3\n",
            "28517 3\n",
            "28532 3\n",
            "28535 1\n",
            "28586 2\n",
            "28598 2\n",
            "28662 2\n",
            "28672 2\n",
            "28676 3\n",
            "28682 2\n",
            "28753 1\n",
            "28792 3\n",
            "28852 2\n",
            "28876 1\n",
            "28993 3\n",
            "29022 0\n",
            "29026 0\n",
            "29050 3\n",
            "29086 2\n",
            "29128 2\n",
            "29199 0\n",
            "29200 3\n",
            "29212 2\n",
            "29224 3\n",
            "29230 3\n",
            "29238 3\n",
            "29274 0\n",
            "29276 3\n",
            "29325 3\n",
            "29330 3\n",
            "29334 3\n",
            "29337 3\n",
            "29338 3\n",
            "29344 2\n",
            "29370 2\n",
            "29397 2\n",
            "29469 3\n",
            "29506 3\n",
            "29635 0\n",
            "29648 2\n",
            "29709 2\n",
            "29765 0\n",
            "29847 0\n",
            "29876 2\n",
            "30011 2\n",
            "30048 2\n",
            "30062 2\n",
            "30114 2\n",
            "30139 0\n",
            "30146 0\n",
            "30170 3\n",
            "30185 2\n",
            "30309 2\n",
            "30345 2\n",
            "30372 2\n",
            "30398 2\n",
            "30407 2\n",
            "30409 2\n",
            "30432 0\n",
            "30436 0\n",
            "30441 0\n",
            "30444 0\n",
            "30445 0\n",
            "30447 0\n",
            "30449 0\n",
            "30478 2\n",
            "30485 0\n",
            "30489 2\n",
            "30544 2\n",
            "30720 0\n",
            "30742 2\n",
            "30750 2\n",
            "30795 0\n",
            "30920 0\n",
            "31125 0\n",
            "31143 2\n",
            "31149 2\n",
            "31172 0\n",
            "31260 2\n",
            "31261 2\n",
            "31272 2\n",
            "31326 2\n",
            "31423 2\n",
            "31424 3\n",
            "31531 2\n",
            "31541 2\n",
            "31617 0\n",
            "31618 2\n",
            "31687 2\n",
            "31756 2\n",
            "31800 2\n",
            "31840 3\n",
            "31883 2\n",
            "31992 2\n",
            "32001 3\n",
            "32032 3\n",
            "32078 0\n",
            "32082 2\n",
            "32101 1\n",
            "32141 0\n",
            "32190 3\n",
            "32327 2\n",
            "32504 2\n",
            "32543 2\n",
            "32574 2\n",
            "32658 0\n",
            "32760 3\n",
            "32768 2\n",
            "32771 3\n",
            "32790 2\n",
            "32800 3\n",
            "32806 2\n",
            "32810 3\n",
            "32838 2\n",
            "32843 2\n",
            "32846 2\n",
            "32865 1\n",
            "32866 3\n",
            "32896 1\n",
            "32977 0\n",
            "33055 3\n",
            "33056 2\n",
            "33057 2\n",
            "33078 2\n",
            "33124 2\n",
            "33232 2\n",
            "33280 3\n",
            "33406 2\n",
            "33446 3\n",
            "33447 2\n",
            "33454 3\n",
            "33512 2\n",
            "33519 2\n",
            "33520 3\n",
            "33565 1\n",
            "33592 2\n",
            "33685 3\n",
            "33737 2\n",
            "33802 2\n",
            "34137 2\n",
            "34142 3\n",
            "34149 2\n",
            "34167 2\n",
            "34447 3\n",
            "34574 2\n",
            "34585 3\n",
            "34586 2\n",
            "34588 3\n",
            "34590 3\n",
            "34591 2\n",
            "34630 3\n",
            "34683 2\n",
            "34738 2\n",
            "34762 0\n",
            "34763 2\n",
            "34770 2\n",
            "34786 3\n",
            "34800 0\n",
            "34900 2\n",
            "34950 2\n",
            "34963 0\n",
            "35096 0\n",
            "35316 2\n",
            "35364 3\n",
            "35470 2\n",
            "35685 3\n",
            "35713 2\n",
            "35715 2\n",
            "35772 2\n",
            "35779 2\n",
            "35804 0\n",
            "35807 0\n",
            "35808 0\n",
            "35809 2\n",
            "35926 3\n",
            "35984 2\n",
            "36010 0\n",
            "36056 3\n",
            "36125 2\n",
            "36212 2\n",
            "36281 3\n",
            "36386 2\n",
            "36453 0\n",
            "36488 0\n",
            "36608 3\n",
            "36695 2\n",
            "36727 3\n",
            "36728 3\n",
            "36795 3\n",
            "36831 0\n",
            "36839 2\n",
            "36856 2\n",
            "36913 2\n",
            "37052 2\n",
            "37121 3\n",
            "37122 2\n",
            "37123 2\n",
            "37126 2\n",
            "37131 2\n",
            "37132 1\n",
            "37133 2\n",
            "37134 2\n",
            "37135 3\n",
            "37137 1\n",
            "37143 1\n",
            "37144 3\n",
            "37145 3\n",
            "37146 3\n",
            "37147 2\n",
            "37148 2\n",
            "37149 2\n",
            "37150 1\n",
            "37225 2\n",
            "37255 3\n",
            "37301 3\n",
            "37303 3\n",
            "37304 1\n",
            "37305 1\n",
            "37342 1\n",
            "37343 2\n",
            "37359 3\n",
            "37387 2\n",
            "37388 1\n",
            "37391 3\n",
            "37394 3\n",
            "37395 1\n",
            "37396 3\n",
            "37397 3\n",
            "37402 3\n",
            "37407 2\n",
            "37409 2\n",
            "37417 2\n",
            "37460 1\n",
            "37469 2\n",
            "37473 2\n",
            "37474 2\n",
            "37475 3\n",
            "37476 2\n",
            "37477 2\n",
            "37479 3\n",
            "37495 2\n",
            "37497 1\n",
            "37500 3\n",
            "37506 0\n",
            "37524 3\n",
            "37535 0\n",
            "37540 3\n",
            "37555 3\n",
            "37556 2\n",
            "37558 2\n",
            "37561 3\n",
            "37562 2\n",
            "37564 0\n",
            "37565 3\n",
            "37566 0\n",
            "37567 3\n",
            "37568 3\n",
            "37569 1\n",
            "37575 2\n",
            "37584 3\n",
            "37664 2\n",
            "37671 2\n",
            "37673 3\n",
            "37675 3\n",
            "37677 3\n",
            "37678 2\n",
            "37679 0\n",
            "37680 2\n",
            "37681 3\n",
            "37687 2\n",
            "37747 1\n",
            "37771 3\n",
            "37772 2\n",
            "37774 2\n",
            "37775 3\n",
            "37828 1\n",
            "37829 1\n",
            "37848 2\n",
            "37849 1\n",
            "37851 1\n",
            "37855 2\n",
            "37857 2\n",
            "37929 1\n",
            "37934 2\n",
            "37981 2\n",
            "38020 3\n",
            "38022 3\n",
            "38024 3\n",
            "38025 3\n",
            "38026 3\n",
            "38027 3\n",
            "38033 0\n",
            "38036 3\n",
            "38044 3\n",
            "38047 1\n",
            "38063 2\n",
            "38074 3\n",
            "38075 1\n",
            "38089 2\n",
            "38091 1\n",
            "38093 2\n",
            "38094 2\n",
            "38095 3\n",
            "38096 2\n",
            "38102 3\n",
            "38103 0\n",
            "38106 3\n",
            "38107 1\n",
            "38122 1\n",
            "38130 1\n",
            "38195 3\n",
            "38209 2\n",
            "38228 3\n",
            "38262 1\n",
            "38266 1\n",
            "38315 2\n",
            "38319 3\n",
            "38320 3\n",
            "38321 0\n",
            "38329 2\n",
            "38351 2\n",
            "38352 1\n",
            "38387 2\n",
            "38389 2\n",
            "38406 2\n",
            "38409 2\n",
            "38410 3\n",
            "38439 3\n",
            "38442 1\n",
            "38498 3\n",
            "38499 3\n",
            "38500 3\n",
            "38528 2\n",
            "38637 2\n",
            "38638 2\n",
            "38640 2\n",
            "38643 1\n",
            "38644 1\n",
            "38645 2\n",
            "38652 2\n",
            "38654 1\n",
            "38657 2\n",
            "38658 1\n",
            "38659 1\n",
            "38660 2\n",
            "38661 2\n",
            "38662 1\n",
            "38665 3\n",
            "38666 2\n",
            "38726 3\n",
            "38768 0\n",
            "38811 2\n",
            "38825 2\n",
            "38827 2\n",
            "38852 3\n",
            "38864 1\n",
            "38865 1\n",
            "38948 1\n",
            "38949 1\n",
            "38950 2\n",
            "38951 1\n",
            "38952 2\n",
            "38961 3\n",
            "38970 3\n",
            "38996 2\n",
            "39002 3\n",
            "39004 2\n",
            "39074 2\n",
            "39078 1\n",
            "39080 2\n",
            "39081 3\n",
            "39115 3\n",
            "39138 2\n",
            "39143 2\n",
            "39144 3\n",
            "39145 1\n",
            "39146 1\n",
            "39153 2\n",
            "39154 3\n",
            "39156 2\n",
            "39158 2\n",
            "39159 1\n",
            "39165 2\n",
            "39167 2\n",
            "39169 2\n",
            "39174 3\n",
            "39196 3\n",
            "39197 3\n",
            "39200 2\n",
            "39231 1\n",
            "39264 1\n",
            "39307 2\n",
            "39308 2\n",
            "39310 2\n",
            "39311 1\n",
            "39314 3\n",
            "39315 2\n",
            "39339 3\n",
            "39442 3\n",
            "39466 1\n",
            "39497 2\n",
            "39503 3\n",
            "355 3\n",
            "357 3\n",
            "725 2\n",
            "1915 2\n",
            "3784 3\n",
            "3837 3\n",
            "3845 2\n",
            "3971 3\n",
            "4285 0\n",
            "4342 2\n",
            "4345 1\n",
            "4346 3\n",
            "4406 3\n",
            "4529 0\n",
            "4554 0\n",
            "4557 0\n",
            "4687 0\n",
            "4804 1\n",
            "4810 0\n",
            "4853 2\n",
            "4932 3\n",
            "4935 3\n",
            "4966 3\n",
            "4967 3\n",
            "4972 2\n",
            "5368 2\n",
            "5370 3\n",
            "5371 2\n",
            "5373 1\n",
            "5430 0\n",
            "5609 2\n",
            "5618 2\n",
            "11557 2\n",
            "15976 0\n",
            "15977 2\n",
            "22522 2\n",
            "22991 3\n",
            "22996 3\n",
            "22997 3\n",
            "23010 3\n",
            "23229 1\n",
            "23765 3\n",
            "23766 3\n",
            "25161 2\n",
            "25162 2\n",
            "25709 2\n",
            "27934 3\n",
            "29519 3\n",
            "30733 0\n",
            "30775 0\n",
            "31450 2\n",
            "31530 2\n",
            "31555 2\n",
            "31610 2\n",
            "31716 2\n",
            "33647 1\n",
            "33648 3\n",
            "33860 1\n",
            "33881 2\n",
            "34436 0\n",
            "34437 2\n",
            "34560 2\n",
            "34561 2\n",
            "34746 2\n",
            "35112 2\n",
            "35387 2\n",
            "37063 1\n",
            "37084 1\n",
            "37226 2\n",
            "37279 3\n",
            "37319 2\n",
            "37969 3\n",
            "37970 3\n",
            "38029 3\n",
            "38241 3\n",
            "38345 3\n",
            "38346 3\n",
            "38348 3\n",
            "38475 3\n",
            "38476 3\n",
            "38540 1\n",
            "38545 3\n",
            "38548 3\n",
            "38550 1\n",
            "38551 3\n",
            "38552 3\n",
            "38553 3\n",
            "38555 3\n",
            "38562 3\n",
            "38564 3\n",
            "38567 3\n",
            "38571 3\n",
            "38574 0\n",
            "38578 1\n",
            "38579 1\n",
            "38580 1\n",
            "38585 3\n",
            "38587 3\n",
            "38593 3\n",
            "38598 3\n",
            "38720 2\n",
            "38891 0\n",
            "39248 3\n",
            "39250 3\n",
            "39336 3\n",
            "39342 3\n",
            "39487 1\n",
            "381 3\n",
            "385 3\n",
            "834 3\n",
            "917 3\n",
            "1006 3\n",
            "1145 3\n",
            "1423 2\n",
            "1498 3\n",
            "1538 2\n",
            "1561 3\n",
            "1631 2\n",
            "1736 0\n",
            "1799 2\n",
            "1845 0\n",
            "1846 3\n",
            "2306 2\n",
            "2370 1\n",
            "2438 3\n",
            "2634 0\n",
            "3368 3\n",
            "3768 3\n",
            "4217 2\n",
            "4521 2\n",
            "4716 3\n",
            "4851 2\n",
            "5198 2\n",
            "5957 2\n",
            "6158 2\n",
            "6457 0\n",
            "6461 0\n",
            "6658 0\n",
            "8243 0\n",
            "8632 3\n",
            "8789 0\n",
            "9132 2\n",
            "9133 3\n",
            "9181 0\n",
            "9183 0\n",
            "9186 0\n",
            "9362 0\n",
            "9367 0\n",
            "9471 0\n",
            "10093 2\n",
            "10506 2\n",
            "11911 0\n",
            "12005 0\n",
            "12008 0\n",
            "12021 0\n",
            "12603 0\n",
            "14244 0\n",
            "14506 0\n",
            "15274 0\n",
            "15276 3\n",
            "15706 0\n",
            "16270 0\n",
            "16273 0\n",
            "16275 3\n",
            "17214 2\n",
            "18275 0\n",
            "19794 3\n",
            "19799 3\n",
            "20963 0\n",
            "20964 3\n",
            "21022 3\n",
            "21023 3\n",
            "21026 3\n",
            "21081 2\n",
            "21084 2\n",
            "21941 3\n",
            "22200 2\n",
            "22804 3\n",
            "22805 3\n",
            "22806 0\n",
            "23253 3\n",
            "23255 3\n",
            "23256 3\n",
            "23459 3\n",
            "23682 3\n",
            "23913 3\n",
            "23943 3\n",
            "24312 3\n",
            "24612 0\n",
            "24613 3\n",
            "24614 3\n",
            "24615 3\n",
            "24616 3\n",
            "24618 3\n",
            "24620 3\n",
            "25148 3\n",
            "25149 3\n",
            "25150 3\n",
            "25382 3\n",
            "25702 3\n",
            "25703 3\n",
            "25704 3\n",
            "25778 3\n",
            "25779 3\n",
            "25780 3\n",
            "25781 3\n",
            "25782 3\n",
            "25945 3\n",
            "25958 3\n",
            "25960 3\n",
            "25969 3\n",
            "25970 3\n",
            "26628 3\n",
            "26916 3\n",
            "27082 3\n",
            "27341 3\n",
            "27342 3\n",
            "27343 3\n",
            "27344 3\n",
            "27345 3\n",
            "27346 3\n",
            "27437 3\n",
            "27883 3\n",
            "28821 3\n",
            "28822 3\n",
            "28823 3\n",
            "28824 3\n",
            "28825 3\n",
            "28826 3\n",
            "28827 3\n",
            "28828 3\n",
            "28830 3\n",
            "28834 3\n",
            "28835 3\n",
            "28836 3\n",
            "29380 3\n",
            "29752 2\n",
            "30190 3\n",
            "30368 3\n",
            "30392 2\n",
            "30420 3\n",
            "30465 0\n",
            "30467 0\n",
            "30628 2\n",
            "30685 3\n",
            "30872 0\n",
            "30893 3\n",
            "30980 0\n",
            "31288 3\n",
            "31400 0\n",
            "31462 1\n",
            "31732 3\n",
            "31819 2\n",
            "31830 3\n",
            "31953 3\n",
            "31980 0\n",
            "32181 3\n",
            "32309 2\n",
            "32542 3\n",
            "32549 3\n",
            "32927 0\n",
            "33317 2\n",
            "33539 3\n",
            "34751 3\n",
            "35709 3\n",
            "35905 3\n",
            "36233 3\n",
            "37072 3\n",
            "37073 3\n",
            "37441 3\n",
            "38253 2\n",
            "38817 2\n",
            "38830 3\n",
            "38831 3\n",
            "38832 3\n",
            "39467 3\n",
            "39468 3\n",
            "39486 3\n",
            "566 1\n",
            "1117 2\n",
            "2040 2\n",
            "2066 2\n",
            "2200 2\n",
            "2880 1\n",
            "3374 2\n",
            "3651 2\n",
            "4136 0\n",
            "4229 3\n",
            "4672 0\n",
            "6942 2\n",
            "6945 2\n",
            "8832 0\n",
            "9768 3\n",
            "10875 2\n",
            "12428 2\n",
            "15675 0\n",
            "17626 2\n",
            "17775 0\n",
            "18027 3\n",
            "18142 0\n",
            "18669 0\n",
            "18797 0\n",
            "21093 3\n",
            "21094 3\n",
            "21184 3\n",
            "21185 3\n",
            "21238 3\n",
            "21243 3\n",
            "21248 3\n",
            "21518 3\n",
            "21519 3\n",
            "22233 2\n",
            "22348 2\n",
            "22487 3\n",
            "22488 3\n",
            "22704 3\n",
            "22705 1\n",
            "22706 3\n",
            "22707 1\n",
            "22708 3\n",
            "22710 3\n",
            "22711 3\n",
            "22712 1\n",
            "22713 3\n",
            "22714 3\n",
            "23254 3\n",
            "23680 3\n",
            "24052 3\n",
            "24054 3\n",
            "24055 2\n",
            "24056 3\n",
            "24059 2\n",
            "24061 2\n",
            "24066 2\n",
            "24269 3\n",
            "24270 3\n",
            "24271 3\n",
            "24272 3\n",
            "24273 2\n",
            "24379 3\n",
            "24497 3\n",
            "24498 3\n",
            "24499 3\n",
            "24621 3\n",
            "24651 3\n",
            "24674 3\n",
            "25245 3\n",
            "25441 2\n",
            "25443 2\n",
            "25714 2\n",
            "25715 3\n",
            "25716 3\n",
            "25721 2\n",
            "25992 3\n",
            "26021 3\n",
            "26023 3\n",
            "26024 3\n",
            "26025 3\n",
            "26375 3\n",
            "26764 3\n",
            "26765 3\n",
            "27500 3\n",
            "28414 2\n",
            "29219 3\n",
            "29842 3\n",
            "29905 2\n",
            "29911 2\n",
            "30063 2\n",
            "30548 2\n",
            "30566 2\n",
            "30646 2\n",
            "30647 2\n",
            "31029 2\n",
            "31050 2\n",
            "31114 2\n",
            "31472 2\n",
            "31667 2\n",
            "31669 2\n",
            "31685 3\n",
            "31723 0\n",
            "32056 2\n",
            "32307 2\n",
            "32428 2\n",
            "32632 2\n",
            "32801 2\n",
            "32816 2\n",
            "32881 2\n",
            "33921 2\n",
            "33922 2\n",
            "34047 2\n",
            "34346 2\n",
            "35114 2\n",
            "35208 2\n",
            "35321 2\n",
            "35340 2\n",
            "35341 2\n",
            "35355 2\n",
            "35442 2\n",
            "35448 2\n",
            "37217 3\n",
            "37219 0\n",
            "37340 0\n",
            "37543 2\n",
            "37547 3\n",
            "37810 2\n",
            "38041 0\n",
            "38667 3\n",
            "38829 3\n",
            "39351 0\n",
            "39490 3\n",
            "39491 1\n",
            "804 2\n",
            "980 2\n",
            "1085 2\n",
            "1086 3\n",
            "1115 2\n",
            "1524 3\n",
            "1743 2\n",
            "2205 2\n",
            "2255 2\n",
            "2256 3\n",
            "2257 3\n",
            "2650 2\n",
            "2762 2\n",
            "4720 2\n",
            "5986 2\n",
            "6219 3\n",
            "6251 2\n",
            "6262 2\n",
            "6532 2\n",
            "7520 2\n",
            "16434 2\n",
            "16978 3\n",
            "17125 3\n",
            "17126 2\n",
            "17378 2\n",
            "17385 3\n",
            "17470 2\n",
            "17769 2\n",
            "17781 2\n",
            "18023 2\n",
            "18024 2\n",
            "18288 2\n",
            "18369 3\n",
            "18700 2\n",
            "19301 2\n",
            "19666 2\n",
            "21341 1\n",
            "26924 2\n",
            "27623 3\n",
            "29256 3\n",
            "29518 3\n",
            "29606 2\n",
            "29771 2\n",
            "29822 2\n",
            "29823 2\n",
            "29824 2\n",
            "29832 2\n",
            "29839 2\n",
            "29851 2\n",
            "30127 2\n",
            "30129 2\n",
            "30204 3\n",
            "30229 2\n",
            "30280 2\n",
            "30307 2\n",
            "30390 2\n",
            "30452 2\n",
            "30469 2\n",
            "30670 2\n",
            "30707 2\n",
            "30815 2\n",
            "30825 2\n",
            "30838 2\n",
            "30844 2\n",
            "30848 2\n",
            "30850 2\n",
            "30852 2\n",
            "31010 2\n",
            "31394 2\n",
            "31407 2\n",
            "31568 0\n",
            "31606 2\n",
            "31701 2\n",
            "31745 2\n",
            "31868 2\n",
            "31869 2\n",
            "32005 2\n",
            "32029 2\n",
            "32153 2\n",
            "32204 2\n",
            "32210 2\n",
            "32217 2\n",
            "32259 2\n",
            "32260 2\n",
            "32289 2\n",
            "32290 2\n",
            "32352 3\n",
            "32441 2\n",
            "32448 2\n",
            "32499 2\n",
            "32561 2\n",
            "32669 2\n",
            "32809 2\n",
            "32811 2\n",
            "32852 2\n",
            "32945 2\n",
            "32954 2\n",
            "33107 2\n",
            "33316 2\n",
            "33501 2\n",
            "33502 2\n",
            "33505 2\n",
            "33506 2\n",
            "33517 2\n",
            "33587 2\n",
            "33706 2\n",
            "33780 2\n",
            "33796 2\n",
            "33797 2\n",
            "33863 2\n",
            "33864 2\n",
            "33865 2\n",
            "33941 2\n",
            "33969 2\n",
            "34046 2\n",
            "34049 2\n",
            "34051 2\n",
            "34085 2\n",
            "34593 2\n",
            "34598 2\n",
            "34629 2\n",
            "34678 2\n",
            "34680 2\n",
            "34829 2\n",
            "34850 2\n",
            "34851 2\n",
            "34931 2\n",
            "35108 2\n",
            "35171 2\n",
            "35219 2\n",
            "35220 2\n",
            "35555 2\n",
            "35594 2\n",
            "35788 0\n",
            "35849 2\n",
            "36277 2\n",
            "36321 2\n",
            "37331 3\n",
            "37344 3\n",
            "37349 3\n",
            "37610 3\n",
            "37910 3\n",
            "37914 3\n",
            "37915 3\n",
            "37997 1\n",
            "38339 2\n",
            "38413 3\n",
            "38747 3\n",
            "38847 3\n",
            "38892 2\n",
            "38895 2\n",
            "38896 3\n",
            "38898 3\n",
            "38899 2\n",
            "39083 3\n",
            "1098 2\n",
            "1142 2\n",
            "3347 3\n",
            "3753 3\n",
            "3897 2\n",
            "3902 2\n",
            "4145 2\n",
            "4263 3\n",
            "4281 3\n",
            "4630 2\n",
            "4899 2\n",
            "5282 3\n",
            "5444 1\n",
            "6185 3\n",
            "6589 0\n",
            "8973 0\n",
            "16412 2\n",
            "18887 2\n",
            "19832 2\n",
            "21851 3\n",
            "21852 3\n",
            "21853 3\n",
            "21918 3\n",
            "23248 3\n",
            "23916 3\n",
            "24321 3\n",
            "24325 0\n",
            "24370 0\n",
            "24372 3\n",
            "25108 0\n",
            "25109 3\n",
            "25111 3\n",
            "25621 3\n",
            "25713 3\n",
            "26001 3\n",
            "26004 3\n",
            "27560 2\n",
            "27561 0\n",
            "27564 3\n",
            "27570 0\n",
            "27571 3\n",
            "28318 3\n",
            "28518 3\n",
            "28584 3\n",
            "28846 0\n",
            "28847 0\n",
            "28850 0\n",
            "28854 2\n",
            "28862 3\n",
            "28875 3\n",
            "28877 2\n",
            "28879 3\n",
            "29099 3\n",
            "29101 3\n",
            "29228 0\n",
            "29340 3\n",
            "29422 3\n",
            "29470 3\n",
            "29471 3\n",
            "29474 3\n",
            "29481 3\n",
            "29483 3\n",
            "29523 3\n",
            "29630 2\n",
            "30645 2\n",
            "30730 2\n",
            "30811 2\n",
            "31138 2\n",
            "31490 2\n",
            "31658 2\n",
            "32252 3\n",
            "32328 2\n",
            "32596 2\n",
            "32613 2\n",
            "33062 2\n",
            "33356 2\n",
            "33818 2\n",
            "34551 2\n",
            "34564 2\n",
            "36120 2\n",
            "36611 2\n",
            "37138 3\n",
            "37139 2\n",
            "37406 3\n",
            "37516 2\n",
            "37525 1\n",
            "37527 3\n",
            "37530 3\n",
            "37531 1\n",
            "37534 1\n",
            "37537 0\n",
            "37538 3\n",
            "37539 3\n",
            "37541 3\n",
            "37553 3\n",
            "38031 2\n",
            "38083 3\n",
            "38084 3\n",
            "38250 1\n",
            "38265 2\n",
            "38407 3\n",
            "38460 0\n",
            "38715 2\n",
            "38717 1\n",
            "38735 3\n",
            "38968 3\n",
            "38973 3\n",
            "39024 2\n",
            "39034 3\n",
            "39108 3\n",
            "39126 2\n",
            "39131 3\n",
            "39132 3\n",
            "39155 1\n",
            "39447 3\n",
            "39448 0\n",
            "39449 0\n",
            "39451 3\n",
            "39452 3\n",
            "39453 1\n",
            "39454 3\n",
            "39496 1\n",
            "39498 3\n",
            "1404 2\n",
            "1966 0\n",
            "2216 3\n",
            "2469 0\n",
            "2729 2\n",
            "2730 3\n",
            "2943 2\n",
            "5511 3\n",
            "6264 2\n",
            "6282 0\n",
            "6374 0\n",
            "8209 3\n",
            "11456 0\n",
            "11458 0\n",
            "13367 0\n",
            "13948 3\n",
            "15774 2\n",
            "16098 2\n",
            "16134 0\n",
            "16792 2\n",
            "17307 0\n",
            "17325 3\n",
            "17379 3\n",
            "17381 2\n",
            "17382 1\n",
            "17383 1\n",
            "17384 1\n",
            "17388 3\n",
            "17418 0\n",
            "17583 0\n",
            "17926 0\n",
            "17983 3\n",
            "18652 3\n",
            "19487 1\n",
            "21085 1\n",
            "21298 3\n",
            "21332 3\n",
            "21334 1\n",
            "21335 1\n",
            "21336 1\n",
            "21342 1\n",
            "21345 3\n",
            "21348 3\n",
            "21350 1\n",
            "21354 3\n",
            "21355 3\n",
            "21362 3\n",
            "21363 1\n",
            "21367 3\n",
            "21373 3\n",
            "21376 1\n",
            "21377 1\n",
            "21379 3\n",
            "21380 1\n",
            "21383 3\n",
            "21384 3\n",
            "21385 1\n",
            "21388 3\n",
            "21475 3\n",
            "21476 1\n",
            "21478 3\n",
            "21479 3\n",
            "21481 1\n",
            "21636 3\n",
            "21640 3\n",
            "21642 3\n",
            "21643 1\n",
            "21801 3\n",
            "21805 3\n",
            "21807 1\n",
            "21808 3\n",
            "21814 1\n",
            "22017 3\n",
            "22021 1\n",
            "22027 3\n",
            "22037 3\n",
            "22133 2\n",
            "22174 3\n",
            "22208 3\n",
            "22516 3\n",
            "22647 3\n",
            "22653 3\n",
            "22719 3\n",
            "22725 3\n",
            "22728 3\n",
            "22763 3\n",
            "22880 1\n",
            "22899 3\n",
            "22903 3\n",
            "22944 3\n",
            "22976 1\n",
            "22979 3\n",
            "23502 1\n",
            "23503 3\n",
            "23506 3\n",
            "23697 3\n",
            "23707 3\n",
            "23917 3\n",
            "24138 1\n",
            "24189 3\n",
            "24377 1\n",
            "24483 3\n",
            "24643 3\n",
            "24669 1\n",
            "24670 1\n",
            "25086 3\n",
            "25087 3\n",
            "25089 3\n",
            "25090 3\n",
            "25160 3\n",
            "25346 1\n",
            "25347 3\n",
            "25458 3\n",
            "25544 3\n",
            "25644 2\n",
            "25645 1\n",
            "25675 3\n",
            "25678 3\n",
            "26240 1\n",
            "26640 3\n",
            "27950 3\n",
            "28079 3\n",
            "28591 3\n",
            "28592 3\n",
            "28593 3\n",
            "29361 3\n",
            "29651 3\n",
            "29834 0\n",
            "29875 0\n",
            "29877 0\n",
            "29879 0\n",
            "29987 1\n",
            "29988 0\n",
            "30015 0\n",
            "30057 0\n",
            "30140 0\n",
            "30142 0\n",
            "30172 0\n",
            "30274 0\n",
            "30285 0\n",
            "30349 0\n",
            "30351 1\n",
            "30528 0\n",
            "30753 3\n",
            "30824 3\n",
            "30985 2\n",
            "31483 0\n",
            "31499 0\n",
            "31551 0\n",
            "31553 0\n",
            "31693 2\n",
            "31920 0\n",
            "32081 0\n",
            "32083 0\n",
            "32085 0\n",
            "32097 2\n",
            "32135 0\n",
            "32200 0\n",
            "32383 0\n",
            "32396 3\n",
            "32423 0\n",
            "32729 0\n",
            "32815 1\n",
            "32907 0\n",
            "32918 0\n",
            "32972 0\n",
            "32975 0\n",
            "33070 0\n",
            "33246 0\n",
            "33321 0\n",
            "33322 0\n",
            "33323 0\n",
            "33327 0\n",
            "33600 0\n",
            "33653 0\n",
            "33758 0\n",
            "33805 2\n",
            "33832 0\n",
            "34188 0\n",
            "34230 1\n",
            "34277 0\n",
            "34278 0\n",
            "34311 0\n",
            "34313 0\n",
            "34460 0\n",
            "34464 0\n",
            "34469 0\n",
            "34477 1\n",
            "34534 0\n",
            "34555 2\n",
            "34558 3\n",
            "34739 0\n",
            "35152 0\n",
            "35445 0\n",
            "35767 3\n",
            "35791 0\n",
            "35903 0\n",
            "36682 0\n",
            "37384 3\n",
            "37571 1\n",
            "37993 1\n",
            "37994 3\n",
            "37996 1\n",
            "37998 1\n",
            "37999 1\n",
            "38000 1\n",
            "38001 1\n",
            "38003 3\n",
            "38005 3\n",
            "38382 1\n",
            "38391 3\n",
            "38444 3\n",
            "38516 1\n",
            "39329 0\n",
            "39374 3\n",
            "3305 1\n",
            "3330 3\n",
            "4402 3\n",
            "4403 3\n",
            "4404 3\n",
            "4407 3\n",
            "4409 3\n",
            "4421 3\n",
            "4422 3\n",
            "4956 0\n",
            "5309 3\n",
            "5318 3\n",
            "5321 3\n",
            "5323 3\n",
            "5324 3\n",
            "5326 3\n",
            "5564 0\n",
            "5565 0\n",
            "5939 3\n",
            "5948 3\n",
            "5949 3\n",
            "6355 3\n",
            "6484 0\n",
            "6610 0\n",
            "8964 0\n",
            "14968 0\n",
            "15030 0\n",
            "15303 0\n",
            "15304 0\n",
            "15307 0\n",
            "15308 0\n",
            "15309 0\n",
            "15637 0\n",
            "15685 0\n",
            "15686 0\n",
            "15699 0\n",
            "16080 0\n",
            "16160 0\n",
            "16485 0\n",
            "16590 0\n",
            "16593 0\n",
            "17563 0\n",
            "17567 0\n",
            "18017 0\n",
            "18388 0\n",
            "18660 0\n",
            "19137 0\n",
            "19168 0\n",
            "19172 0\n",
            "19174 2\n",
            "19263 0\n",
            "19295 0\n",
            "19297 0\n",
            "19516 0\n",
            "19806 0\n",
            "19808 0\n",
            "19809 0\n",
            "19823 0\n",
            "19824 0\n",
            "20073 0\n",
            "20195 0\n",
            "20210 0\n",
            "23442 2\n",
            "23448 2\n",
            "23451 2\n",
            "23452 2\n",
            "23453 3\n",
            "24581 2\n",
            "25132 2\n",
            "25177 0\n",
            "25863 0\n",
            "28391 2\n",
            "29971 3\n",
            "30227 0\n",
            "30404 0\n",
            "31413 3\n",
            "31958 2\n",
            "31974 2\n",
            "31976 2\n",
            "32338 0\n",
            "32505 2\n",
            "33298 0\n",
            "33574 0\n",
            "33580 0\n",
            "33749 0\n",
            "33861 0\n",
            "34074 0\n",
            "34097 0\n",
            "34098 0\n",
            "35641 0\n",
            "35749 2\n",
            "35977 0\n",
            "37748 1\n",
            "3377 4\n",
            "3381 4\n",
            "3382 4\n",
            "3383 4\n",
            "3384 4\n",
            "3385 4\n",
            "3554 4\n",
            "4352 3\n",
            "4399 4\n",
            "4500 4\n",
            "4501 4\n",
            "4507 4\n",
            "4599 4\n",
            "4600 4\n",
            "4601 4\n",
            "4654 4\n",
            "4844 4\n",
            "4845 4\n",
            "4846 4\n",
            "4847 4\n",
            "4848 4\n",
            "4869 4\n",
            "4941 4\n",
            "4942 4\n",
            "4991 4\n",
            "5013 4\n",
            "5023 4\n",
            "5024 4\n",
            "5025 4\n",
            "5027 4\n",
            "5040 4\n",
            "5041 4\n",
            "5053 4\n",
            "5055 4\n",
            "5056 4\n",
            "5057 4\n",
            "5058 4\n",
            "5060 4\n",
            "5061 4\n",
            "5073 4\n",
            "5074 4\n",
            "5076 4\n",
            "5135 4\n",
            "5136 4\n",
            "5137 4\n",
            "5151 4\n",
            "5152 4\n",
            "5157 4\n",
            "5158 4\n",
            "5159 4\n",
            "5160 4\n",
            "5161 4\n",
            "5162 4\n",
            "5163 4\n",
            "5233 4\n",
            "5286 4\n",
            "5344 4\n",
            "5354 4\n",
            "5360 4\n",
            "5361 4\n",
            "5362 4\n",
            "5363 4\n",
            "5364 4\n",
            "5411 4\n",
            "5412 4\n",
            "5413 4\n",
            "5415 4\n",
            "5416 4\n",
            "5583 4\n",
            "5584 4\n",
            "5585 4\n",
            "5586 4\n",
            "5587 4\n",
            "5588 4\n",
            "5589 4\n",
            "5590 4\n",
            "5593 4\n",
            "5624 4\n",
            "5625 4\n",
            "5626 4\n",
            "5678 4\n",
            "5680 4\n",
            "5681 4\n",
            "5682 4\n",
            "5683 4\n",
            "5684 4\n",
            "5685 4\n",
            "5690 4\n",
            "5692 4\n",
            "5693 4\n",
            "5715 4\n",
            "5716 4\n",
            "5717 4\n",
            "5718 4\n",
            "5777 4\n",
            "5885 4\n",
            "5886 4\n",
            "5887 4\n",
            "5889 4\n",
            "5890 4\n",
            "5891 4\n",
            "5892 4\n",
            "5893 4\n",
            "5895 4\n",
            "5896 4\n",
            "5897 4\n",
            "5898 4\n",
            "5899 4\n",
            "5900 4\n",
            "5901 4\n",
            "5918 4\n",
            "5994 4\n",
            "6134 4\n",
            "6220 4\n",
            "6221 4\n",
            "6245 4\n",
            "6259 4\n",
            "6260 4\n",
            "6261 4\n",
            "6316 4\n",
            "6317 4\n",
            "6321 4\n",
            "10922 4\n",
            "10925 0\n",
            "11763 0\n",
            "12035 0\n",
            "12122 1\n",
            "12123 1\n",
            "12385 1\n",
            "12478 0\n",
            "13130 1\n",
            "13422 4\n",
            "15384 4\n",
            "16014 4\n",
            "16015 1\n",
            "16016 1\n",
            "16017 4\n",
            "16028 4\n",
            "16597 0\n",
            "16603 0\n",
            "16795 4\n",
            "16895 0\n",
            "16911 0\n",
            "16912 3\n",
            "16913 0\n",
            "16914 0\n",
            "17016 4\n",
            "17018 4\n",
            "17019 4\n",
            "17020 4\n",
            "17022 4\n",
            "17776 4\n",
            "17930 0\n",
            "17931 0\n",
            "18242 0\n",
            "18243 4\n",
            "18465 1\n",
            "18602 4\n",
            "19368 4\n",
            "19565 4\n",
            "20687 4\n",
            "20688 4\n",
            "20695 0\n",
            "20696 1\n",
            "21118 4\n",
            "21324 4\n",
            "22006 4\n",
            "23897 4\n",
            "23910 4\n",
            "24466 4\n",
            "24505 4\n",
            "24963 3\n",
            "25513 4\n",
            "25521 4\n",
            "25700 4\n",
            "25729 4\n",
            "26190 4\n",
            "26702 4\n",
            "27023 4\n",
            "27677 4\n",
            "27678 4\n",
            "27847 4\n",
            "27852 4\n",
            "27859 4\n",
            "27901 4\n",
            "27903 4\n",
            "28004 4\n",
            "28005 4\n",
            "28006 4\n",
            "28129 4\n",
            "28392 3\n",
            "28659 4\n",
            "28790 4\n",
            "28791 4\n",
            "29537 4\n",
            "29556 4\n",
            "29658 4\n",
            "29675 4\n",
            "29685 4\n",
            "29707 4\n",
            "29754 4\n",
            "32126 4\n",
            "32407 4\n",
            "32556 4\n",
            "32557 4\n",
            "32558 4\n",
            "32559 4\n",
            "33992 1\n",
            "33993 1\n",
            "34042 0\n",
            "35052 1\n",
            "35053 4\n",
            "37282 4\n",
            "37760 4\n",
            "37908 3\n",
            "37909 3\n",
            "37911 3\n",
            "37912 1\n",
            "37913 3\n",
            "5568 0\n",
            "5570 3\n",
            "7186 2\n",
            "7262 2\n",
            "21004 2\n",
            "21078 3\n",
            "21271 1\n",
            "21426 2\n",
            "21439 3\n",
            "21565 2\n",
            "21567 3\n",
            "21568 2\n",
            "21580 3\n",
            "21674 3\n",
            "21681 2\n",
            "21700 2\n",
            "21713 3\n",
            "21715 2\n",
            "21717 2\n",
            "21741 2\n",
            "21747 3\n",
            "21748 3\n",
            "21831 0\n",
            "21832 2\n",
            "21876 3\n",
            "21929 0\n",
            "21930 2\n",
            "21962 3\n",
            "21969 2\n",
            "22014 2\n",
            "22054 2\n",
            "22073 3\n",
            "22221 2\n",
            "22264 2\n",
            "22317 2\n",
            "22318 2\n",
            "22403 0\n",
            "22426 2\n",
            "22483 2\n",
            "22549 2\n",
            "22639 2\n",
            "22689 3\n",
            "22744 2\n",
            "22780 3\n",
            "22789 3\n",
            "22832 1\n",
            "22896 0\n",
            "22966 2\n",
            "22969 1\n",
            "23116 3\n",
            "23118 3\n",
            "23174 2\n",
            "23200 3\n",
            "23206 1\n",
            "23317 2\n",
            "23412 3\n",
            "23449 3\n",
            "23606 0\n",
            "23724 3\n",
            "23841 3\n",
            "24058 2\n",
            "24060 3\n",
            "24067 0\n",
            "24077 2\n",
            "24090 2\n",
            "24091 1\n",
            "24103 2\n",
            "24117 3\n",
            "24119 3\n",
            "24206 2\n",
            "24207 2\n",
            "24298 2\n",
            "24404 0\n",
            "24432 3\n",
            "24449 2\n",
            "24485 1\n",
            "24660 2\n",
            "24782 3\n",
            "24866 2\n",
            "24868 2\n",
            "24870 2\n",
            "24923 0\n",
            "25025 2\n",
            "25054 3\n",
            "25064 2\n",
            "25124 2\n",
            "25157 2\n",
            "25327 3\n",
            "25444 3\n",
            "25496 0\n",
            "25620 2\n",
            "25676 2\n",
            "25707 1\n",
            "25708 1\n",
            "25738 2\n",
            "25864 2\n",
            "25951 2\n",
            "25952 2\n",
            "25953 2\n",
            "25985 2\n",
            "26355 3\n",
            "26364 2\n",
            "26938 1\n",
            "27057 1\n",
            "27067 3\n",
            "27138 1\n",
            "27142 3\n",
            "27257 3\n",
            "27272 2\n",
            "27339 3\n",
            "27438 1\n",
            "27567 2\n",
            "27609 2\n",
            "27649 3\n",
            "27662 2\n",
            "27674 1\n",
            "27810 1\n",
            "27884 2\n",
            "28000 2\n",
            "28168 2\n",
            "28170 2\n",
            "28174 3\n",
            "28175 2\n",
            "28176 2\n",
            "28199 0\n",
            "28301 2\n",
            "28468 3\n",
            "28515 3\n",
            "28540 2\n",
            "28541 2\n",
            "28681 2\n",
            "28756 3\n",
            "28943 2\n",
            "28950 3\n",
            "29006 0\n",
            "29089 3\n",
            "29091 3\n",
            "29092 3\n",
            "29093 3\n",
            "29094 2\n",
            "29096 3\n",
            "29100 3\n",
            "29102 3\n",
            "29103 3\n",
            "29105 3\n",
            "29239 3\n",
            "29241 1\n",
            "29463 2\n",
            "29476 3\n",
            "36952 0\n",
            "37142 2\n",
            "37256 3\n",
            "37345 2\n",
            "37358 3\n",
            "37390 1\n",
            "37470 1\n",
            "37471 3\n",
            "37499 3\n",
            "37507 2\n",
            "37509 1\n",
            "37510 1\n",
            "37515 1\n",
            "37518 1\n",
            "37520 1\n",
            "37528 2\n",
            "37529 1\n",
            "37536 2\n",
            "37542 1\n",
            "37544 2\n",
            "37548 2\n",
            "37549 3\n",
            "37550 2\n",
            "37551 1\n",
            "37552 2\n",
            "37576 3\n",
            "37578 3\n",
            "37676 3\n",
            "37928 1\n",
            "37930 1\n",
            "37932 1\n",
            "37982 1\n",
            "38034 2\n",
            "38071 1\n",
            "38117 1\n",
            "38264 1\n",
            "38457 1\n",
            "38458 3\n",
            "38642 3\n",
            "38653 1\n",
            "38680 2\n",
            "38714 2\n",
            "38716 2\n",
            "38962 0\n",
            "38965 1\n",
            "38972 3\n",
            "39017 2\n",
            "39020 2\n",
            "39094 2\n",
            "39160 1\n",
            "39162 2\n",
            "39163 1\n",
            "39164 3\n",
            "39199 1\n",
            "39203 3\n",
            "39344 2\n",
            "39345 3\n",
            "39372 2\n",
            "39499 0\n",
            "{'china': {0: 6858, 2: 26, 4: 185, 1: 17, 3: 44}, 'us': {1: 2894, 2: 1863, 0: 5322, 3: 2609, 4: 482}, 'italy': {3: 50, 0: 87, 2: 169, 1: 2}, 'thailand': {2: 77, 3: 7, 0: 97, 1: 5}, 'india': {0: 133, 3: 241, 2: 111, 1: 20}, 'new zealand': {2: 56, 3: 35, 0: 34, 1: 3}, 'australia': {2: 284, 1: 38, 0: 52, 3: 48}, 'iraq': {3: 64, 2: 29, 1: 8}, 'japan': {0: 355, 2: 271, 1: 19, 3: 64}, 'uk': {2: 1043, 0: 359, 1: 590, 3: 934, 4: 2}, 'taiwan': {0: 242, 3: 2, 1: 1, 2: 4}, 'korea': {0: 113, 2: 29, 3: 17, 1: 3}, 'singapore': {0: 46, 2: 62, 3: 19, 1: 14}, 'france': {2: 99, 3: 66, 0: 56, 1: 31}, 'germany': {2: 144, 0: 322, 3: 45, 1: 19}, 'canada': {2: 256, 0: 175, 1: 45, 3: 28}, 'mexico': {2: 192, 3: 142, 0: 310, 1: 17}, 'russia': {3: 49, 2: 31, 0: 13, 1: 14}, 'brazil': {3: 107, 2: 26, 0: 36, 1: 2}, 'spain': {1: 6, 2: 57, 0: 13, 3: 57}, 'afghanistan': {2: 124, 3: 27, 1: 2, 0: 2}, 'vietnam': {2: 15, 0: 69, 3: 86, 1: 45}, 'switzerland': {1: 2, 3: 21, 0: 57, 2: 13}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_sFeRAVBSuV",
        "outputId": "97c43383-e9e8-4875-c7ae-50f0943c78b0"
      },
      "source": [
        "stat = {0:0, 1:0, 2:0, 3:0, 4:0}\n",
        "for k, item in ruiqi.items():\n",
        "    stat[int(item)] += 1\n",
        "\n",
        "print(stat)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 14751, 1: 3797, 2: 4981, 3: 4762, 4: 669}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}